# backend/services/api/routers/llm.py
"""
RAG API Router
RAG ì‹œìŠ¤í…œì„ ì‚¬ìš©í•œ LLM ê¸°ëŠ¥ì„ ìœ„í•œ FastAPI ì—”ë“œí¬ì¸íŠ¸
"""

from fastapi import APIRouter, HTTPException, Depends
from pydantic import BaseModel, Field
from typing import List, Optional, Dict
import logging
import os

from backend.services.rag.rag_system import RAGSystem
from backend.services.rag.models.config import EmbeddingModelType
from backend.services.rag.generation.generator import OllamaGenerator, GenerationConfig
from backend.services.rag.retrieval.reranker import KeywordReranker
from backend.services.rag.augmentation.formatters import EnhancedPromptFormatter
from backend.services.api.utils.summarizer import summarize_title, summarize_conversation_batch

from typing import Literal

# ModelType ì •ì˜
ModelType = Literal["ollama", "openai", "groq"]

logger = logging.getLogger(__name__)

router = APIRouter(prefix="/api/llm", tags=["LLM"])

# RAG ì‹œìŠ¤í…œ ì¸ìŠ¤í„´ìŠ¤ (ì‹±ê¸€í†¤ íŒ¨í„´)
_rag_system: Optional[RAGSystem] = None


def get_db_config() -> dict:
    """ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì • ê°€ì ¸ì˜¤ê¸°"""
    return {
        'host': os.getenv('PG_HOST', 'localhost'),
        'port': os.getenv('PG_PORT', '5432'),
        'database': os.getenv('PG_DB', 'rey'),
        'user': os.getenv('PG_USER', 'postgres'),
        'password': os.getenv('PG_PASSWORD', 'post1234')
    }


def get_model_type_from_env() -> EmbeddingModelType:
    """í™˜ê²½ ë³€ìˆ˜ì—ì„œ ì„ë² ë”© ëª¨ë¸ íƒ€ì… ê°€ì ¸ì˜¤ê¸°"""
    model_name = os.getenv('RAG_EMBEDDING_MODEL', 'E5_SMALL').upper()
    mapping = {
        "E5_SMALL": EmbeddingModelType.MULTILINGUAL_E5_SMALL,
        "E5_BASE": EmbeddingModelType.MULTILINGUAL_E5_BASE,
        "E5_LARGE": EmbeddingModelType.MULTILINGUAL_E5_LARGE,
        "KAKAO": EmbeddingModelType.KAKAOBANK_DEBERTA
    }
    return mapping.get(model_name, EmbeddingModelType.MULTILINGUAL_E5_SMALL)


def get_rag_system() -> RAGSystem:
    """RAG ì‹œìŠ¤í…œ ì¸ìŠ¤í„´ìŠ¤ ê°€ì ¸ì˜¤ê¸° (ì‹±ê¸€í†¤, í™˜ê²½ ë³€ìˆ˜ ë³€ê²½ ì‹œ ì¬ìƒì„±)"""
    global _rag_system
    
    # í™˜ê²½ ë³€ìˆ˜ì—ì„œ í˜„ì¬ ëª¨ë¸ íƒ€ì… ê°€ì ¸ì˜¤ê¸°
    current_model_type = get_model_type_from_env()
    
    # ì¸ìŠ¤í„´ìŠ¤ê°€ ì—†ê±°ë‚˜ ëª¨ë¸ íƒ€ì…ì´ ë³€ê²½ëœ ê²½ìš° ì¬ìƒì„±
    if _rag_system is None or _rag_system.retriever.model_type != current_model_type:
        # ê¸°ì¡´ ì¸ìŠ¤í„´ìŠ¤ê°€ ìˆìœ¼ë©´ ë¡œê·¸ ì¶œë ¥
        if _rag_system is not None:
            logger.info(f"Model type changed, recreating RAG system: {_rag_system.retriever.model_type.value} -> {current_model_type.value}")
        
        # DB ì„¤ì • ê°€ì ¸ì˜¤ê¸°
        db_config = get_db_config()
        
        # LLM Generator ì´ˆê¸°í™”
        # Ollama URL í™˜ê²½ ë³€ìˆ˜ì—ì„œ ì½ê¸° (ë„ì»¤ì—ì„œëŠ” host.docker.internal ì‚¬ìš©)
        ollama_url = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
        llm_generator = OllamaGenerator(
            base_url=ollama_url,
            default_model="gemma3:4b"
        )
        
        # RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        _rag_system = RAGSystem(
            model_type=current_model_type,
            db_config=db_config,
            reranker=KeywordReranker(
                use_llm_extraction=False  # Regex ì‚¬ìš© (ì†ë„ í–¥ìƒ: LLM í˜¸ì¶œ ì œê±°)
            ),
            formatter=EnhancedPromptFormatter(),
            llm_generator=llm_generator,
            enable_generation=True
        )
        logger.info(f"RAG System initialized with {current_model_type.value}")
    
    return _rag_system


# summarize_conversation_batch í•¨ìˆ˜ëŠ” ì´ì œ backend/services/api/utils/summarizer.pyì—ì„œ
# mT5-baseë¥¼ ì‚¬ìš©í•˜ì—¬ êµ¬í˜„ë©ë‹ˆë‹¤. (Ollama ëŒ€ì‹  êµ¬ê¸€ ëª¨ë¸ ì‚¬ìš©)
# importëŠ” ìƒë‹¨ì—ì„œ ì²˜ë¦¬: from backend.services.api.utils.summarizer import summarize_conversation_batch


# =============================================================================
# Request/Response Models
# =============================================================================

class QuestionRequest(BaseModel):
    """ì§ˆë¬¸ ìš”ì²­ ëª¨ë¸"""
    question: str = Field(..., description="ì‚¬ìš©ì ì§ˆë¬¸")
    model_type: ModelType = Field(default="ollama", description="ì‚¬ìš©í•  ëª¨ë¸ íƒ€ì…")
    with_memory: bool = Field(default=False, description="ëŒ€í™” ê¸°ë¡ ì‚¬ìš© ì—¬ë¶€")


class SourceInfo(BaseModel):
    """ì¶œì²˜ ì •ë³´ ëª¨ë¸"""
    title: str = Field(..., description="ì£¼íƒëª…")
    address: str = Field(..., description="ì£¼ì†Œ")
    district: str = Field(..., description="ì‹œêµ°êµ¬")


class QuestionResponse(BaseModel):
    """ì§ˆë¬¸ ì‘ë‹µ ëª¨ë¸"""
    answer: str = Field(..., description="ìƒì„±ëœ ë‹µë³€")
    sources: List[SourceInfo] = Field(default=[], description="ì°¸ê³  ë¬¸ì„œ")


class RecommendationRequest(BaseModel):
    """ì¶”ì²œ ìš”ì²­ ëª¨ë¸"""
    location: str = Field(..., description="í¬ë§ ìœ„ì¹˜", example="ê°•ë‚¨êµ¬")
    budget: str = Field(..., description="ì˜ˆì‚° ë²”ìœ„", example="ì›”ì„¸ 50ë§Œì› ì´í•˜")
    preferences: str = Field(..., description="ì„ í˜¸ ì¡°ê±´", example="ì§€í•˜ì² ì—­ ê·¼ì²˜")
    model_type: ModelType = Field(default="ollama", description="ì‚¬ìš©í•  ëª¨ë¸ íƒ€ì…")


class CandidateInfo(BaseModel):
    """ì¶”ì²œ í›„ë³´ ì •ë³´"""
    title: str
    address: str
    district: str


class RecommendationResponse(BaseModel):
    """ì¶”ì²œ ì‘ë‹µ ëª¨ë¸"""
    recommendation: str = Field(..., description="ì¶”ì²œ ì„¤ëª…")
    candidates: List[CandidateInfo] = Field(..., description="ì¶”ì²œ í›„ë³´ ëª©ë¡")


class ChatMessage(BaseModel):
    """ì±„íŒ… ë©”ì‹œì§€"""
    role: str = Field(..., description="ì—­í•  (user/assistant)")
    content: str = Field(..., description="ë©”ì‹œì§€ ë‚´ìš©")


class ChatRequest(BaseModel):
    """ì±„íŒ… ìš”ì²­"""
    messages: List[ChatMessage] = Field(..., description="ëŒ€í™” íˆìŠ¤í† ë¦¬")
    model_type: ModelType = Field(default="ollama", description="ëª¨ë¸ íƒ€ì…")


class ChatResponse(BaseModel):
    """ì±„íŒ… ì‘ë‹µ"""
    message: str = Field(..., description="AI ì‘ë‹µ")
    sources: List[SourceInfo] = Field(default=[], description="ì°¸ê³  ë¬¸ì„œ")
    title: Optional[str] = Field(default=None, description="ëŒ€í™” ì œëª© (ìš”ì•½)")


# =============================================================================
# Dependency: RAG System Instance
# =============================================================================

# RAG ì‹œìŠ¤í…œì€ get_rag_system() í•¨ìˆ˜ë¥¼ í†µí•´ ì‹±ê¸€í†¤ìœ¼ë¡œ ê´€ë¦¬ë©ë‹ˆë‹¤


# =============================================================================
# API Endpoints
# =============================================================================

@router.post("/ask", response_model=QuestionResponse)
async def ask_question(request: QuestionRequest, rag_system: RAGSystem = Depends(get_rag_system)):
    """
    ì§ˆë¬¸ì— ë‹µë³€í•˜ê¸° (RAG ì‹œìŠ¤í…œ ì‚¬ìš©)
    
    **ì‚¬ìš© ì˜ˆì‹œ:**
```json
    {
        "question": "ê°•ë‚¨ì—­ ê·¼ì²˜ ì²­ë…„ì£¼íƒ ì•Œë ¤ì¤˜",
        "model_type": "ollama",
        "with_memory": false
    }
```
    """
    try:
        # RAG ì‹œìŠ¤í…œìœ¼ë¡œ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (top_k=3ìœ¼ë¡œ ì†ë„ ê°œì„ )
        response = rag_system.generate_answer(
            query=request.question,
            top_k=3,  # ë¹„êµ ìë£Œ ê¸°ì¤€ìœ¼ë¡œ 3ê°œ ì‚¬ìš© (ì†ë„ ê°œì„ )
            use_reranker=True,
            generation_config=GenerationConfig(
                model="gemma3:4b",
                temperature=0.7,
                max_tokens=2000,
                timeout=180  # íƒ€ì„ì•„ì›ƒ 180ì´ˆ (LLM ì‘ë‹µ ëŒ€ê¸° ì‹œê°„ ì¦ê°€)
            )
        )
        
        # ì†ŒìŠ¤ ì •ë³´ ì¶”ì¶œ (ìƒìœ„ 1ê°œë§Œ)
        sources = []
        if response.retrieved_documents and len(response.retrieved_documents) > 0:
            doc = response.retrieved_documents[0]  # ìƒìœ„ 1ê°œë§Œ
            metadata = doc.get("metadata", {})
            # source í•„ë“œ ìš°ì„  ì‚¬ìš©, ì—†ìœ¼ë©´ metadataì—ì„œ source_doc_title, ê·¸ë˜ë„ ì—†ìœ¼ë©´ title ë˜ëŠ” content
            source_title = (
                doc.get("source") or 
                metadata.get("source") or 
                metadata.get("source_doc_title") or 
                doc.get("title") or 
                (doc.get("content", "")[:50] if doc.get("content") else "ë¬¸ì„œ")
            )
            sources.append(SourceInfo(
                title=source_title or "ë¬¸ì„œ",
                address=doc.get("address", ""),
                district=doc.get("district", "")
            ))
        
        return QuestionResponse(
            answer=response.generated_answer.answer if response.generated_answer else "",
            sources=sources
        )
        
    except Exception as e:
        logger.error(f"Error in ask_question: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/ask-agent", response_model=ChatResponse)
async def ask_question_with_agent(request: QuestionRequest, rag_system: RAGSystem = Depends(get_rag_system)):
    """
    RAG ì‹œìŠ¤í…œì„ ì‚¬ìš©í•œ ì§ˆë¬¸ ë‹µë³€ (ë¦¬ë­í‚¹ í¬í•¨)
    
    **ì‚¬ìš© ì˜ˆì‹œ:**
```json
    {
        "question": "ê°•ë‚¨êµ¬ ì²­ë…„ì£¼íƒ ëª¨ë‘ ë³´ì—¬ì¤˜",
        "model_type": "ollama",
        "with_memory": false
    }
```
    """
    try:
        # ë³‘ëª© í˜„ìƒ ë¶„ì„ (í™˜ê²½ ë³€ìˆ˜ë¡œ í™œì„±í™”)
        import os
        enable_profiling = os.getenv("RAG_PROFILING", "false").lower() == "true"
        
        if enable_profiling:
            from backend.services.rag.profiler import RAGProfiler
            profiler = RAGProfiler(enable_detailed_logging=True)
            profile = profiler.profile_full_pipeline(
                rag_system=rag_system,
                query=request.question,
                top_k=3,
                use_reranker=True,
                generation_config=GenerationConfig(
                    model="gemma3:4b",
                    temperature=0.7,
                    max_tokens=2000,
                    timeout=180
                )
            )
            profiler.print_profile(profile)
            # í”„ë¡œíŒŒì¼ë§ í›„ì—ë„ ì •ìƒ ì‘ë‹µ ë°˜í™˜
            response = rag_system.generate_answer(
                query=request.question,
                top_k=3,
                use_reranker=True,
                generation_config=GenerationConfig(
                    model="gemma3:4b",
                    temperature=0.7,
                    max_tokens=2000,
                    timeout=180
                )
            )
        else:
            # RAG ì‹œìŠ¤í…œìœ¼ë¡œ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (ë¦¬ë­í‚¹ í¬í•¨, top_k=3ìœ¼ë¡œ ì†ë„ ê°œì„ )
            try:
                response = rag_system.generate_answer(
                    query=request.question,
                    top_k=3,  # ë¹„êµ ìë£Œ ê¸°ì¤€ìœ¼ë¡œ 3ê°œ ì‚¬ìš© (ì†ë„ ê°œì„ )
                    use_reranker=True,  # ë¦¬ë­í‚¹ ì‚¬ìš© (ì´ë¯¸ Regexë¡œ ìµœì í™”ë¨)
                    generation_config=GenerationConfig(
                        model="gemma3:4b",
                        temperature=0.7,
                        max_tokens=1500,  # 2000 â†’ 1500ìœ¼ë¡œ ê°ì†Œ (ìƒì„± ì‹œê°„ ë‹¨ì¶•)
                        timeout=120  # 180 â†’ 120ì´ˆë¡œ ê°ì†Œ (ì ì ˆí•œ íƒ€ì„ì•„ì›ƒ)
                    )
                )
            except Exception as gen_error:
                logger.error(f"Generation error: {gen_error}", exc_info=True)
                # íƒ€ì„ì•„ì›ƒ ë˜ëŠ” ìƒì„± ì˜¤ë¥˜ ì‹œ ê¸°ë³¸ ë©”ì‹œì§€ ë°˜í™˜
                if "ì‹œê°„ ì´ˆê³¼" in str(gen_error) or "timeout" in str(gen_error).lower():
                    answer_text = "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ìƒì„± ì‹œê°„ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
                else:
                    answer_text = f"ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ìƒì„± ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(gen_error)}"
                # ê¸°ë³¸ ì†ŒìŠ¤ ì •ë³´ë¼ë„ ì œê³µ
                sources = []
                return ChatResponse(message=answer_text, sources=sources)
        
        # ìƒì„±ëœ ë‹µë³€ì´ ì—†ëŠ” ê²½ìš° ì—ëŸ¬ ë©”ì‹œì§€
        if not response.generated_answer:
            logger.error("Generated answer is None. RAG pipeline may have failed.")
            answer_text = "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì„ í™•ì¸í•´ì£¼ì„¸ìš”."
        elif not response.generated_answer.answer or len(response.generated_answer.answer.strip()) == 0:
            logger.warning("Generated answer is empty. Check if Ollama is running and accessible.")
            answer_text = "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Ollama ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•´ì£¼ì„¸ìš”."
        else:
            answer_text = response.generated_answer.answer.strip()
        
        # ì†ŒìŠ¤ ì •ë³´ ì¶”ì¶œ (ìƒìœ„ 1ê°œë§Œ)
        sources = []
        if response.retrieved_documents and len(response.retrieved_documents) > 0:
            doc = response.retrieved_documents[0]  # ìƒìœ„ 1ê°œë§Œ
            metadata = doc.get("metadata", {})
            # source í•„ë“œ ìš°ì„  ì‚¬ìš©, ì—†ìœ¼ë©´ metadataì—ì„œ source_doc_title, ê·¸ë˜ë„ ì—†ìœ¼ë©´ title ë˜ëŠ” content
            source_title = (
                doc.get("source") or 
                metadata.get("source") or 
                metadata.get("source_doc_title") or 
                doc.get("title") or 
                (doc.get("content", "")[:50] if doc.get("content") else "ë¬¸ì„œ")
            )
            sources.append(SourceInfo(
                title=source_title or "ë¬¸ì„œ",
                address=doc.get("address", ""),
                district=doc.get("district", "")
            ))
        
        return ChatResponse(
            message=answer_text,
            sources=sources
        )
        
    except Exception as e:
        logger.error(f"Error in ask_question_with_agent: {e}", exc_info=True)
        # ì—ëŸ¬ ë©”ì‹œì§€ë¥¼ ë” ìƒì„¸í•˜ê²Œ ë¡œê¹…
        import traceback
        logger.error(f"Traceback: {traceback.format_exc()}")
        
        # ì‚¬ìš©ìì—ê²Œ ì¹œì ˆí•œ ì—ëŸ¬ ë©”ì‹œì§€ ë°˜í™˜
        error_message = "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ìƒì„± ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
        if "database" in str(e).lower() or "postgres" in str(e).lower():
            error_message = "ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
        elif "timeout" in str(e).lower() or "ì‹œê°„ ì´ˆê³¼" in str(e):
            error_message = "ë‹µë³€ ìƒì„± ì‹œê°„ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
        elif "ollama" in str(e).lower():
            error_message = "Ollama ì„œë²„ ì—°ê²°ì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤. ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•´ì£¼ì„¸ìš”."
        
        return ChatResponse(
            message=error_message,
            sources=[]
        )


@router.post("/recommend", response_model=RecommendationResponse)
async def get_recommendation(request: RecommendationRequest):
    """
    ì£¼íƒ ì¶”ì²œí•˜ê¸°
    """
    try:
        # ì¶”ì²œ ë¡œì§ êµ¬í˜„
        recommendation_text = f"{request.location} ì§€ì—­ì˜ {request.budget} ì˜ˆì‚°ìœ¼ë¡œ {request.preferences} ì¡°ê±´ì— ë§ëŠ” ì£¼íƒì„ ì¶”ì²œí•©ë‹ˆë‹¤."
        
        # TODO: ì‹¤ì œ ì¶”ì²œ ë¡œì§ êµ¬í˜„
        candidates = []
        
        return RecommendationResponse(
            recommendation=recommendation_text,
            candidates=candidates
        )
        
    except Exception as e:
        logger.error(f"Error in get_recommendation: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/chat", response_model=ChatResponse)
async def chat(request: ChatRequest, rag_system: RAGSystem = Depends(get_rag_system)):
    """
    ëŒ€í™”í˜• ì±„íŒ… (RAG ì‹œìŠ¤í…œ ì‚¬ìš©, ëŒ€í™” íˆìŠ¤í† ë¦¬ ê¸°ì–µ ë° ìš”ì•½)
    
    **ì‚¬ìš© ì˜ˆì‹œ:**
```json
    {
        "messages": [
            {"role": "user", "content": "ì•ˆë…•í•˜ì„¸ìš”"},
            {"role": "assistant", "content": "ì•ˆë…•í•˜ì„¸ìš”! ì£¼íƒ ê²€ìƒ‰ì„ ë„ì™€ë“œë¦¬ê² ìŠµë‹ˆë‹¤."},
            {"role": "user", "content": "ê°•ë‚¨êµ¬ ì²­ë…„ì£¼íƒ ì•Œë ¤ì¤˜"}
        ],
        "model_type": "ollama"
    }
```
    """
    try:
        # ë§ˆì§€ë§‰ ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ì¶œ
        user_messages = [msg for msg in request.messages if msg.role == "user"]
        if not user_messages:
            raise HTTPException(status_code=400, detail="No user message found")
        
        last_user_message = user_messages[-1].content
        
        # ëŒ€í™” íˆìŠ¤í† ë¦¬ ì²˜ë¦¬: 10ê°œë§ˆë‹¤ ìš”ì•½í•˜ê³ , ìš”ì•½ëœ ë‚´ìš© + ìµœê·¼ ë©”ì‹œì§€ ì¡°í•©
        total_messages = len(request.messages)
        recent_message_count = 10  # ìµœê·¼ ë©”ì‹œì§€ ê°œìˆ˜
        summary_interval = 10  # 10ê°œë§ˆë‹¤ ìš”ì•½
        
        # ë©”ì‹œì§€ë¥¼ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë³€í™˜
        messages_dict = [{"role": msg.role, "content": msg.content} for msg in request.messages]
        
        # ìš”ì•½ì´ í•„ìš”í•œ êµ¬ê°„ ê³„ì‚°
        conversation_summaries = []  # ìš”ì•½ëœ ëŒ€í™”ë“¤
        recent_messages_for_context = []  # ìµœê·¼ ë©”ì‹œì§€ë“¤
        
        if total_messages > summary_interval:
            # ìš”ì•½ì´ í•„ìš”í•œ êµ¬ê°„ë“¤ ì²˜ë¦¬ (10ê°œ ì´ìƒì¸ ê²½ìš°)
            summary_start = 0
            while summary_start < total_messages - recent_message_count:
                summary_end = min(summary_start + summary_interval, total_messages - recent_message_count)
                batch_messages = messages_dict[summary_start:summary_end]
                
                if batch_messages:
                    summary = summarize_conversation_batch(batch_messages)
                    if summary:
                        conversation_summaries.append(summary)
                
                summary_start = summary_end
            
            # ìµœê·¼ ë©”ì‹œì§€ë“¤ (ìš”ì•½ë˜ì§€ ì•Šì€ ë¶€ë¶„)
            recent_messages_for_context = messages_dict[-(recent_message_count + 1):-1]  # ë§ˆì§€ë§‰ ì§ˆë¬¸ ì œì™¸
        else:
            # 10ê°œ ë¯¸ë§Œì¼ ë•ŒëŠ” ìš”ì•½ ì—†ì´ ì „ì²´ ëŒ€í™” íˆìŠ¤í† ë¦¬ ì‚¬ìš© (ë§ˆì§€ë§‰ ì§ˆë¬¸ ì œì™¸)
            recent_messages_for_context = messages_dict[:-1] if len(messages_dict) > 1 else []
        
        # ëŒ€í™” íˆìŠ¤í† ë¦¬ í¬ë§·íŒ…
        history_parts = []
        
        # ìš”ì•½ëœ ë‚´ìš© ì¶”ê°€ (10ê°œ ì´ìƒì¼ ë•Œë§Œ)
        if conversation_summaries:
            history_parts.append("## ì´ì „ ëŒ€í™” ìš”ì•½")
            for i, summary in enumerate(conversation_summaries, 1):
                history_parts.append(f"{i}. {summary}")
        
        # ìµœê·¼ ë©”ì‹œì§€ ì¶”ê°€ (10ê°œ ë¯¸ë§Œì¼ ë•ŒëŠ” ì „ì²´ íˆìŠ¤í† ë¦¬, 10ê°œ ì´ìƒì¼ ë•ŒëŠ” ìµœê·¼ ë©”ì‹œì§€)
        if recent_messages_for_context:
            if conversation_summaries:
                history_parts.append("\n## ìµœê·¼ ëŒ€í™”")
            elif total_messages <= summary_interval:
                # 10ê°œ ë¯¸ë§Œì¼ ë•ŒëŠ” "ì´ì „ ëŒ€í™” ë‚´ìš©"ìœ¼ë¡œ í‘œì‹œ
                history_parts.append("## ì´ì „ ëŒ€í™” ë‚´ìš©")
            
            for msg in recent_messages_for_context:
                if msg["role"] == "user":
                    history_parts.append(f"ì‚¬ìš©ì: {msg['content']}")
                elif msg["role"] == "assistant":
                    history_parts.append(f"ì–´ì‹œìŠ¤í„´íŠ¸: {msg['content']}")
        
        history_text = "\n".join(history_parts) if history_parts else ""
        
        # ë””ë²„ê¹…: íˆìŠ¤í† ë¦¬ ë‚´ìš© ë¡œê¹…
        logger.info(f"Total messages: {total_messages}, History included: {len(recent_messages_for_context)} messages")
        
        # RAG ì‹œìŠ¤í…œìœ¼ë¡œ ê²€ìƒ‰ ë° ì»¨í…ìŠ¤íŠ¸ ìƒì„±
        rag_response = rag_system.retrieve_and_augment(
            query=last_user_message,
            top_k=3,
            use_reranker=True
        )
        
        # ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ í¬í•¨í•œ í”„ë¡¬í”„íŠ¸ ìƒì„±
        context_text = rag_response.augmented_context.context_text if rag_response.augmented_context else ""
        
        # Ollamaë¡œ ëŒ€í™” íˆìŠ¤í† ë¦¬ í¬í•¨ ë‹µë³€ ìƒì„±
        ollama_url = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
        
        if history_text:
            # ëŒ€í™” íˆìŠ¤í† ë¦¬ê°€ ìˆëŠ” ê²½ìš° - ì´ì „ ëŒ€í™”ë¥¼ ìš°ì„  ì°¸ê³ í•˜ë„ë¡ êµ¬ì¡° ë³€ê²½
            prompt = f"""ë‹¹ì‹ ì€ ì²­ë…„ ì£¼ê±° ì •ì±… ì „ë¬¸ ìƒë‹´ì‚¬ì…ë‹ˆë‹¤. **ë°˜ë“œì‹œ ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ë¨¼ì € í™•ì¸í•˜ê³ **, ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì°¸ê³  ë¬¸ì„œì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì•„ ë‹µë³€í•˜ì„¸ìš”.

## âš ï¸ ë§¤ìš° ì¤‘ìš”: ë‹µë³€ ì „ í•„ìˆ˜ ì ˆì°¨ (ìˆœì„œëŒ€ë¡œ ë°˜ë“œì‹œ ìˆ˜í–‰í•˜ì„¸ìš”!)

### 1ë‹¨ê³„: í˜„ì¬ ì§ˆë¬¸ ì´í•´
- í˜„ì¬ ì§ˆë¬¸: "{last_user_message}"ë¥¼ ë¨¼ì € ì½ìœ¼ì„¸ìš”

### 2ë‹¨ê³„: ì´ì „ ëŒ€í™” ë‚´ìš© í™•ì¸ (ê°€ì¥ ì¤‘ìš”! ì •í™•ì„± í•„ìˆ˜!)
**ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ë°˜ë“œì‹œ ì½ê³  ë‹¤ìŒì„ í™•ì¸í•˜ì„¸ìš”:**
- ì‚¬ìš©ìì˜ ì´ë¦„: (ì´ì „ ëŒ€í™”ì—ì„œ **ì‹¤ì œë¡œ ì–¸ê¸‰ë˜ì—ˆì„ ë•Œë§Œ** ì°¾ì•„ì„œ ê¸°ì–µ. ì—†ìœ¼ë©´ "ì •ë³´ ì—†ìŒ"ìœ¼ë¡œ ê¸°ë¡)
- ì‚¬ìš©ìì˜ ê±°ì£¼ì§€: (ì´ì „ ëŒ€í™”ì—ì„œ **ì‹¤ì œë¡œ ì–¸ê¸‰ë˜ì—ˆì„ ë•Œë§Œ** ì°¾ì•„ì„œ ê¸°ì–µ. ì—†ìœ¼ë©´ "ì •ë³´ ì—†ìŒ"ìœ¼ë¡œ ê¸°ë¡)
- ì´ì „ ì§ˆë¬¸ ì£¼ì œ: (ì´ì „ ëŒ€í™”ì—ì„œ ë¬´ì—‡ì„ ë¬¼ì–´ë´¤ëŠ”ì§€)
- ì´ì „ ë‹µë³€ ë‚´ìš©: (ì´ì „ì— ë¬´ì—‡ì„ ë‹µë³€í–ˆëŠ”ì§€)

**âš ï¸ ë§¤ìš° ì¤‘ìš”**: ì´ì „ ëŒ€í™”ì—ì„œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìœ¼ë©´ **ì ˆëŒ€ë¡œ** ì¶”ì¸¡í•˜ê±°ë‚˜ ë§Œë“¤ì–´ë‚´ì§€ ë§ˆì„¸ìš”. "ì •ë³´ ì—†ìŒ"ìœ¼ë¡œ ê¸°ë¡í•˜ê³  ë‹µë³€ì— ë°˜ì˜í•˜ì„¸ìš”.

### 3ë‹¨ê³„: í˜„ì¬ ì§ˆë¬¸ê³¼ ì´ì „ ëŒ€í™” ì—°ê²° (ì •í™•ì„± í•„ìˆ˜!)
- "ì–¼ë§ˆê¹Œì§€ ì§€ì›í•´ì£¼ëŠ”ê±°ì•¼?" â†’ ì´ì „ ëŒ€í™”ì—ì„œ ì„¤ëª…í•œ ì§€ì›ê¸ˆì˜ êµ¬ì²´ ê¸ˆì•¡ì„ ì°¾ì•„ ë‹µë³€
- "ë‚´ê°€ ì–´ë”” ì‚¬ëŠ”ì§€ ê¸°ì–µí•´?" â†’ **ì´ì „ ëŒ€í™”ì— ì‹¤ì œë¡œ ê±°ì£¼ì§€ê°€ ì–¸ê¸‰ë˜ì–´ ìˆì„ ë•Œë§Œ** "ë„¤, [ê±°ì£¼ì§€]ì— ê±°ì£¼í•˜ì‹œëŠ” ê²ƒìœ¼ë¡œ ê¸°ì–µí•˜ê³  ìˆìŠµë‹ˆë‹¤" í˜•íƒœë¡œ ë‹µë³€. **ì—†ìœ¼ë©´** "ì´ì „ ëŒ€í™”ì—ì„œ ê±°ì£¼ì§€ ì •ë³´ë¥¼ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µë³€
- "ë‚´ ì´ë¦„ ê¸°ì–µí•˜ì§€?" â†’ **ì´ì „ ëŒ€í™”ì— ì‹¤ì œë¡œ ì´ë¦„ì´ ì–¸ê¸‰ë˜ì–´ ìˆì„ ë•Œë§Œ** ë‹µë³€. **ì—†ìœ¼ë©´** "ì´ì „ ëŒ€í™”ì—ì„œ ì´ë¦„ ì •ë³´ë¥¼ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µë³€
- "ë‹¤ì‹œ ì•Œë ¤ì¤˜" â†’ ì´ì „ ë‹µë³€ ë‚´ìš©ì„ ì¬í™œìš©
- **âš ï¸ ì ˆëŒ€ í•˜ì§€ ë§ ê²ƒ**: ì˜ˆì‹œë‚˜ ë‹¤ë¥¸ ì •ë³´ë¥¼ ì‹¤ì œ ì •ë³´ì²˜ëŸ¼ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”. ë°˜ë“œì‹œ ì´ì „ ëŒ€í™”ì—ì„œ ì‹¤ì œë¡œ ì–¸ê¸‰ëœ ì •ë³´ë§Œ ì‚¬ìš©í•˜ì„¸ìš”

### 4ë‹¨ê³„: ì°¸ê³  ë¬¸ì„œ í™œìš©
- ì°¸ê³  ë¬¸ì„œëŠ” ì´ì „ ëŒ€í™” ë§¥ë½ì— ë§ëŠ” ì •ë³´ë§Œ ì„ íƒì ìœ¼ë¡œ ì‚¬ìš©
- ì´ì „ ëŒ€í™”ì™€ ê´€ë ¨ ì—†ëŠ” ì •ë³´ëŠ” ë¬´ì‹œí•˜ì„¸ìš”

## í˜„ì¬ ì§ˆë¬¸
ì‚¬ìš©ì: {last_user_message}

## âš ï¸ ë§¤ìš° ì¤‘ìš”: ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ë¨¼ì € ì½ìœ¼ì„¸ìš”!
ë‹¤ìŒ ë‚´ìš©ì—ì„œ **ë°˜ë“œì‹œ** ë‹¤ìŒ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ê³  ê¸°ì–µí•˜ì„¸ìš”:

{history_text}

**ìœ„ ì´ì „ ëŒ€í™” ë‚´ìš©ì—ì„œ ì°¾ì•„ì•¼ í•  ì •ë³´:**
1. ì‚¬ìš©ìê°€ ì–¸ê¸‰í•œ **ì´ë¦„** 
2. ì‚¬ìš©ìê°€ ì–¸ê¸‰í•œ **ê±°ì£¼ì§€** 
3. ì´ì „ ì§ˆë¬¸ ì£¼ì œ (ì˜ˆ: "ì›”ì„¸ ì§€ì›ê¸ˆ", "ì „ì„¸ ëŒ€ì¶œ" ë“±)
4. ì´ì „ ë‹µë³€ ë‚´ìš©

**íŠ¹íˆ ì£¼ì˜ì‚¬í•­:**
- "ë‚´ê°€ ì–´ë”” ì‚¬ëŠ”ì§€ ê¸°ì–µí•´?" ê°™ì€ ì§ˆë¬¸ì—ëŠ” ë°˜ë“œì‹œ ì´ì „ ëŒ€í™”ì—ì„œ ê±°ì£¼ì§€ë¥¼ ì°¾ì•„ì„œ ë‹µë³€í•˜ì„¸ìš”
- **ì¤‘ìš”**: ì´ì „ ëŒ€í™”ì—ì„œ ì‹¤ì œë¡œ ê±°ì£¼ì§€ê°€ ì–¸ê¸‰ë˜ì—ˆì„ ë•Œë§Œ ê±°ì£¼ì§€ë¥¼ ë‹µë³€í•˜ì„¸ìš”. ì´ì „ ëŒ€í™”ì— ê±°ì£¼ì§€ ì •ë³´ê°€ ì—†ìœ¼ë©´ "ì´ì „ ëŒ€í™”ì—ì„œ ê±°ì£¼ì§€ ì •ë³´ë¥¼ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µë³€í•˜ì„¸ìš”
- **ì¤‘ìš”**: ì´ì „ ëŒ€í™”ì—ì„œ ì‹¤ì œë¡œ ì´ë¦„ì´ ì–¸ê¸‰ë˜ì—ˆì„ ë•Œë§Œ ì´ë¦„ì„ ë‹µë³€í•˜ì„¸ìš”. ì´ì „ ëŒ€í™”ì— ì´ë¦„ ì •ë³´ê°€ ì—†ìœ¼ë©´ "ì´ì „ ëŒ€í™”ì—ì„œ ì´ë¦„ì„ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µë³€í•˜ì„¸ìš”
- ì´ì „ ëŒ€í™”ë¥¼ ì½ì§€ ì•Šê³  "ë¬¸ì„œì— ì—†ëŠ” ì •ë³´"ë¼ê³  ë‹µë³€í•˜ì§€ ë§ˆì„¸ìš”
- **ì ˆëŒ€ë¡œ** ì´ì „ ëŒ€í™”ì— ì—†ëŠ” ì •ë³´ë¥¼ ì„ì˜ë¡œ ë§Œë“¤ì–´ë‚´ì§€ ë§ˆì„¸ìš”. ë°˜ë“œì‹œ ì´ì „ ëŒ€í™” ë‚´ìš©ì—ì„œë§Œ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”

## ì°¸ê³  ë¬¸ì„œ (ì´ì „ ëŒ€í™” ë§¥ë½ì— ë§ê²Œ ì°¸ê³ , ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ìš°ì„ í•˜ì„¸ìš”)
{context_text}

## ë‹µë³€ ì‘ì„± ê·œì¹™ (ìš°ì„ ìˆœìœ„ ìˆœ)
1. **ì´ì „ ëŒ€í™” ë§¥ë½ ìš°ì„  (ìµœìš°ì„ , ì •í™•ì„± í•„ìˆ˜)**: 
   - ì´ì „ ëŒ€í™”ì—ì„œ ì–¸ê¸‰ëœ ëª¨ë“  ì •ë³´(ì´ë¦„, ê±°ì£¼ì§€, ì§ˆë¬¸ ì£¼ì œ, ë‹µë³€ ë‚´ìš©)ë¥¼ ë°˜ë“œì‹œ ë¨¼ì € í™•ì¸í•˜ê³  í™œìš©í•˜ì„¸ìš”
   - **âš ï¸ ë§¤ìš° ì¤‘ìš”**: ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ì •í™•íˆ ì½ê³ , ì‹¤ì œë¡œ ì–¸ê¸‰ëœ ì •ë³´ë§Œ ì‚¬ìš©í•˜ì„¸ìš”
   - "ë‚´ê°€ ì–´ë”” ì‚¬ëŠ”ì§€ ê¸°ì–µí•´?" â†’ ì´ì „ ëŒ€í™”ì—ì„œ ì‚¬ìš©ìê°€ **ì‹¤ì œë¡œ ê±°ì£¼ì§€ë¥¼ ì–¸ê¸‰í–ˆì„ ë•Œë§Œ** "ë„¤, [ê±°ì£¼ì§€]ì— ê±°ì£¼í•˜ì‹œëŠ” ê²ƒìœ¼ë¡œ ê¸°ì–µí•˜ê³  ìˆìŠµë‹ˆë‹¤"ë¼ê³  ë‹µë³€. **ê±°ì£¼ì§€ê°€ ì–¸ê¸‰ë˜ì§€ ì•Šì•˜ìœ¼ë©´** "ì£„ì†¡í•˜ì§€ë§Œ ì´ì „ ëŒ€í™”ì—ì„œ ê±°ì£¼ì§€ ì •ë³´ë¥¼ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µë³€í•˜ì„¸ìš”
   - "ë‚´ ì´ë¦„ ê¸°ì–µí•˜ì§€?" â†’ ì´ì „ ëŒ€í™”ì—ì„œ **ì‹¤ì œë¡œ ì´ë¦„ì´ ì–¸ê¸‰ë˜ì—ˆì„ ë•Œë§Œ** ë‹µë³€. **ì´ë¦„ì´ ì–¸ê¸‰ë˜ì§€ ì•Šì•˜ìœ¼ë©´** "ì£„ì†¡í•˜ì§€ë§Œ ì´ì „ ëŒ€í™”ì—ì„œ ì´ë¦„ ì •ë³´ë¥¼ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µë³€í•˜ì„¸ìš”
   - "ì–¼ë§ˆê¹Œì§€ ì§€ì›í•´ì£¼ëŠ”ê±°ì•¼?" â†’ ì´ì „ ëŒ€í™”ì—ì„œ ì„¤ëª…í•œ ì§€ì›ê¸ˆ/ì •ì±…ì— ëŒ€í•œ êµ¬ì²´ì ì¸ ê¸ˆì•¡ì„ ì°¾ì•„ì„œ ë‹µë³€
   - **ì ˆëŒ€ë¡œ** ì´ì „ ëŒ€í™”ì— ì—†ëŠ” ì •ë³´ë¥¼ ì¶”ì¸¡í•˜ê±°ë‚˜ ì„ì˜ë¡œ ë§Œë“¤ì–´ë‚´ì§€ ë§ˆì„¸ìš”
   - **ì ˆëŒ€ë¡œ** ì˜ˆì‹œë‚˜ ë‹¤ë¥¸ ëŒ€í™”ì˜ ì •ë³´ë¥¼ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”

2. **ì—°ì†ì„± ìœ ì§€ (ì¤‘ìš”: ì •í™•ì„± ìš°ì„ )**: 
   - ì´ì „ ëŒ€í™”ì˜ ì£¼ì œì™€ í˜„ì¬ ì§ˆë¬¸ì˜ ì—°ê²°ì ì„ ì •í™•íˆ íŒŒì•…í•˜ì„¸ìš”
   - ì´ì „ì— ì œê³µí•œ ì •ë³´ëŠ” ê·¸ëŒ€ë¡œ í™œìš©í•˜ê³ , ì¶”ê°€ ì •ë³´ë§Œ ë³´ì™„í•˜ì„¸ìš”
   - **ì ˆëŒ€ë¡œ** ì´ì „ ëŒ€í™”ì— ì—†ëŠ” ì •ë³´(ì´ë¦„, ê±°ì£¼ì§€ ë“±)ë¥¼ ì¶”ì¸¡í•˜ê±°ë‚˜ ì„ì˜ë¡œ ë§Œë“¤ì–´ë‚´ì§€ ë§ˆì„¸ìš”
   - ì´ì „ ëŒ€í™”ì—ì„œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìœ¼ë©´ "ì´ì „ ëŒ€í™”ì—ì„œ [ì •ë³´]ë¥¼ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ëª…í™•íˆ ë§í•˜ì„¸ìš”

3. **ì •í™•ì„±**: 
   - ì°¸ê³  ë¬¸ì„œì˜ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ë˜, ì´ì „ ëŒ€í™” ë§¥ë½ì— ë§ê²Œ ì„ íƒí•˜ì„¸ìš”
   - ì´ì „ ëŒ€í™”ì™€ ê´€ë ¨ ì—†ëŠ” ì •ë³´ëŠ” ì œê³µí•˜ì§€ ë§ˆì„¸ìš”

4. **êµ¬ì²´ì„±**: 
   - ì´ì „ ëŒ€í™”ì—ì„œ ì–¸ê¸‰ëœ ë‚´ìš©ì— ëŒ€í•œ êµ¬ì²´ì ì¸ ê¸ˆì•¡ì´ë‚˜ ìˆ˜ì¹˜ë¥¼ ëª…ì‹œí•˜ì„¸ìš”

5. **ì¼ê´€ì„±**: 
   - ì´ì „ ëŒ€í™”ì—ì„œ ì–¸ê¸‰ëœ ì´ë¦„, ê±°ì£¼ì§€, ê°œì¸ ì •ë³´ë¥¼ ì¼ê´€ë˜ê²Œ ì‚¬ìš©í•˜ì„¸ìš”

6. **ì¹œì ˆì„±**: í•œêµ­ì–´ë¡œ ìì—°ìŠ¤ëŸ½ê³  ì¹œì ˆí•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”

ë‹µë³€:"""
        else:
            # ëŒ€í™” íˆìŠ¤í† ë¦¬ê°€ ì—†ëŠ” ê²½ìš° (ì²« ì§ˆë¬¸)
            prompt = f"""ë‹¹ì‹ ì€ ì²­ë…„ ì£¼ê±° ì •ì±… ì „ë¬¸ ìƒë‹´ì‚¬ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ì¹œì ˆí•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.

## ì°¸ê³  ë¬¸ì„œ
{context_text}

## ì‚¬ìš©ì ì§ˆë¬¸
{last_user_message}

## ë‹µë³€ ì‘ì„± ê·œì¹™
1. **ì •í™•ì„±**: ì°¸ê³  ë¬¸ì„œì˜ ë‚´ìš©ë§Œì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ê³ , ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”
2. **êµ¬ì¡°í™”**: ëª…í™•í•œ ì„¹ì…˜ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ì²´ê³„ì ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš” (ì˜ˆ: ì‹ ì²­ ìê²©, ê¸ˆë¦¬, í•œë„, ì‹ ì²­ ë°©ë²• ë“±)
3. **ì™„ì „ì„±**: í‘œë‚˜ ë¦¬ìŠ¤íŠ¸ë¥¼ ì‘ì„±í•  ë•Œ ì¤‘ê°„ì— ëŠê¸°ì§€ ì•Šë„ë¡ ì™„ì „í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”
4. **êµ¬ì²´ì„±**: ì¡°ê±´, ê¸ˆì•¡, ê¸ˆë¦¬, ì ˆì°¨ ë“± êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ì™€ ì •ë³´ë¥¼ ëª…ì‹œí•˜ì„¸ìš”
5. **ëª…í™•ì„±**: ë§ˆí¬ë‹¤ìš´ í˜•ì‹ì„ ì‚¬ìš©í•˜ì—¬ ì½ê¸° ì‰½ê²Œ ì‘ì„±í•˜ì„¸ìš” (**, -, 1. ë“±)
6. **ê´€ë ¨ì„±**: ì§ˆë¬¸ê³¼ ì§ì ‘ ê´€ë ¨ëœ ì •ë³´ë§Œ ì œê³µí•˜ê³ , ë¶ˆí•„ìš”í•œ ë‚´ìš©ì€ ì œì™¸í•˜ì„¸ìš”
7. **ì¹œì ˆì„±**: í•œêµ­ì–´ë¡œ ìì—°ìŠ¤ëŸ½ê³  ì¹œì ˆí•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”

ë‹µë³€:"""
        
        import requests
        payload = {
            "model": "gemma3:4b",
            "prompt": prompt,
            "stream": False,
            "options": {
                "temperature": 0.7,
                "num_predict": 2000
            }
        }
        
        response = requests.post(
            f"{ollama_url}/api/generate",
            json=payload,
            timeout=180
        )
        response.raise_for_status()
        
        result = response.json()
        answer = result.get("response", "").strip()
        
        # mT5-baseë¥¼ ì‚¬ìš©í•˜ì—¬ ì œëª© ìš”ì•½ ìƒì„± (ì²« ë²ˆì§¸ AI ë‹µë³€ì¸ ê²½ìš°ì—ë§Œ)
        title = None
        # ì²« ë²ˆì§¸ ì‚¬ìš©ì ì§ˆë¬¸ì¸ ê²½ìš° (len(user_messages) == 1)
        # ì´ ì‹œì ì—ì„œ AI ë‹µë³€(answer)ì´ ì´ë¯¸ ìƒì„±ë˜ì–´ ìˆìœ¼ë¯€ë¡œ, ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì œëª© ìƒì„±
        if len(user_messages) == 1:  # ì²« ë²ˆì§¸ ì‚¬ìš©ì ì§ˆë¬¸ = ì²« ë²ˆì§¸ AI ë‹µë³€ ìƒì„± ì‹œì 
            logger.info(f"ì²« ë²ˆì§¸ ì‚¬ìš©ì ì§ˆë¬¸ ê°ì§€, ì²« ë²ˆì§¸ AI ë‹µë³€ ê¸°ë°˜ ì œëª© ìƒì„± ì‹œì‘. ì‘ë‹µ ê¸¸ì´: {len(answer)}ì")
            try:
                # ì²« ë²ˆì§¸ AI ë‹µë³€ì„ mT5-baseë¡œ ìš”ì•½í•˜ì—¬ 25ì ë‚´ì™¸ ì œëª© ìƒì„±
                logger.info("mT5-base ìš”ì•½ í•¨ìˆ˜ í˜¸ì¶œ ì¤‘...")
                title = summarize_title(answer, max_length=25)
                if title:
                    logger.info(f"âœ… mT5-base ìš”ì•½ ì„±ê³µ (ì²« ë²ˆì§¸ AI ë‹µë³€ ê¸°ë°˜): '{title}' ({len(title)}ì)")
                else:
                    logger.warning("âš ï¸ mT5-base ìš”ì•½ ê²°ê³¼ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
            except Exception as e:
                logger.error(f"âŒ mT5-base ìš”ì•½ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
                # ì‹¤íŒ¨ ì‹œ fallback: ì²« 25ì ì‚¬ìš©
                title = answer[:25] if answer else None
                logger.info(f"Fallback ì œëª© ì‚¬ìš©: '{title}'")
        else:
            logger.debug(f"ì²« ë²ˆì§¸ ì§ˆë¬¸ì´ ì•„ë‹˜ (ì‚¬ìš©ì ë©”ì‹œì§€ ìˆ˜: {len(user_messages)}), ì œëª© ìƒì„± ìŠ¤í‚µ")
        
        # ì†ŒìŠ¤ ì •ë³´ ì¶”ì¶œ
        sources = []
        for doc in rag_response.retrieved_documents[:5]:  # ìƒìœ„ 5ê°œë§Œ
            metadata = doc.get("metadata", {})
            # source í•„ë“œ ìš°ì„  ì‚¬ìš©, ì—†ìœ¼ë©´ metadataì—ì„œ source_doc_title, ê·¸ë˜ë„ ì—†ìœ¼ë©´ title ë˜ëŠ” content
            source_title = (
                doc.get("source") or 
                metadata.get("source") or 
                metadata.get("source_doc_title") or 
                doc.get("title") or 
                (doc.get("content", "")[:50] if doc.get("content") else "ë¬¸ì„œ")
            )
            district = doc.get("district", "") or metadata.get("district", "")
            address = doc.get("address", "") or metadata.get("address", "")
            
            # ë¹ˆ í•„ë“œê°€ ëª¨ë‘ ì—†ëŠ” ê²½ìš°ì—ë§Œ ì œì™¸
            if source_title or district or address:
                sources.append(SourceInfo(
                    title=source_title or "ë¬¸ì„œ",
                    address=address,
                    district=district
                ))
        
        # ì œëª© ìƒì„± ê²°ê³¼ ë¡œê¹…
        if title:
            logger.info(f"ğŸ“Œ ìµœì¢… ì œëª© ë°˜í™˜: '{title}' ({len(title)}ì)")
        else:
            logger.warning("âš ï¸ ì œëª©ì´ Noneì…ë‹ˆë‹¤. í”„ë¡ íŠ¸ì—”ë“œì—ì„œ fallback ì‚¬ìš©ë  ê²ƒì…ë‹ˆë‹¤.")
        
        return ChatResponse(
            message=answer,
            sources=sources,
            title=title
        )
        
    except Exception as e:
        logger.error(f"Error in chat: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/health")
async def health_check():
    """ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸"""
    return {"status": "healthy", "service": "LLM API"}


@router.post("/clear-memory")
async def clear_memory(model_type: ModelType = "ollama"):
    """ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”"""
    try:
        # TODO: ë©”ëª¨ë¦¬ ì´ˆê¸°í™” ë¡œì§ êµ¬í˜„
        return {"message": "Memory cleared successfully"}
    except Exception as e:
        logger.error(f"Error clearing memory: {e}")
        raise HTTPException(status_code=500, detail=str(e))
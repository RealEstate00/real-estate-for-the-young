"""
Housing ChromaDB - All-in-One Vector Database
ì£¼íƒ ë°ì´í„° ë²¡í„°DB í†µí•© ëª¨ë“ˆ (ë¡œë“œ-ë¶„í• -ì„ë² ë”©-ì €ì¥)
"""

import hashlib
import logging
import pandas as pd
from pathlib import Path
from typing import Dict, Any, List, Optional, Union
from tqdm import tqdm

# External dependencies
import chromadb
from chromadb.config import Settings # ChromaDB ì„¤ì •
from sentence_transformers import SentenceTransformer # ì„ë² ë”© ëª¨ë¸
from langchain.text_splitter import RecursiveCharacterTextSplitter # ë¬¸ì„œ ë¶„í• 

logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO)

# ============================================================================
# 1. ì„¤ì • (Configuration)
# ============================================================================

class VectorConfig:
    """ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì •"""
    
    def __init__(self):
        # í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì„¤ì • (Streamlit í˜¸í™˜)
        import sys
        from pathlib import Path
        
        # í˜„ì¬ íŒŒì¼ ìœ„ì¹˜ì—ì„œ í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì°¾ê¸°
        current_file = Path(__file__)
        # chromadb.py -> inha -> vector_db -> services -> backend -> project_root
        # ì‹¤ì œë¡œëŠ” 6ë‹¨ê³„ ìœ„ê°€ ì•„ë‹ˆë¼ 5ë‹¨ê³„ ìœ„ê°€ ì •í™•
        self.project_root = current_file.parent.parent.parent.parent.parent
        
        # ChromaDB ì„¤ì • (ì ˆëŒ€ ê²½ë¡œ)
        self.persist_directory = self.project_root / "backend" / "data" / "chroma_db" / "inha"
        
        # ì„ë² ë”© ëª¨ë¸ ì„¤ì •
        self.embedding_model = "jhgan/ko-sbert-nli"
        
        # ì»¬ë ‰ì…˜ ì„¤ì •
        self.housing_collection_name = "housing_embeddings"
        
        # ì„±ëŠ¥ ì„¤ì •
        self.batch_size = 32
        self.max_results = 50
        
        # ë””ë°”ì´ìŠ¤ ì„¤ì •
        self.device: Optional[str] = None
        
        # CSV íŒŒì¼ ê²½ë¡œ (ì ˆëŒ€ ê²½ë¡œ)
        self.default_csv_path = self.project_root / "backend" / "data" / "raw" / "for_vectorDB" / "housing_vector_data.csv"
        
        # ë¬¸ì„œ ë¶„í•  ì„¤ì •
        self.chunk_size = 500
        self.chunk_overlap = 50
        
        # í…Œë§ˆ í‚¤ì›Œë“œ
        self.theme_keywords = ["ì²­ë…„", "ì‹ í˜¼", "ìœ¡ì•„", "ì‹œë‹ˆì–´", "ì˜ˆìˆ ", "ë°˜ë ¤ë™ë¬¼", "ì—¬ì„±ì•ˆì‹¬"]
        
        # ì§€ì—­ í‚¤ì›Œë“œ íŒ¨í„´
        self.location_patterns = ["êµ¬", "ë™", "ì‹œ", "êµ°"]
        self.station_pattern = "ì—­"


# ============================================================================
# 2. ì„ë² ë”© (Korean Embeddings)
# ============================================================================

class KoreanEmbedder:
    """í•œêµ­ì–´ ì„ë² ë”© ì²˜ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self, model_name: str = None, config: VectorConfig = None):
        self.config = config or VectorConfig()
        self.model_name = model_name or self.config.embedding_model
        self.model = None
        
        # ë¬¸ì„œ ë¶„í• ê¸° ì´ˆê¸°í™” - ì¤„ë°”ê¿ˆ, ë¬¸ì¥ë¶€í˜¸, ì‰¼í‘œ, ê³µë°± ê¸°ì¤€
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=self.config.chunk_size,
            chunk_overlap=self.config.chunk_overlap,
            separators=["\n\n", "\n", ".", "!", "?", ",", " ", ""]
        )
        
    def load_model(self) -> None:
        """ì„ë² ë”© ëª¨ë¸ ë¡œë“œ"""
        if self.model is None:
            logger.info(f"Loading embedding model: {self.model_name}")
            self.model = SentenceTransformer(self.model_name)
            logger.info("Embedding model loaded successfully")
    
    def is_loaded(self) -> bool:
        """ëª¨ë¸ ë¡œë“œ ìƒíƒœ í™•ì¸"""
        return self.model is not None
    
    def encode_text(self, text: Union[str, List[str]]) -> Union[List[float], List[List[float]]]:
        """í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜"""
        if not self.is_loaded():
            self.load_model()
        
        is_single = isinstance(text, str)
        if is_single:
            text = [text]
        
        embeddings = self.model.encode(
            text, 
            convert_to_tensor=False, 
            normalize_embeddings=True
        ).tolist()
        
        return embeddings[0] if is_single else embeddings
    
    def create_housing_text(self, housing_data: Dict[str, Any]) -> str:
        """ì£¼íƒ ë°ì´í„°ë¥¼ ê²€ìƒ‰ìš© í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
        parts = []
        
        if housing_data.get('ì£¼íƒëª…'):
            parts.append(f"ì£¼íƒëª…: {housing_data['ì£¼íƒëª…']}")
        if housing_data.get('ë„ë¡œëª…ì£¼ì†Œ'):
            parts.append(f"ì£¼ì†Œ: {housing_data['ë„ë¡œëª…ì£¼ì†Œ']}")
        if housing_data.get('ì‹œêµ°êµ¬'):
            parts.append(f"ì§€ì—­: {housing_data['ì‹œêµ°êµ¬']}")
        if housing_data.get('ë™ëª…'):
            parts.append(f"ë™: {housing_data['ë™ëª…']}")
        if housing_data.get('íƒœê·¸'):
            parts.append(f"íŠ¹ì„±: {housing_data['íƒœê·¸']}")
        
        return " ".join(parts)
    
    def split_text(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• """
        if not text or len(text.strip()) == 0:
            return [text]
        
        # ì§§ì€ í…ìŠ¤íŠ¸ëŠ” ë¶„í• í•˜ì§€ ì•ŠìŒ
        if len(text) <= self.config.chunk_size:
            return [text]
        
        # ê¸´ í…ìŠ¤íŠ¸ëŠ” ë¶„í• 
        chunks = self.text_splitter.split_text(text)
        logger.debug(f"Split text into {len(chunks)} chunks")
        return chunks
    
    def parse_tags(self, tags_string: str) -> Dict[str, str]:
        """íƒœê·¸ ë¬¸ìì—´ì„ êµ¬ì¡°í™”ëœ ë©”íƒ€ë°ì´í„°ë¡œ íŒŒì‹±"""
        parsed = {}
        if not tags_string:
            return parsed
        
        tag_parts = tags_string.split(', ')
        key_mapping = {
            'í…Œë§ˆ': 'theme', 'ì§€í•˜ì² ': 'subway', 'ìê²©ìš”ê±´': 'requirements',
            'ë§ˆíŠ¸': 'mart', 'ë³‘ì›': 'hospital', 'í•™êµ': 'school',
            'ì‹œì„¤': 'facilities', 'ì¹´í˜': 'cafe'
        }
        
        for part in tag_parts:
            if ':' in part:
                key, value = part.split(':', 1)
                key, value = key.strip(), value.strip()
                english_key = key_mapping.get(key, key)
                parsed[english_key] = value
        
        return parsed
    
    def validate_housing_data(self, housing_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ì£¼íƒ ë°ì´í„° ê²€ì¦ ë° ì •ë¦¬"""
        valid_data = []
        
        for i, record in enumerate(housing_data):
            try:
                # í•„ìˆ˜ í•„ë“œ í™•ì¸
                if not record.get('ì£¼íƒëª…'):
                    logger.warning(f"Record {i}: Missing ì£¼íƒëª…, skipping")
                    continue
                
                # ë¬¸ìì—´ í•„ë“œ ì •ë¦¬
                for field in ['ì£¼íƒëª…', 'ì§€ë²ˆì£¼ì†Œ', 'ë„ë¡œëª…ì£¼ì†Œ', 'ì‹œêµ°êµ¬', 'ë™ëª…', 'íƒœê·¸']:
                    if pd.isna(record.get(field)):
                        record[field] = ''
                    else:
                        record[field] = str(record[field]).strip()
                
                valid_data.append(record)
                
            except Exception as e:
                logger.warning(f"Record {i}: Validation error - {e}, skipping")
                continue
        
        logger.info(f"Validated {len(valid_data)} out of {len(housing_data)} records")
        return valid_data


# ============================================================================
# 3. ChromaDB ì»¬ë ‰ì…˜ ê´€ë¦¬ (Collections)
# ============================================================================

class HousingCollection:
    """ì£¼íƒ ë°ì´í„° ì»¬ë ‰ì…˜ ê´€ë¦¬"""
    
    def __init__(self, embedder: KoreanEmbedder = None, config: VectorConfig = None):
        self.config = config or VectorConfig()
        self.embedder = embedder or KoreanEmbedder(config=self.config)
        self.client = None
        self.collection = None
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        self.config.persist_directory.mkdir(parents=True, exist_ok=True)
    
    def connect(self) -> None:
        """ChromaDB ì—°ê²°"""
        if self.client is None:
            logger.info(f"Connecting to ChromaDB at {self.config.persist_directory}")
            self.client = chromadb.PersistentClient(
                path=str(self.config.persist_directory),
                settings=Settings(anonymized_telemetry=False, allow_reset=True)
            )
            logger.info("Connected to ChromaDB")
    
    def disconnect(self) -> None:
        """ChromaDB ì—°ê²° í•´ì œ"""
        if self.client:
            self.client = None
            self.collection = None
            logger.info("Disconnected from ChromaDB")
    
    def get_collection(self):
        """ì»¬ë ‰ì…˜ ê°€ì ¸ì˜¤ê¸° ë˜ëŠ” ìƒì„±"""
        if self.collection is None:
            self.connect()
            try:
                self.collection = self.client.get_collection(self.config.housing_collection_name)
                logger.info(f"Retrieved existing collection: {self.config.housing_collection_name}")
            except:
                # ì»¤ìŠ¤í…€ ì„ë² ë”© í•¨ìˆ˜ ìƒì„±
                from chromadb.utils import embedding_functions
                embedding_function = embedding_functions.SentenceTransformerEmbeddingFunction(
                    model_name=self.config.embedding_model
                )
                
                self.collection = self.client.create_collection(
                    name=self.config.housing_collection_name,
                    metadata={
                        "description": "Housing data embeddings with text chunking",
                        "model": self.config.embedding_model,
                        "chunk_size": self.config.chunk_size,
                        "chunk_overlap": self.config.chunk_overlap
                    },
                    embedding_function=embedding_function
                )
                logger.info(f"Created new collection: {self.config.housing_collection_name}")
        return self.collection
    
    def prepare_document(self, housing_data: Dict[str, Any], chunk_id: int = 0) -> Dict[str, Any]:
        """ì£¼íƒ ë°ì´í„°ë¥¼ ë²¡í„°DB ë¬¸ì„œë¡œ ì¤€ë¹„ (ì²­í‚¹ ì§€ì›)"""
        # ê³ ìœ  ID ìƒì„± (ì²­í¬ ID í¬í•¨)
        base_id = f"{housing_data.get('ì£¼íƒëª…', '')}-{housing_data.get('ë„ë¡œëª…ì£¼ì†Œ', '')}"
        doc_id = hashlib.md5(f"{base_id}-chunk-{chunk_id}".encode('utf-8')).hexdigest()
        
        # í…ìŠ¤íŠ¸ ìƒì„± ë° ë¶„í• 
        document_text = self.embedder.create_housing_text(housing_data)
        text_chunks = self.embedder.split_text(document_text)
        
        # í•´ë‹¹ ì²­í¬ ì„ íƒ (ì²­í¬ê°€ ì—†ìœ¼ë©´ ì›ë³¸ í…ìŠ¤íŠ¸)
        if chunk_id < len(text_chunks):
            chunk_text = text_chunks[chunk_id]
        else:
            chunk_text = document_text
        
        # ì„ë² ë”© ìƒì„±
        embedding = self.embedder.encode_text(chunk_text)
        
        # ë©”íƒ€ë°ì´í„° ì¤€ë¹„
        metadata = {
            "ì£¼íƒëª…": housing_data.get('ì£¼íƒëª…', ''),
            "ì§€ë²ˆì£¼ì†Œ": housing_data.get('ì§€ë²ˆì£¼ì†Œ', ''),
            "ë„ë¡œëª…ì£¼ì†Œ": housing_data.get('ë„ë¡œëª…ì£¼ì†Œ', ''),
            "ì‹œêµ°êµ¬": housing_data.get('ì‹œêµ°êµ¬', ''),
            "ë™ëª…": housing_data.get('ë™ëª…', ''),
            "íƒœê·¸": housing_data.get('íƒœê·¸', ''),
            "chunk_id": chunk_id,
            "total_chunks": len(text_chunks)
        }
        
        # íƒœê·¸ íŒŒì‹± ì¶”ê°€
        metadata.update(self.embedder.parse_tags(housing_data.get('íƒœê·¸', '')))
        
        return {
            'id': doc_id,
            'embedding': embedding,
            'metadata': metadata,
            'document': chunk_text
        }
    
    def add_documents(self, documents: List[Dict[str, Any]], batch_size: int = None) -> None:
        """ë¬¸ì„œë“¤ì„ ë²¡í„°DBì— ì¶”ê°€ (ì²­í‚¹ ì§€ì›)"""
        collection = self.get_collection()
        batch_size = batch_size or self.config.batch_size
        
        logger.info(f"Adding {len(documents)} documents to vector database with chunking")
        
        # ëª¨ë“  ì²­í¬ ì¤€ë¹„
        all_chunks = []
        for doc in documents:
            # í…ìŠ¤íŠ¸ ë¶„í• í•˜ì—¬ ì²­í¬ ê°œìˆ˜ í™•ì¸
            document_text = self.embedder.create_housing_text(doc)
            text_chunks = self.embedder.split_text(document_text)
            
            # ê° ì²­í¬ì— ëŒ€í•´ ë¬¸ì„œ ì¤€ë¹„
            for chunk_id in range(len(text_chunks)):
                chunk_doc = self.prepare_document(doc, chunk_id)
                all_chunks.append(chunk_doc)
        
        logger.info(f"Total chunks to process: {len(all_chunks)}")
        
        # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
        for i in range(0, len(all_chunks), batch_size):
            batch = all_chunks[i:i + batch_size]
            
            # ë°°ì¹˜ ë°ì´í„° ì¤€ë¹„
            ids, embeddings, metadatas, documents_text = [], [], [], []
            
            for chunk in batch:
                ids.append(chunk['id'])
                embeddings.append(chunk['embedding'])
                metadatas.append(chunk['metadata'])
                documents_text.append(chunk['document'])
            
            # ì»¬ë ‰ì…˜ì— ì¶”ê°€
            try:
                collection.add(
                    ids=ids,
                    embeddings=embeddings,
                    metadatas=metadatas,
                    documents=documents_text
                )
            except Exception as e:
                logger.error(f"Failed to add batch: {e}")
                continue
        
        logger.info(f"Successfully loaded {collection.count()} document chunks")
    
    def count(self) -> int:
        """ì»¬ë ‰ì…˜ ë‚´ ë¬¸ì„œ ìˆ˜ ë°˜í™˜"""
        collection = self.get_collection()
        return collection.count()
    
    def clear(self) -> None:
        """ì»¬ë ‰ì…˜ ì´ˆê¸°í™”"""
        self.connect()
        try:
            existing_collections = [col.name for col in self.client.list_collections()]
            if self.config.housing_collection_name in existing_collections:
                self.client.delete_collection(self.config.housing_collection_name)
                self.collection = None
                logger.info(f"Cleared collection: {self.config.housing_collection_name}")
            else:
                logger.info("Collection does not exist, nothing to clear")
        except Exception as e:
            logger.warning(f"Failed to clear collection: {e}")
    
    def get_statistics(self) -> Dict[str, Any]:
        """ì»¬ë ‰ì…˜ í†µê³„ ì •ë³´"""
        collection = self.get_collection()
        
        try:
            all_docs = collection.get(include=['metadatas'])
            
            if not all_docs['metadatas']:
                return {"total_count": 0}
            
            # í†µê³„ ë¶„ì„
            districts, themes, chunks_info = {}, {}, {}
            
            for metadata in all_docs['metadatas']:
                # ì§€ì—­ í†µê³„
                district = metadata.get('ì‹œêµ°êµ¬', '')
                if district:
                    districts[district] = districts.get(district, 0) + 1
                
                # í…Œë§ˆ í†µê³„
                theme = metadata.get('theme', '')
                if theme:
                    for part in theme.split():
                        themes[part] = themes.get(part, 0) + 1
                
                # ì²­í¬ í†µê³„
                chunk_id = metadata.get('chunk_id', 0)
                total_chunks = metadata.get('total_chunks', 1)
                chunks_info[f"chunk_{chunk_id}"] = chunks_info.get(f"chunk_{chunk_id}", 0) + 1
            
            return {
                "total_count": len(all_docs['metadatas']),
                "districts": dict(sorted(districts.items(), key=lambda x: x[1], reverse=True)),
                "themes": dict(sorted(themes.items(), key=lambda x: x[1], reverse=True)),
                "chunks_info": chunks_info
            }
            
        except Exception as e:
            logger.error(f"Failed to get statistics: {e}")
            raise


# ============================================================================
# 4. ë©”ì¸ ë²¡í„°DB í´ë˜ìŠ¤ (Main Vector Database)
# ============================================================================

class HousingVectorDB:
    """ì£¼íƒ ë²¡í„°DB ì˜¬ì¸ì› í´ë˜ìŠ¤ - ë¡œë“œ/ë¶„í• /ì„ë² ë”©/ì €ì¥"""
    
    def __init__(self):
        self.config = VectorConfig()
        self.embedder = KoreanEmbedder(config=self.config)
        self.collection = HousingCollection(self.embedder, self.config)

    def load_csv_data(self, csv_path: str = None) -> None:
        """CSV íŒŒì¼ì—ì„œ ì£¼íƒ ë°ì´í„° ë¡œë“œ ë° ì²­í‚¹"""
        csv_path = csv_path or self.config.default_csv_path
        logger.info(f"Loading data from {csv_path}")
        
        # CSV ì½ê¸°
        df = pd.read_csv(csv_path)
        df = df.dropna(subset=['ì£¼íƒëª…'])
        housing_data = df.to_dict('records')
        
        # ë°ì´í„° ê²€ì¦ ë° ì •ë¦¬
        valid_data = self.embedder.validate_housing_data(housing_data)
        
        # ë²¡í„°DBì— ë¡œë“œ (ì²­í‚¹ í¬í•¨)
        self.add_documents(valid_data)
    
    def add_documents(self, documents: List[Dict[str, Any]], batch_size: int = None) -> None:
        """ë¬¸ì„œë“¤ì„ ë²¡í„°DBì— ì¶”ê°€ (ì²­í‚¹ ì§€ì›)"""
        batch_size = batch_size or self.config.batch_size
        
        # ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
        if not self.embedder.is_loaded():
            self.embedder.load_model()
        
        logger.info(f"Adding {len(documents)} documents to vector database with chunking")
        
        with tqdm(total=len(documents), desc="Processing documents") as pbar:
            # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
            for i in range(0, len(documents), batch_size):
                batch = documents[i:i + batch_size]
                
                try:
                    self.collection.add_documents(batch, batch_size=len(batch))
                    pbar.update(len(batch))
                except Exception as e:
                    logger.error(f"Failed to add batch: {e}")
                    pbar.update(len(batch))
        
        logger.info(f"Successfully loaded {self.collection.count()} document chunks")
    
    def get_info(self) -> Dict[str, Any]:
        """ë°ì´í„°ë² ì´ìŠ¤ ì •ë³´"""
        self.collection.connect()
        collections = [col.name for col in self.collection.client.list_collections()]
        
        info = {
            "persist_directory": str(self.config.persist_directory),
            "embedding_model": self.config.embedding_model,
            "chunk_size": self.config.chunk_size,
            "chunk_overlap": self.config.chunk_overlap,
            "collections": collections,
            "total_collections": len(collections)
        }
        
        if self.config.housing_collection_name in collections:
            info["housing_collection"] = {
                "name": self.config.housing_collection_name,
                "count": self.collection.count()
            }
        
        return info
    
    def get_statistics(self) -> Dict[str, Any]:
        """ë°ì´í„°ë² ì´ìŠ¤ í†µê³„ ì •ë³´"""
        return self.collection.get_statistics()
    
    def clear_database(self) -> None:
        """ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”"""
        self.collection.clear()
        logger.info("Database cleared successfully")


# ============================================================================
# 5. í¸ì˜ í•¨ìˆ˜ë“¤ (Utility Functions)
# ============================================================================

def create_housing_vector_db(csv_path: str = None) -> HousingVectorDB:
    """ì£¼íƒ ë²¡í„°DB ìƒì„± í¸ì˜ í•¨ìˆ˜"""
    db = HousingVectorDB()
    if csv_path:
        db.load_csv_data(csv_path)
    return db

def get_default_config() -> VectorConfig:
    """ê¸°ë³¸ ì„¤ì • ë°˜í™˜"""
    return VectorConfig()


# ============================================================================
# 6. ë©”ì¸ ì‹¤í–‰ë¶€ (Main Execution)
# ============================================================================

if __name__ == "__main__":
    # ê¸°ë³¸ ë¡œê¹… ì„¤ì •
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    # ë²¡í„°DB ìƒì„± ë° í…ŒìŠ¤íŠ¸
    print("=" * 80)
    print("Housing Vector Database - All-in-One")
    print("=" * 80)
    
    try:
        # ë²¡í„°DB ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        db = HousingVectorDB()
        
        # ì •ë³´ ì¶œë ¥
        info = db.get_info()
        print(f"âœ“ Persist Directory: {info['persist_directory']}")
        print(f"âœ“ Embedding Model: {info['embedding_model']}")
        print(f"âœ“ Chunk Size: {info['chunk_size']}")
        print(f"âœ“ Chunk Overlap: {info['chunk_overlap']}")
        
        # CSV ë°ì´í„° ë¡œë“œ (íŒŒì¼ì´ ìˆëŠ” ê²½ìš°)
        config = get_default_config()
        if Path(config.default_csv_path).exists():
            print(f"\nğŸ“ Loading CSV data from: {config.default_csv_path}")
            db.load_csv_data()
            
            # í†µê³„ ì¶œë ¥
            stats = db.get_statistics()
            print(f"âœ“ Total document chunks: {stats['total_count']}")
            if stats.get('districts'):
                print(f"âœ“ Top districts: {list(stats['districts'].keys())[:3]}")
        else:
            print(f"\nâš ï¸  CSV file not found: {config.default_csv_path}")
            print("   Please provide CSV data to test the vector database.")
        
        print("\nâœ… Housing Vector Database initialized successfully!")
        
    except Exception as e:
        print(f"\nâŒ Error: {e}")
        logger.exception("Failed to initialize Housing Vector Database")

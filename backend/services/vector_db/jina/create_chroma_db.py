# backend/services/vector_db/jina/create_chroma_db.py
"""
ChromaDB Vector Database Creator
CSV ÌååÏùºÏùÑ ÏùΩÏñ¥ ChromaDB Î≤°ÌÑ∞ Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ ÏÉùÏÑ±
"""

import pandas as pd
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document
from pathlib import Path
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


# =============================================================================
# 1. ChromaDB Creator Class
# =============================================================================

class ChromaDBCreator:
    """
    CSV Îç∞Ïù¥ÌÑ∞Î•º ChromaDB Î≤°ÌÑ∞ Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§Î°ú Î≥ÄÌôò
    """
    
    def __init__(
        self,
        csv_path: str,
        embedding_model: str = "jhgan/ko-sbert-nli",
        chunk_size: int = 500,
        chunk_overlap: int = 50,
        collection_name: str = "housing_data"
    ):
        """
        Initialize ChromaDB Creator
        
        Args:
            csv_path: Path to input CSV file
            embedding_model: Korean embedding model name
            chunk_size: Maximum chunk size for text splitting
            chunk_overlap: Overlap between chunks
            collection_name: ChromaDB collection name
        """
        self.csv_path = csv_path
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.collection_name = collection_name
        
        # -----------------------------
        # 1.1. Initialize Embedding Model
        # -----------------------------
        logger.info(f"Loading embedding model: {embedding_model}")
        self.embeddings = HuggingFaceEmbeddings(
            model_name=embedding_model,
            model_kwargs={'device': 'cpu'},
            encode_kwargs={'normalize_embeddings': True}
        )
        
        # -----------------------------
        # 1.2. Initialize Text Splitter
        # -----------------------------
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            separators=["\n\n", "\n", ". ", " ", ""]
        )
    
    # =============================================================================
    # 2. Data Loading
    # =============================================================================
    
    def load_csv(self) -> pd.DataFrame:
        """
        Load CSV file
        
        Returns:
            DataFrame containing housing data
        """
        logger.info(f"Loading CSV: {self.csv_path}")
        df = pd.read_csv(self.csv_path)
        logger.info(f"Loaded {len(df)} records")
        return df
    
    # =============================================================================
    # 3. Document Creation
    # =============================================================================
    
    def create_documents(self, df: pd.DataFrame) -> list[Document]:
        """
        Convert DataFrame rows to LangChain Documents
        
        Args:
            df: Input DataFrame
            
        Returns:
            List of Document objects
        """
        documents = []
        
        for idx, row in df.iterrows():
            # Create searchable text content
            content = self._create_content(row)
            
            # Create metadata for filtering
            metadata = self._create_metadata(row, idx)
            
            # Create Document object
            doc = Document(
                page_content=content,
                metadata=metadata
            )
            documents.append(doc)
        
        logger.info(f"Created {len(documents)} documents")
        return documents
    
    def _create_content(self, row: pd.Series) -> str:
        """
        Convert CSV row to searchable text
        
        Args:
            row: DataFrame row
            
        Returns:
            Formatted text string
        """
        parts = []
        
        if pd.notna(row.get('Ï£ºÌÉùÎ™Ö')):
            parts.append(f"Ï£ºÌÉùÎ™Ö: {row['Ï£ºÌÉùÎ™Ö']}")
        
        if pd.notna(row.get('ÏßÄÎ≤àÏ£ºÏÜå')):
            parts.append(f"ÏßÄÎ≤àÏ£ºÏÜå: {row['ÏßÄÎ≤àÏ£ºÏÜå']}")
        
        if pd.notna(row.get('ÎèÑÎ°úÎ™ÖÏ£ºÏÜå')):
            parts.append(f"ÎèÑÎ°úÎ™ÖÏ£ºÏÜå: {row['ÎèÑÎ°úÎ™ÖÏ£ºÏÜå']}")
        
        if pd.notna(row.get('ÏãúÍµ∞Íµ¨')):
            parts.append(f"ÏãúÍµ∞Íµ¨: {row['ÏãúÍµ∞Íµ¨']}")
        
        if pd.notna(row.get('ÎèôÎ™Ö')):
            parts.append(f"ÎèôÎ™Ö: {row['ÎèôÎ™Ö']}")
        
        if pd.notna(row.get('ÌÉúÍ∑∏')):
            parts.append(f"ÌÉúÍ∑∏: {row['ÌÉúÍ∑∏']}")
        
        return "\n".join(parts)
    
    def _create_metadata(self, row: pd.Series, idx: int) -> dict:
        """
        Create metadata for filtering and display
        
        Args:
            row: DataFrame row
            idx: Row index
            
        Returns:
            Metadata dictionary
        """
        metadata = {
            'row_id': int(idx),
            'source': 'housing_vector_data.csv'
        }
        
        # Add all columns as metadata (ChromaDB requires specific types)
        key_fields = ['Ï£ºÌÉùÎ™Ö', 'ÏßÄÎ≤àÏ£ºÏÜå', 'ÎèÑÎ°úÎ™ÖÏ£ºÏÜå', 'ÏãúÍµ∞Íµ¨', 'ÎèôÎ™Ö', 'ÌÉúÍ∑∏']
        
        for field in key_fields:
            if field in row and pd.notna(row[field]):
                metadata[field] = str(row[field])
        
        return metadata
    
    # =============================================================================
    # 4. Vector DB Creation
    # =============================================================================
    
    def create_vector_db(self, persist_directory: str = None) -> Chroma:
        """
        Create ChromaDB vector database from CSV
        
        Args:
            persist_directory: Directory to save ChromaDB data
            
        Returns:
            Chroma vector database instance
        """
        
        # -----------------------------
        # 4.1. Load CSV Data
        # -----------------------------
        df = self.load_csv()
        
        # -----------------------------
        # 4.2. Create Documents
        # -----------------------------
        documents = self.create_documents(df)
        
        # -----------------------------
        # 4.3. Split Text into Chunks
        # -----------------------------
        logger.info("Splitting documents into chunks...")
        split_docs = self.text_splitter.split_documents(documents)
        logger.info(f"Created {len(split_docs)} text chunks")
        
        # -----------------------------
        # 4.4. Create Vector DB
        # -----------------------------
        if persist_directory is None:
            persist_directory = "backend/data/vector_db/chroma"
        
        logger.info("Creating ChromaDB vector database...")
        logger.info(f"This may take a few minutes for embedding generation...")
        
        vector_db = Chroma.from_documents(
            documents=split_docs,
            embedding=self.embeddings,
            collection_name=self.collection_name,
            persist_directory=persist_directory
        )
        
        logger.info(f"‚úÖ Vector DB saved to: {persist_directory}")
        
        return vector_db


# =============================================================================
# 5. Main Execution
# =============================================================================

def main():
    """Main execution function"""
    
    # -----------------------------
    # 5.1. Set Paths
    # -----------------------------
    base_path = Path(__file__).parent.parent.parent.parent
    csv_path = base_path / "data" / "vector_db" / "housing_vector_data.csv"
    persist_directory = base_path / "data" / "vector_db" / "chroma"
    
    print("=" * 80)
    print("ChromaDB Vector Database Creator")
    print("=" * 80)
    print(f"Input CSV: {csv_path}")
    print(f"Output DB: {persist_directory}")
    print("=" * 80)
    
    # Check if CSV exists
    if not csv_path.exists():
        print(f"\n‚ùå Error: CSV file not found at {csv_path}")
        print("Please run create_vector_db_csv.py first!")
        return
    
    # -----------------------------
    # 5.2. Create Vector DB
    # -----------------------------
    creator = ChromaDBCreator(
        csv_path=str(csv_path),
        collection_name="housing_data"
    )
    
    vector_db = creator.create_vector_db(str(persist_directory))
    
    # -----------------------------
    # 5.3. Test Search
    # -----------------------------
    print("\n" + "=" * 80)
    print("Test Search")
    print("=" * 80)
    
    test_queries = [
        "Í∞ïÎÇ® Ï≤≠ÎÖÑÏ£ºÌÉù",
        "ÏßÄÌïòÏ≤†Ïó≠ Í∑ºÏ≤ò",
        "Í≥µÏõê"
    ]
    
    for query in test_queries:
        print(f"\nüîç Query: '{query}'")
        results = vector_db.similarity_search(query, k=3)
        
        for i, doc in enumerate(results, 1):
            print(f"\n  [{i}] {doc.metadata.get('Ï£ºÌÉùÎ™Ö', 'N/A')}")
            print(f"      Ï£ºÏÜå: {doc.metadata.get('ÏßÄÎ≤àÏ£ºÏÜå', 'N/A')[:60]}...")
    
    print("\n" + "=" * 80)
    print("‚úÖ ChromaDB creation complete!")
    print("=" * 80)


if __name__ == "__main__":
    main()
"""
RAG ì„±ëŠ¥ í‰ê°€ê¸°

ë‹¤ì–‘í•œ ëª¨ë¸ì˜ ê²€ìƒ‰ ì„±ëŠ¥ì„ í‰ê°€í•˜ê³  ë¹„êµí•©ë‹ˆë‹¤.
- í‘œì¤€ ê²€ìƒ‰ ë©”íŠ¸ë¦­ (Precision@K, Recall@K, MRR, NDCG)
- Latency ë©”íŠ¸ë¦­ (percentiles)
- í•œêµ­ì–´ ì´í•´ë„ ë©”íŠ¸ë¦­
- ë¦¬ë­í‚¹ ì „í›„ ì„±ëŠ¥ ë¹„êµ
"""

import time
import logging
import json
from pathlib import Path
from typing import List, Dict, Any, Optional
from datetime import datetime

from ..config import EmbeddingModelType
from .search import VectorRetriever
from .metrics import MetricsCalculator

logger = logging.getLogger(__name__)


class RAGEvaluator:
    """RAG ì‹œìŠ¤í…œì˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ëŠ” í´ë˜ìŠ¤"""

    def __init__(self, db_config: Dict[str, str] = None):
        self.db_config = db_config or {
            'host': 'localhost',
            'port': '5432',
            'database': 'rey',
            'user': 'postgres',
            'password': 'post1234'
        }

        self.metrics_calculator = MetricsCalculator()

        # í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ë“¤ (test_queries.txtì—ì„œ ë¡œë“œ)
        self.test_queries, self.expected_keywords = self._load_test_queries()

    def _load_test_queries(self) -> tuple[List[str], Dict[str, List[str]]]:
        """
        test_queries.txt íŒŒì¼ì—ì„œ í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ì™€ ì˜ˆìƒ í‚¤ì›Œë“œë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.

        Returns:
            (ì¿¼ë¦¬ ë¦¬ìŠ¤íŠ¸, ì¿¼ë¦¬ë³„ ì˜ˆìƒ í‚¤ì›Œë“œ ë”•ì…”ë„ˆë¦¬)
        """
        try:
            queries_file = Path(__file__).parent.parent / "cli" / "test_queries.txt"
            queries = []
            expected_keywords = {}

            if queries_file.exists():
                with open(queries_file, 'r', encoding='utf-8') as f:
                    for line in f:
                        line = line.strip()
                        # ì£¼ì„ì´ë‚˜ ë¹ˆ ì¤„ ê±´ë„ˆë›°ê¸°
                        if not line or line.startswith('#'):
                            continue

                        # "ì¿¼ë¦¬|í‚¤ì›Œë“œ1,í‚¤ì›Œë“œ2,..." í˜•ì‹ íŒŒì‹±
                        if '|' in line:
                            query, keywords_str = line.split('|', 1)
                            query = query.strip()
                            keywords = [kw.strip() for kw in keywords_str.split(',')]
                            queries.append(query)
                            expected_keywords[query] = keywords
                        else:
                            # êµ¬ë²„ì „ í˜¸í™˜ì„± - í‚¤ì›Œë“œ ì—†ì´ ì¿¼ë¦¬ë§Œ
                            queries.append(line)

                logger.info(f"í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ {len(queries)}ê°œ ë¡œë“œë¨ (í‚¤ì›Œë“œ ì •ì˜: {len(expected_keywords)}ê°œ)")
                return queries, expected_keywords
            else:
                logger.warning("test_queries.txt íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ì¿¼ë¦¬ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                default_queries = [
                    "ì‹ í˜¼ë¶€ë¶€ ì„ì°¨ë³´ì¦ê¸ˆ ì´ìì§€ì›",
                    "ëŒ€ì¶œ í•œë„ì™€ ì†Œë“ ê¸°ì¤€",
                    "ì‹ ì²­ ì ˆì°¨ì™€ í•„ìš” ì„œë¥˜"
                ]
                return default_queries, {}

        except Exception as e:
            logger.error(f"í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return ["ì‹ í˜¼ë¶€ë¶€ ì„ì°¨ë³´ì¦ê¸ˆ ì´ìì§€ì›"], {}

    def evaluate_model(
        self,
        model_type: EmbeddingModelType,
        queries: List[str] = None,
        top_k: int = 5,
        use_reranking: bool = False,
        save_search_results: bool = False
    ) -> Dict[str, Any]:
        """
        ë‹¨ì¼ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•©ë‹ˆë‹¤.

        Args:
            model_type: í‰ê°€í•  ëª¨ë¸ íƒ€ì…
            queries: í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ (Noneì´ë©´ ê¸°ë³¸ ì¿¼ë¦¬ ì‚¬ìš©)
            top_k: ê²€ìƒ‰í•  ê²°ê³¼ ìˆ˜
            use_reranking: ë¦¬ë­í‚¹ ì‚¬ìš© ì—¬ë¶€
            save_search_results: ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì €ì¥í• ì§€ ì—¬ë¶€ (False=ë©”íŠ¸ë¦­ë§Œ, True=ì „ì²´)

        Returns:
            í‰ê°€ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        if queries is None:
            queries = self.test_queries

        logger.info(f"ëª¨ë¸ {model_type.value} ì„±ëŠ¥ í‰ê°€ ì‹œì‘ (ë¦¬ë­í‚¹: {use_reranking})")

        retriever = VectorRetriever(model_type, self.db_config)

        try:
            # ê²€ìƒ‰ ìˆ˜í–‰
            search_results = {}
            for query in queries:
                logger.info(f"ì¿¼ë¦¬ ê²€ìƒ‰ ì¤‘: {query}")
                start_time = time.time()

                if use_reranking:
                    results = retriever.search_with_reranking(
                        query=query,
                        top_k=top_k,
                        rerank_top_k=top_k * 4  # 4ë°° ë§ì´ ê°€ì ¸ì™€ì„œ ë¦¬ë­í‚¹
                    )
                else:
                    results = retriever.search(
                        query=query,
                        top_k=top_k
                    )

                search_time = (time.time() - start_time) * 1000  # ms

                search_results[query] = {
                    'results': results,
                    'search_time_ms': search_time,
                    'result_count': len(results)
                }

            # ì„±ëŠ¥ ì§€í‘œ ê³„ì‚° (ì˜ˆìƒ í‚¤ì›Œë“œ í¬í•¨)
            metrics = self.metrics_calculator.calculate_metrics(
                search_results,
                expected_keywords=self.expected_keywords if self.expected_keywords else None
            )

            # ê²°ê³¼ ì •ë¦¬
            evaluation_result = {
                'model_name': model_type.value,
                'total_queries': len(queries),
                'successful_queries': metrics.successful_queries,
                'use_reranking': use_reranking,
                'metrics': self.metrics_calculator.to_dict(metrics),
                'timestamp': datetime.now().isoformat()
            }

            # ê²€ìƒ‰ ê²°ê³¼ ì €ì¥ ì˜µì…˜ (ê¸°ë³¸: ë©”íŠ¸ë¦­ë§Œ ì €ì¥)
            if save_search_results:
                evaluation_result['search_results'] = search_results
            else:
                # ê°„ë‹¨í•œ ìš”ì•½ë§Œ ì €ì¥
                evaluation_result['search_summary'] = {
                    'total_queries': len(search_results),
                    'avg_result_count': sum(r['result_count'] for r in search_results.values()) / len(search_results)
                }

            logger.info(f"ëª¨ë¸ {model_type.value} í‰ê°€ ì™„ë£Œ")

            # ë©”íŠ¸ë¦­ ì¶œë ¥
            self.metrics_calculator.print_metrics(metrics, model_type.value)

            return evaluation_result

        except Exception as e:
            logger.error(f"ëª¨ë¸ {model_type.value} í‰ê°€ ì‹¤íŒ¨: {e}", exc_info=True)
            return {
                'model_name': model_type.value,
                'error': str(e),
                'status': 'failed',
                'timestamp': datetime.now().isoformat()
            }
        finally:
            retriever.close()

    def evaluate_all_models(
        self,
        models: List[EmbeddingModelType],
        queries: List[str] = None,
        top_k: int = 5,
        compare_reranking: bool = False,
        save_search_results: bool = False
    ) -> Dict[str, Any]:
        """
        ì—¬ëŸ¬ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ê³  ë¹„êµí•©ë‹ˆë‹¤.

        Args:
            models: í‰ê°€í•  ëª¨ë¸ íƒ€ì… ë¦¬ìŠ¤íŠ¸
            queries: í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬
            top_k: ê²€ìƒ‰í•  ê²°ê³¼ ìˆ˜
            compare_reranking: ë¦¬ë­í‚¹ ì „í›„ ì„±ëŠ¥ ë¹„êµ ì—¬ë¶€
            save_search_results: ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì €ì¥í• ì§€ ì—¬ë¶€ (ê¸°ë³¸: False)

        Returns:
            ì¢…í•© í‰ê°€ ê²°ê³¼
        """
        if queries is None:
            queries = self.test_queries

        logger.info(f"{len(models)}ê°œ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ì‹œì‘")

        results = {}

        for model_type in models:
            try:
                # ê¸°ë³¸ ê²€ìƒ‰ í‰ê°€
                result = self.evaluate_model(
                    model_type, queries, top_k,
                    use_reranking=False,
                    save_search_results=save_search_results
                )
                results[f"{model_type.value}_baseline"] = result

                # ë¦¬ë­í‚¹ í‰ê°€ (ì˜µì…˜)
                if compare_reranking:
                    result_reranked = self.evaluate_model(
                        model_type, queries, top_k,
                        use_reranking=True,
                        save_search_results=save_search_results
                    )
                    results[f"{model_type.value}_reranked"] = result_reranked

            except Exception as e:
                logger.error(f"ëª¨ë¸ {model_type.value} í‰ê°€ ì‹¤íŒ¨: {e}")
                results[model_type.value] = {
                    'model_name': model_type.value,
                    'error': str(e),
                    'status': 'failed'
                }

        # ëª¨ë¸ ë¹„êµ
        comparison = self._compare_models(results)

        # ë¦¬ë­í‚¹ íš¨ê³¼ ë¹„êµ
        reranking_comparison = None
        if compare_reranking:
            reranking_comparison = self._compare_reranking_effect(results)

        return {
            'evaluation_results': results,
            'comparison': comparison,
            'reranking_comparison': reranking_comparison,
            'summary': self._generate_summary(results, comparison),
            'timestamp': datetime.now().isoformat()
        }

    def _compare_models(self, results: Dict[str, Any]) -> Dict[str, Any]:
        """ëª¨ë¸ë“¤ì„ ë¹„êµí•©ë‹ˆë‹¤."""

        successful_models = {k: v for k, v in results.items() if 'metrics' in v}

        if not successful_models:
            return {'error': 'No successful evaluations to compare'}

        # ì£¼ìš” ë©”íŠ¸ë¦­ ì¶”ì¶œ
        comparisons = {
            'latency': {},
            'accuracy': {},
            'recall': {},
            'mrr': {},
            'ndcg': {}
        }

        for model_name, result in successful_models.items():
            metrics = result['metrics']

            # Latency
            if metrics.get('latency_metrics'):
                lm = metrics['latency_metrics']
                comparisons['latency'][model_name] = {
                    'avg': lm['avg_latency_ms'],
                    'p95': lm['p95_latency_ms'],
                    'p99': lm['p99_latency_ms']
                }

            # Accuracy (ìœ ì‚¬ë„)
            comparisons['accuracy'][model_name] = metrics.get('avg_similarity', 0)

            # Recall, MRR, NDCG (í‘œì¤€ ë©”íŠ¸ë¦­ì´ ìˆëŠ” ê²½ìš°)
            if metrics.get('standard_metrics'):
                sm = metrics['standard_metrics']
                comparisons['recall'][model_name] = sm.get('recall_at_5', 0)
                comparisons['mrr'][model_name] = sm.get('mrr', 0)
                comparisons['ndcg'][model_name] = sm.get('ndcg_at_5', 0)

        # ìˆœìœ„ ê³„ì‚°
        rankings = {}
        rankings['fastest'] = sorted(comparisons['latency'].items(),
                                     key=lambda x: x[1]['avg'])[0][0] if comparisons['latency'] else None
        rankings['most_accurate'] = sorted(comparisons['accuracy'].items(),
                                          key=lambda x: x[1], reverse=True)[0][0] if comparisons['accuracy'] else None
        rankings['best_recall'] = sorted(comparisons['recall'].items(),
                                        key=lambda x: x[1], reverse=True)[0][0] if comparisons['recall'] else None
        rankings['best_mrr'] = sorted(comparisons['mrr'].items(),
                                     key=lambda x: x[1], reverse=True)[0][0] if comparisons['mrr'] else None

        return {
            'rankings': rankings,
            'comparisons': comparisons
        }

    def _compare_reranking_effect(self, results: Dict[str, Any]) -> Dict[str, Any]:
        """ë¦¬ë­í‚¹ ì „í›„ ì„±ëŠ¥ ë¹„êµ"""

        reranking_effects = {}

        for key, result in results.items():
            if '_baseline' in key:
                model_name = key.replace('_baseline', '')
                reranked_key = f"{model_name}_reranked"

                if reranked_key in results and 'metrics' in result and 'metrics' in results[reranked_key]:
                    baseline_metrics = result['metrics']
                    reranked_metrics = results[reranked_key]['metrics']

                    # ê°œì„  ì •ë„ ê³„ì‚°
                    improvements = {}

                    # ìœ ì‚¬ë„ ê°œì„ 
                    baseline_sim = baseline_metrics.get('avg_similarity', 0)
                    reranked_sim = reranked_metrics.get('avg_similarity', 0)
                    improvements['similarity'] = ((reranked_sim - baseline_sim) / baseline_sim * 100) if baseline_sim > 0 else 0

                    # Latency ë³€í™”
                    if baseline_metrics.get('latency_metrics') and reranked_metrics.get('latency_metrics'):
                        baseline_lat = baseline_metrics['latency_metrics']['avg_latency_ms']
                        reranked_lat = reranked_metrics['latency_metrics']['avg_latency_ms']
                        improvements['latency_overhead'] = reranked_lat - baseline_lat

                    # í‘œì¤€ ë©”íŠ¸ë¦­ ê°œì„ 
                    if baseline_metrics.get('standard_metrics') and reranked_metrics.get('standard_metrics'):
                        baseline_sm = baseline_metrics['standard_metrics']
                        reranked_sm = reranked_metrics['standard_metrics']

                        improvements['recall_at_5'] = (reranked_sm.get('recall_at_5', 0) -
                                                      baseline_sm.get('recall_at_5', 0))
                        improvements['ndcg_at_5'] = (reranked_sm.get('ndcg_at_5', 0) -
                                                    baseline_sm.get('ndcg_at_5', 0))

                    reranking_effects[model_name] = improvements

        return reranking_effects

    def _generate_summary(self, results: Dict[str, Any], comparison: Dict[str, Any]) -> str:
        """í‰ê°€ ê²°ê³¼ ìš”ì•½ì„ ìƒì„±í•©ë‹ˆë‹¤."""

        successful_count = len([r for r in results.values() if 'metrics' in r])
        failed_count = len(results) - successful_count

        rankings = comparison.get('rankings', {})

        summary = f"""
ğŸ† RAG ì„±ëŠ¥ í‰ê°€ ê²°ê³¼ ìš”ì•½
{'='*50}
ğŸ“Š í‰ê°€ëœ ëª¨ë¸: {len(results)}ê°œ
âœ… ì„±ê³µ: {successful_count}ê°œ
âŒ ì‹¤íŒ¨: {failed_count}ê°œ

ğŸƒâ€â™‚ï¸ ê°€ì¥ ë¹ ë¥¸ ëª¨ë¸: {rankings.get('fastest', 'N/A')}
ğŸ¯ ê°€ì¥ ì •í™•í•œ ëª¨ë¸: {rankings.get('most_accurate', 'N/A')}
ğŸ” Best Recall: {rankings.get('best_recall', 'N/A')}
â­ Best MRR: {rankings.get('best_mrr', 'N/A')}
"""
        return summary

    def save_results(self, results: Dict[str, Any], output_path: Optional[Path] = None) -> Path:
        """í‰ê°€ ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""

        if output_path is None:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            output_dir = Path(__file__).parent.parent / "results"
            output_dir.mkdir(parents=True, exist_ok=True)
            output_path = output_dir / f"evaluation_{timestamp}.json"

        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2, default=str)

        logger.info(f"í‰ê°€ ê²°ê³¼ ì €ì¥: {output_path}")
        return output_path

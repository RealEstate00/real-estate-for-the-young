"""
ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°ê¸°

ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë¶„ì„í•˜ì—¬ ë‹¤ì–‘í•œ ì„±ëŠ¥ ì§€í‘œë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
- ê¸°ë³¸ ê²€ìƒ‰ ë©”íŠ¸ë¦­ (ìœ ì‚¬ë„, ê²€ìƒ‰ ì‹œê°„)
- ì •ë³´ ê²€ìƒ‰ í‘œì¤€ ë©”íŠ¸ë¦­ (Precision@K, Recall@K, F1@K, MRR, NDCG)
- í•œêµ­ì–´ ì´í•´ë„ ë©”íŠ¸ë¦­ (í‚¤ì›Œë“œ ì •í™•ë„, ë„ë©”ì¸ íŠ¹í™”ì„±)
- Latency ë©”íŠ¸ë¦­ (percentiles)
"""

import logging
import math
import numpy as np
from typing import List, Dict, Any, Optional, Set
from statistics import mean, median, stdev
from dataclasses import dataclass, asdict

logger = logging.getLogger(__name__)


@dataclass
class StandardMetrics:
    """ì •ë³´ ê²€ìƒ‰ í‘œì¤€ ë©”íŠ¸ë¦­"""
    # Precision, Recall, F1
    precision_at_1: float = 0.0
    precision_at_3: float = 0.0
    precision_at_5: float = 0.0
    recall_at_1: float = 0.0
    recall_at_3: float = 0.0
    recall_at_5: float = 0.0
    f1_at_1: float = 0.0
    f1_at_3: float = 0.0
    f1_at_5: float = 0.0

    # MRR & NDCG
    mrr: float = 0.0  # Mean Reciprocal Rank
    ndcg_at_3: float = 0.0  # Normalized Discounted Cumulative Gain
    ndcg_at_5: float = 0.0

    # Hit Rate
    hit_rate_at_1: float = 0.0
    hit_rate_at_3: float = 0.0
    hit_rate_at_5: float = 0.0


@dataclass
class LatencyMetrics:
    """Latency í†µê³„ ë©”íŠ¸ë¦­"""
    avg_latency_ms: float = 0.0
    median_latency_ms: float = 0.0
    min_latency_ms: float = 0.0
    max_latency_ms: float = 0.0
    stddev_latency_ms: float = 0.0
    p50_latency_ms: float = 0.0  # 50th percentile
    p95_latency_ms: float = 0.0  # 95th percentile
    p99_latency_ms: float = 0.0  # 99th percentile


@dataclass
class KoreanMetrics:
    """í•œêµ­ì–´ íŠ¹í™” ë©”íŠ¸ë¦­"""
    keyword_precision: float = 0.0
    keyword_recall: float = 0.0
    keyword_f1: float = 0.0
    domain_specificity: float = 0.0
    semantic_coherence: float = 0.0


@dataclass
class ComprehensiveMetrics:
    """ì¢…í•© ë©”íŠ¸ë¦­"""
    # ê¸°ë³¸ í†µê³„
    total_queries: int = 0
    successful_queries: int = 0
    success_rate: float = 0.0
    total_results: int = 0
    avg_result_count: float = 0.0

    # ìœ ì‚¬ë„ ë©”íŠ¸ë¦­
    avg_similarity: float = 0.0
    min_similarity: float = 0.0
    max_similarity: float = 0.0
    stddev_similarity: float = 0.0

    # í‘œì¤€ ë©”íŠ¸ë¦­
    standard_metrics: Optional[StandardMetrics] = None

    # Latency ë©”íŠ¸ë¦­
    latency_metrics: Optional[LatencyMetrics] = None

    # í•œêµ­ì–´ ë©”íŠ¸ë¦­
    korean_metrics: Optional[KoreanMetrics] = None


class MetricsCalculator:
    """ì„±ëŠ¥ ì§€í‘œë¥¼ ê³„ì‚°í•˜ëŠ” í´ë˜ìŠ¤"""

    def __init__(self):
        self.korean_keywords = [
            'ì‹ í˜¼ë¶€ë¶€', 'ì„ì°¨ë³´ì¦ê¸ˆ', 'ì´ìì§€ì›', 'ëŒ€ì¶œ', 'í•œë„', 'ì†Œë“', 'ê¸°ì¤€',
            'ì‹ ì²­', 'ì ˆì°¨', 'ì„œë¥˜', 'ê¸ˆë¦¬', 'ê¸°ê°„', 'ëŒ€ìƒì£¼íƒ', 'ì¡°ê±´', 'ì œì™¸ì‚¬í•­',
            'ë°˜í™˜ë³´ì¦', 'ë³´ì¦ë£Œ', 'ì—°ì¥', 'ì¤‘ë‹¨', 'ì˜ˆë¹„ì‹ í˜¼ë¶€ë¶€', 'ìê²©', 'ì²­ë…„',
            'ì „ì„¸', 'ì›”ì„¸', 'ë²„íŒ€ëª©', 'ì£¼ê±°ê¸‰ì—¬', 'ì„œìš¸í˜•', 'ë°”ìš°ì²˜', 'ê¸´ê¸‰', 'ë³µì§€'
        ]

    def _safe_stdev(self, values: List[float]) -> float:
        """ì•ˆì „í•œ í‘œì¤€í¸ì°¨ ê³„ì‚° (NaN/Inf ê°’ ì²˜ë¦¬)"""
        if not values or len(values) < 2:
            return 0.0
        
        # NaN, Inf ê°’ í•„í„°ë§
        clean_values = []
        for v in values:
            if isinstance(v, (int, float)) and not (math.isnan(v) or math.isinf(v)):
                clean_values.append(v)
        
        if len(clean_values) < 2:
            return 0.0
            
        try:
            return stdev(clean_values)
        except (ValueError, TypeError, AttributeError):
            # stdev í•¨ìˆ˜ì—ì„œ ì˜¤ë¥˜ ë°œìƒ ì‹œ numpy ì‚¬ìš©
            return float(np.std(clean_values))

    def calculate_metrics(
        self,
        search_results: Dict[str, Dict[str, Any]],
        expected_keywords: Optional[Dict[str, List[str]]] = None
    ) -> ComprehensiveMetrics:
        """
        ê²€ìƒ‰ ê²°ê³¼ë¡œë¶€í„° ì¢…í•© ì„±ëŠ¥ ì§€í‘œë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.

        Args:
            search_results: ê²€ìƒ‰ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ {query: {results, search_time_ms, result_count}}
            expected_keywords: ì¿¼ë¦¬ë³„ ì˜ˆìƒ í‚¤ì›Œë“œ ë”•ì…”ë„ˆë¦¬ {query: [keywords]}

        Returns:
            ì¢…í•© ë©”íŠ¸ë¦­
        """
        if not search_results:
            return ComprehensiveMetrics()

        # ê¸°ë³¸ í†µê³„
        search_times = [result['search_time_ms'] for result in search_results.values()]
        result_counts = [result['result_count'] for result in search_results.values()]

        # ìœ ì‚¬ë„ ê³„ì‚° (ê²°ê³¼ê°€ ìˆëŠ” ê²½ìš°, NaN/Inf í•„í„°ë§)
        similarities = []
        for result in search_results.values():
            if result['results']:
                for r in result['results']:
                    sim = r.get('similarity', 0)
                    # NaN, Inf ê°’ í•„í„°ë§
                    if isinstance(sim, (int, float)) and not (math.isnan(sim) or math.isinf(sim)):
                        similarities.append(sim)

        # ê¸°ë³¸ ë©”íŠ¸ë¦­
        total_queries = len(search_results)
        successful_queries = len([r for r in result_counts if r > 0])
        success_rate = successful_queries / total_queries if total_queries else 0

        # Latency ë©”íŠ¸ë¦­ ê³„ì‚°
        latency_metrics = self._calculate_latency_metrics(search_times)

        # í‘œì¤€ ë©”íŠ¸ë¦­ ê³„ì‚° (ì˜ˆìƒ í‚¤ì›Œë“œê°€ ìˆëŠ” ê²½ìš°)
        standard_metrics = None
        if expected_keywords:
            standard_metrics = self._calculate_standard_metrics(search_results, expected_keywords)

        # í•œêµ­ì–´ ë©”íŠ¸ë¦­ ê³„ì‚°
        korean_metrics = self._calculate_korean_metrics(search_results, expected_keywords)

        return ComprehensiveMetrics(
            total_queries=total_queries,
            successful_queries=successful_queries,
            success_rate=success_rate,
            total_results=sum(result_counts),
            avg_result_count=mean(result_counts) if result_counts else 0,

            # ìœ ì‚¬ë„ ë©”íŠ¸ë¦­
            avg_similarity=mean(similarities) if similarities else 0,
            min_similarity=min(similarities) if similarities else 0,
            max_similarity=max(similarities) if similarities else 0,
            stddev_similarity=self._safe_stdev(similarities),

            # ì„œë¸Œ ë©”íŠ¸ë¦­
            standard_metrics=standard_metrics,
            latency_metrics=latency_metrics,
            korean_metrics=korean_metrics
        )

    def _calculate_latency_metrics(self, search_times: List[float]) -> LatencyMetrics:
        """Latency ë©”íŠ¸ë¦­ ê³„ì‚° (percentiles í¬í•¨)"""
        if not search_times:
            return LatencyMetrics()

        sorted_times = sorted(search_times)

        return LatencyMetrics(
            avg_latency_ms=mean(search_times),
            median_latency_ms=median(search_times),
            min_latency_ms=min(search_times),
            max_latency_ms=max(search_times),
            stddev_latency_ms=self._safe_stdev(search_times),
            p50_latency_ms=np.percentile(sorted_times, 50),
            p95_latency_ms=np.percentile(sorted_times, 95),
            p99_latency_ms=np.percentile(sorted_times, 99)
        )

    def _calculate_standard_metrics(
        self,
        search_results: Dict[str, Dict[str, Any]],
        expected_keywords: Dict[str, List[str]]
    ) -> StandardMetrics:
        """ì •ë³´ ê²€ìƒ‰ í‘œì¤€ ë©”íŠ¸ë¦­ ê³„ì‚° (Precision@K, Recall@K, F1@K, MRR, NDCG)"""

        precisions_at_k = {1: [], 3: [], 5: []}
        recalls_at_k = {1: [], 3: [], 5: []}
        f1s_at_k = {1: [], 3: [], 5: []}
        hit_rates_at_k = {1: [], 3: [], 5: []}
        mrr_scores = []
        ndcg_scores_at_3 = []
        ndcg_scores_at_5 = []

        for query, result in search_results.items():
            if query not in expected_keywords:
                continue

            expected = set(expected_keywords[query])
            results = result['results']

            if not results:
                # ê²°ê³¼ ì—†ìŒ - ëª¨ë“  ë©”íŠ¸ë¦­ 0
                for k in [1, 3, 5]:
                    precisions_at_k[k].append(0.0)
                    recalls_at_k[k].append(0.0)
                    f1s_at_k[k].append(0.0)
                    hit_rates_at_k[k].append(0.0)
                mrr_scores.append(0.0)
                ndcg_scores_at_3.append(0.0)
                ndcg_scores_at_5.append(0.0)
                continue

            # Precision@K, Recall@K, F1@K ê³„ì‚°
            for k in [1, 3, 5]:
                top_k_results = results[:k]
                top_k_keywords = set()

                for r in top_k_results:
                    content = r.get('content', '').lower()
                    found = [kw for kw in expected if kw.lower() in content]
                    top_k_keywords.update(found)

                # Precision@K = ìƒìœ„ Kê°œ ì¤‘ ì •ë‹µì¸ ë¹„ìœ¨
                precision = len(top_k_keywords) / min(k, len(results)) if results else 0.0

                # Recall@K = ì „ì²´ ì •ë‹µ ì¤‘ ìƒìœ„ Kê°œì—ì„œ ì°¾ì€ ë¹„ìœ¨
                recall = len(top_k_keywords) / len(expected) if expected else 0.0

                # F1@K
                f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0

                # Hit Rate@K = ìƒìœ„ Kê°œ ì¤‘ ìµœì†Œ 1ê°œ ì´ìƒ ì •ë‹µì´ ìˆëŠ”ì§€
                hit_rate = 1.0 if top_k_keywords else 0.0

                precisions_at_k[k].append(precision)
                recalls_at_k[k].append(recall)
                f1s_at_k[k].append(f1)
                hit_rates_at_k[k].append(hit_rate)

            # MRR (Mean Reciprocal Rank) - ì²« ë²ˆì§¸ ì •ë‹µì˜ ìˆœìœ„
            first_relevant_rank = None
            for idx, r in enumerate(results, 1):
                content = r.get('content', '').lower()
                if any(kw.lower() in content for kw in expected):
                    first_relevant_rank = idx
                    break

            mrr = 1.0 / first_relevant_rank if first_relevant_rank else 0.0
            mrr_scores.append(mrr)

            # NDCG@K (Normalized Discounted Cumulative Gain)
            ndcg_scores_at_3.append(self._calculate_ndcg(results[:3], expected))
            ndcg_scores_at_5.append(self._calculate_ndcg(results[:5], expected))

        return StandardMetrics(
            precision_at_1=mean(precisions_at_k[1]) if precisions_at_k[1] else 0.0,
            precision_at_3=mean(precisions_at_k[3]) if precisions_at_k[3] else 0.0,
            precision_at_5=mean(precisions_at_k[5]) if precisions_at_k[5] else 0.0,
            recall_at_1=mean(recalls_at_k[1]) if recalls_at_k[1] else 0.0,
            recall_at_3=mean(recalls_at_k[3]) if recalls_at_k[3] else 0.0,
            recall_at_5=mean(recalls_at_k[5]) if recalls_at_k[5] else 0.0,
            f1_at_1=mean(f1s_at_k[1]) if f1s_at_k[1] else 0.0,
            f1_at_3=mean(f1s_at_k[3]) if f1s_at_k[3] else 0.0,
            f1_at_5=mean(f1s_at_k[5]) if f1s_at_k[5] else 0.0,
            mrr=mean(mrr_scores) if mrr_scores else 0.0,
            ndcg_at_3=mean(ndcg_scores_at_3) if ndcg_scores_at_3 else 0.0,
            ndcg_at_5=mean(ndcg_scores_at_5) if ndcg_scores_at_5 else 0.0,
            hit_rate_at_1=mean(hit_rates_at_k[1]) if hit_rates_at_k[1] else 0.0,
            hit_rate_at_3=mean(hit_rates_at_k[3]) if hit_rates_at_k[3] else 0.0,
            hit_rate_at_5=mean(hit_rates_at_k[5]) if hit_rates_at_k[5] else 0.0,
        )

    def _calculate_ndcg(self, results: List[Dict[str, Any]], expected_keywords: Set[str]) -> float:
        """NDCG (Normalized Discounted Cumulative Gain) ê³„ì‚°"""
        if not results:
            return 0.0

        # DCG ê³„ì‚°
        dcg = 0.0
        for idx, r in enumerate(results, 1):
            content = r.get('content', '').lower()
            # Relevance: ì˜ˆìƒ í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ 1, ì—†ìœ¼ë©´ 0
            relevance = 1.0 if any(kw.lower() in content for kw in expected_keywords) else 0.0
            # DCG formula: sum(rel_i / log2(i + 1))
            dcg += relevance / np.log2(idx + 1)

        # IDCG (Ideal DCG) ê³„ì‚° - ëª¨ë“  ê²°ê³¼ê°€ ì •ë‹µì¸ ê²½ìš°
        idcg = sum(1.0 / np.log2(idx + 1) for idx in range(1, min(len(results), len(expected_keywords)) + 1))

        # NDCG = DCG / IDCG
        return dcg / idcg if idcg > 0 else 0.0

    def _calculate_korean_metrics(
        self,
        search_results: Dict[str, Dict[str, Any]],
        expected_keywords: Optional[Dict[str, List[str]]] = None
    ) -> KoreanMetrics:
        """í•œêµ­ì–´ ì´í•´ë„ ê´€ë ¨ ì§€í‘œë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""

        if not search_results:
            return KoreanMetrics()

        # í‚¤ì›Œë“œ ì •í™•ë„ (expected_keywordsê°€ ìˆëŠ” ê²½ìš°)
        keyword_precision = 0.0
        keyword_recall = 0.0
        keyword_f1 = 0.0

        if expected_keywords:
            precisions = []
            recalls = []

            for query, result in search_results.items():
                if query not in expected_keywords or not result['results']:
                    continue

                expected = set(expected_keywords[query])
                found_keywords = set()

                for r in result['results']:
                    content = r.get('content', '').lower()
                    for keyword in expected:
                        if keyword.lower() in content:
                            found_keywords.add(keyword)

                # Precision: ì°¾ì€ í‚¤ì›Œë“œ ì¤‘ ì •í™•í•œ ë¹„ìœ¨
                prec = len(found_keywords) / len(expected) if expected else 0.0
                # Recall: ì „ì²´ í‚¤ì›Œë“œ ì¤‘ ì°¾ì€ ë¹„ìœ¨
                rec = len(found_keywords) / len(expected) if expected else 0.0

                precisions.append(prec)
                recalls.append(rec)

            keyword_precision = mean(precisions) if precisions else 0.0
            keyword_recall = mean(recalls) if recalls else 0.0
            keyword_f1 = (2 * keyword_precision * keyword_recall) / (keyword_precision + keyword_recall) \
                if (keyword_precision + keyword_recall) > 0 else 0.0

        # ë„ë©”ì¸ íŠ¹í™”ì„± í‰ê°€
        domain_scores = []
        for result in search_results.values():
            if not result['results']:
                continue

            for r in result['results']:
                content = r.get('content', '').lower()
                found_count = sum(1 for kw in self.korean_keywords if kw in content)
                domain_score = found_count / len(self.korean_keywords)
                domain_scores.append(domain_score)

        domain_specificity = mean(domain_scores) if domain_scores else 0.0

        # ì˜ë¯¸ì  ì¼ê´€ì„± (ìœ ì‚¬ë„ ê¸°ë°˜)
        coherence_scores = []
        for result in search_results.values():
            if result['results']:
                avg_sim = mean([r.get('similarity', 0) for r in result['results']])
                coherence_scores.append(avg_sim)

        semantic_coherence = mean(coherence_scores) if coherence_scores else 0.0

        return KoreanMetrics(
            keyword_precision=keyword_precision,
            keyword_recall=keyword_recall,
            keyword_f1=keyword_f1,
            domain_specificity=domain_specificity,
            semantic_coherence=semantic_coherence
        )

    def calculate_model_comparison(self, model_results: Dict[str, Dict[str, Any]]) -> Dict[str, Any]:
        """ì—¬ëŸ¬ ëª¨ë¸ì˜ ê²°ê³¼ë¥¼ ë¹„êµí•©ë‹ˆë‹¤."""

        if not model_results:
            return {}

        # ê° ëª¨ë¸ì˜ ì§€í‘œ ì¶”ì¶œ
        model_metrics = {}
        for model_name, result in model_results.items():
            if 'metrics' in result:
                model_metrics[model_name] = result['metrics']

        if not model_metrics:
            return {'error': 'No valid model metrics found'}

        # ë¹„êµ ì§€í‘œ ê³„ì‚°
        comparison = {
            'total_models': len(model_metrics),
            'rankings': {}
        }

        # ê° ì§€í‘œë³„ ìˆœìœ„ ê³„ì‚°
        metrics_to_rank = [
            ('avg_search_time_ms', False),  # ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ
            ('avg_similarity', True),       # ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ
            ('success_rate', True),         # ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ
            ('korean_understanding_score', True)  # ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ
        ]

        for metric_name, higher_is_better in metrics_to_rank:
            values = {}
            for model_name, metrics in model_metrics.items():
                if metric_name in metrics:
                    values[model_name] = metrics[metric_name]

            if values:
                sorted_models = sorted(values.items(), key=lambda x: x[1], reverse=higher_is_better)
                comparison['rankings'][metric_name] = sorted_models

        return comparison

    def to_dict(self, metrics: ComprehensiveMetrics) -> Dict[str, Any]:
        """ë©”íŠ¸ë¦­ì„ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
        result = asdict(metrics)
        return result

    def print_metrics(self, metrics: ComprehensiveMetrics, model_name: str = "Unknown"):
        """ë©”íŠ¸ë¦­ì„ ë³´ê¸° ì¢‹ê²Œ ì¶œë ¥"""
        print(f"\n{'='*80}")
        print(f"ğŸ“Š {model_name} ì„±ëŠ¥ ë©”íŠ¸ë¦­")
        print(f"{'='*80}")

        # ê¸°ë³¸ í†µê³„
        print(f"\n[ê¸°ë³¸ í†µê³„]")
        print(f"  ì´ ì¿¼ë¦¬: {metrics.total_queries}")
        print(f"  ì„±ê³µí•œ ì¿¼ë¦¬: {metrics.successful_queries}")
        print(f"  ì„±ê³µë¥ : {metrics.success_rate:.2%}")
        print(f"  í‰ê·  ê²°ê³¼ ìˆ˜: {metrics.avg_result_count:.2f}")

        # Latency ë©”íŠ¸ë¦­
        if metrics.latency_metrics:
            lm = metrics.latency_metrics
            print(f"\n[Latency ë©”íŠ¸ë¦­]")
            print(f"  í‰ê· : {lm.avg_latency_ms:.2f}ms")
            print(f"  ì¤‘ì•™ê°’: {lm.median_latency_ms:.2f}ms")
            print(f"  P50: {lm.p50_latency_ms:.2f}ms")
            print(f"  P95: {lm.p95_latency_ms:.2f}ms")
            print(f"  P99: {lm.p99_latency_ms:.2f}ms")
            print(f"  ìµœì†Œ: {lm.min_latency_ms:.2f}ms")
            print(f"  ìµœëŒ€: {lm.max_latency_ms:.2f}ms")

        # ìœ ì‚¬ë„ ë©”íŠ¸ë¦­
        print(f"\n[ìœ ì‚¬ë„ ë©”íŠ¸ë¦­]")
        print(f"  í‰ê· : {metrics.avg_similarity:.4f}")
        print(f"  ìµœì†Œ: {metrics.min_similarity:.4f}")
        print(f"  ìµœëŒ€: {metrics.max_similarity:.4f}")
        print(f"  í‘œì¤€í¸ì°¨: {metrics.stddev_similarity:.4f}")

        # í‘œì¤€ ë©”íŠ¸ë¦­
        if metrics.standard_metrics:
            sm = metrics.standard_metrics
            print(f"\n[ì •ë³´ ê²€ìƒ‰ í‘œì¤€ ë©”íŠ¸ë¦­]")
            print(f"  Precision@1: {sm.precision_at_1:.4f}")
            print(f"  Precision@3: {sm.precision_at_3:.4f}")
            print(f"  Precision@5: {sm.precision_at_5:.4f}")
            print(f"  Recall@1: {sm.recall_at_1:.4f}")
            print(f"  Recall@3: {sm.recall_at_3:.4f}")
            print(f"  Recall@5: {sm.recall_at_5:.4f}")
            print(f"  F1@1: {sm.f1_at_1:.4f}")
            print(f"  F1@3: {sm.f1_at_3:.4f}")
            print(f"  F1@5: {sm.f1_at_5:.4f}")
            print(f"  MRR: {sm.mrr:.4f}")
            print(f"  NDCG@3: {sm.ndcg_at_3:.4f}")
            print(f"  NDCG@5: {sm.ndcg_at_5:.4f}")
            print(f"  Hit Rate@1: {sm.hit_rate_at_1:.4f}")
            print(f"  Hit Rate@3: {sm.hit_rate_at_3:.4f}")
            print(f"  Hit Rate@5: {sm.hit_rate_at_5:.4f}")

        # í•œêµ­ì–´ ë©”íŠ¸ë¦­
        if metrics.korean_metrics:
            km = metrics.korean_metrics
            print(f"\n[í•œêµ­ì–´ ì´í•´ë„ ë©”íŠ¸ë¦­]")
            print(f"  í‚¤ì›Œë“œ Precision: {km.keyword_precision:.4f}")
            print(f"  í‚¤ì›Œë“œ Recall: {km.keyword_recall:.4f}")
            print(f"  í‚¤ì›Œë“œ F1: {km.keyword_f1:.4f}")
            print(f"  ë„ë©”ì¸ íŠ¹í™”ì„±: {km.domain_specificity:.4f}")
            print(f"  ì˜ë¯¸ì  ì¼ê´€ì„±: {km.semantic_coherence:.4f}")

        print(f"{'='*80}\n")

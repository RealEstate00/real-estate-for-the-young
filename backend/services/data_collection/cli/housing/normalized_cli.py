#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
ì •ê·œí™” CLI
ìµœê·¼ ë‚ ì§œì˜ raw ë°ì´í„°ë¥¼ ì •ê·œí™”ëœ í…Œì´ë¸” êµ¬ì¡°ë¡œ ë³€í™˜
"""

import sys
import argparse
import logging
from pathlib import Path
from typing import List, Dict, Any
import json
import pandas as pd
from datetime import datetime

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python pathì— ì¶”ê°€
project_root = Path(__file__).parent.parent.parent.parent.parent
sys.path.insert(0, str(project_root))

from backend.services.data_collection.normalized.housing.normalizer import DataNormalizer
from backend.services.data_collection.normalized.housing.db_loader import NormalizedDataLoader
from backend.db.db_utils_pg import get_engine

HELP = """data-ingest normalized <command> [args]

Commands:
  process              ìµœê·¼ ë‚ ì§œì˜ ëª¨ë“  raw ë°ì´í„°ë¥¼ ì •ê·œí™”
  process --platform <name>  íŠ¹ì • í”Œë«í¼ë§Œ ì •ê·œí™”
  process --date <date>      íŠ¹ì • ë‚ ì§œë§Œ ì •ê·œí™”
  process --db              ì •ê·œí™” í›„ DBì— ì €ì¥
  process --fresh           ê¸°ì¡´ ì •ê·œí™” ë°ì´í„°ë¥¼ ì‚­ì œí•˜ê³  ìƒˆë¡œ ìƒì„±

Examples:
  data-ingest normalized process
  data-ingest normalized process --platform sohouse
  data-ingest normalized process --date 2025-09-15
  data-ingest normalized process --db
  data-ingest normalized process --fresh
  data-ingest normalized process --platform cohouse --fresh
"""

def find_latest_raw_data(platform: str = None, date: str = None) -> List[Path]:
    """ìµœê·¼ ë‚ ì§œì˜ housing ë°ì´í„° íŒŒì¼ë“¤ì„ ì°¾ê¸°"""
    backend_dir = Path(__file__).parent.parent.parent.parent.parent
    housing_dir = backend_dir / "data" / "housing"
    
    if not housing_dir.exists():
        logging.warning(f"[ERROR] Housing directory not found: {housing_dir}")
        return []
    
    raw_files = []
    
    # í”Œë«í¼ë³„ë¡œ ê²€ìƒ‰
    platforms = [platform] if platform else ['sohouse', 'cohouse', 'youth', 'sh', 'lh']
    logging.debug(f"[DEBUG] Searching platforms: {platforms}")
    
    for platform_name in platforms:
        platform_dir = housing_dir / platform_name
        if not platform_dir.exists():
            continue
            
        # ë‚ ì§œë³„ ë””ë ‰í† ë¦¬ ì°¾ê¸°
        date_dirs = [d for d in platform_dir.iterdir() if d.is_dir()]
        if not date_dirs:
            continue
            
        # ë‚ ì§œë³„ë¡œ ì •ë ¬ (ìµœì‹ ìˆœ)
        date_dirs.sort(key=lambda x: x.name, reverse=True)
        
        # íŠ¹ì • ë‚ ì§œê°€ ì§€ì •ëœ ê²½ìš°
        if date:
            target_dirs = [d for d in date_dirs if d.name.startswith(date)]
        else:
            # ìµœì‹  ë‚ ì§œë§Œ
            target_dirs = [date_dirs[0]]
        
        # ê° ë‚ ì§œ ë””ë ‰í† ë¦¬ì—ì„œ raw.csv ì°¾ê¸°
        for date_dir in target_dirs:
            raw_csv = date_dir / "raw.csv"
            if raw_csv.exists():
                raw_files.append(raw_csv)
    
    return raw_files

def get_normalized_output_path(raw_file: Path) -> Path:
    """ì •ê·œí™”ëœ ë°ì´í„° ì¶œë ¥ ê²½ë¡œ ìƒì„±: data/normalized/ì‘ì—…ì§„í–‰ë‚ ì§œ/í”Œë«í¼ëª…/"""
    # housing ê²½ë¡œì—ì„œ í”Œë«í¼ëª…ë§Œ ì¶”ì¶œ
    # ì˜ˆ: data/housing/sohouse/2025-09-15/raw.csv
    path_parts = raw_file.parts
    platform_name = None
    
    for part in path_parts:
        if part in ['sohouse', 'cohouse', 'youth', 'sh', 'lh']:
            platform_name = part
            break
    
    if not platform_name:
        raise ValueError(f"í”Œë«í¼ëª…ì„ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {raw_file}")
    
    # ì˜¤ëŠ˜ ë‚ ì§œ ì‚¬ìš© (ì •ê·œí™” ì‹¤í–‰ ë‚ ì§œ)
    from datetime import datetime
    today = datetime.now().strftime("%Y-%m-%d")
    
    # backend/data/normalized/ì‘ì—…ì§„í–‰ë‚ ì§œ/í”Œë«í¼ëª…/ êµ¬ì¡°ë¡œ ìƒì„±
    backend_dir = Path(__file__).parent.parent.parent.parent.parent
    output_path = backend_dir / "data" / "normalized" / today / platform_name
    output_path.mkdir(parents=True, exist_ok=True)
    
    return output_path

def normalize_data(raw_csv_path: str) -> bool:
    """raw ë°ì´í„°ë¥¼ ì •ê·œí™”ëœ ë°ì´í„°ë¡œ ë³€í™˜"""
    raw_path = Path(raw_csv_path)
    if not raw_path.exists():
        print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {raw_csv_path}")
        return False
    
    try:
        # ì¶œë ¥ ê²½ë¡œ ìƒì„±
        output_path = get_normalized_output_path(raw_path)
        print(f"ğŸ“ ì¶œë ¥ ê²½ë¡œ: {output_path}")
        
        print(f"ğŸ”„ ì •ê·œí™” ì‹œì‘: {raw_csv_path}")
        
        normalizer = DataNormalizer()
        
        # ì‹¤ì‹œê°„ ì €ì¥ì„ ìœ„í•œ ì½œë°± í•¨ìˆ˜
        def save_progress(table_name: str, data: list):
            output_file = output_path / f"{table_name}.json"
            
            # NaN ê°’ì„ nullë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜
            def convert_nan_to_null(obj):
                if isinstance(obj, dict):
                    return {k: convert_nan_to_null(v) for k, v in obj.items()}
                elif isinstance(obj, list):
                    return [convert_nan_to_null(item) for item in obj]
                elif pd.isna(obj):
                    return None
                elif hasattr(obj, 'isoformat'):  # datetime, Timestamp ë“±
                    return obj.isoformat()
                else:
                    return obj
            
            # ë°ì´í„° ë³€í™˜
            converted_data = convert_nan_to_null(data)
            
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(converted_data, f, ensure_ascii=False, indent=2)
            print(f"âœ… {table_name}: {len(data)}ê°œ ë ˆì½”ë“œ ì €ì¥ â†’ {output_file}")
        
        # ì •ê·œí™” ì‹¤í–‰ (ì‹¤ì‹œê°„ ì €ì¥)
        normalized_data = normalizer.normalize_raw_data(raw_path, save_callback=save_progress)
        
        # codes.json ë³µì‚¬ (ê³µí†µ íŒŒì¼)
        codes_file = Path("backend/data/normalized/2025-09-28/codes.json")
        if codes_file.exists():
            import shutil
            shutil.copy(codes_file, output_path / "codes.json")
            print(f"âœ… codes.json ë³µì‚¬ ì™„ë£Œ")
        
        print(f"âœ… ì •ê·œí™” ì™„ë£Œ: {output_path}")
        return True
        
    except Exception as e:
        print(f"âŒ ì •ê·œí™” ì‹¤íŒ¨: {e}")
        return False

def load_to_db(raw_csv_path: str, db_url: str = None) -> bool:
    """ì •ê·œí™” í›„ DBì— ì €ì¥"""
    raw_path = Path(raw_csv_path)
    if not raw_path.exists():
        print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {raw_csv_path}")
        return False
    
    print(f"ğŸ”„ ì •ê·œí™” ë° DB ì €ì¥ ì‹œì‘: {raw_csv_path}")
    
    try:
        # ì •ê·œí™”
        normalizer = DataNormalizer()
        normalized_data = normalizer.normalize_raw_data(raw_path)
        
        # DB URL ì„¤ì •
        if not db_url:
            db_url = get_engine().url
        
        # DBì— ì €ì¥
        loader = NormalizedDataLoader(db_url)
        success = loader.load_normalized_data(normalized_data)
        
        if success:
            print("âœ… DB ì €ì¥ ì™„ë£Œ")
            return True
        else:
            print("âŒ DB ì €ì¥ ì‹¤íŒ¨")
            return False
                
    except Exception as e:
        print(f"âŒ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        return False

def clean_normalized_data(platform: str = None, date: str = None) -> None:
    """ê¸°ì¡´ ì •ê·œí™”ëœ ë°ì´í„° ì‚­ì œ"""
    backend_dir = Path(__file__).parent.parent.parent.parent.parent
    normalized_dir = backend_dir / "backend" / "data" / "normalized"
    
    if not normalized_dir.exists():
        logging.info("[INFO] Normalized data directory not found")
        return
    
    # ë‚ ì§œë³„ë¡œ ê²€ìƒ‰ (ì‹¤ì œ êµ¬ì¡°: normalized/ë‚ ì§œ/í”Œë«í¼)
    date_dirs = [d for d in normalized_dir.iterdir() if d.is_dir()]
    if not date_dirs:
        logging.info("[INFO] No normalized data found")
        return
        
    # ë‚ ì§œë³„ë¡œ ì •ë ¬ (ìµœì‹ ìˆœ)
    date_dirs.sort(key=lambda x: x.name, reverse=True)
    
    # íŠ¹ì • ë‚ ì§œê°€ ì§€ì •ëœ ê²½ìš°
    if date:
        target_date_dirs = [d for d in date_dirs if d.name.startswith(date)]
    else:
        # ìµœì‹  ë‚ ì§œë§Œ
        target_date_dirs = [date_dirs[0]]
    
    # í”Œë«í¼ë³„ë¡œ ê²€ìƒ‰
    platforms = [platform] if platform else ['sohouse', 'cohouse', 'youth', 'sh', 'lh']
    
    for date_dir in target_date_dirs:
        print(f"ğŸ—‘ï¸  ë‚ ì§œ ë””ë ‰í† ë¦¬ ê²€ì‚¬: {date_dir}")
        
        for platform_name in platforms:
            platform_dir = date_dir / platform_name
            if platform_dir.exists():
                print(f"ğŸ—‘ï¸  ê¸°ì¡´ ì •ê·œí™” ë°ì´í„° ì‚­ì œ: {platform_dir}")
                import shutil
                shutil.rmtree(platform_dir, ignore_errors=True)
        
        # ë‚ ì§œ ë””ë ‰í† ë¦¬ê°€ ë¹„ì–´ìˆìœ¼ë©´ ì‚­ì œ
        if not any(date_dir.iterdir()):
            print(f"ğŸ—‘ï¸  ë¹ˆ ë‚ ì§œ ë””ë ‰í† ë¦¬ ì‚­ì œ: {date_dir}")
            import shutil
            shutil.rmtree(date_dir, ignore_errors=True)

def process_latest_data(platform: str = None, date: str = None, save_to_db: bool = False, fresh: bool = False) -> bool:
    """ìµœê·¼ ë‚ ì§œì˜ ëª¨ë“  raw ë°ì´í„°ë¥¼ ì •ê·œí™”"""
    if fresh:
        print(f"ğŸ§¹ Fresh ëª¨ë“œ: ê¸°ì¡´ ì •ê·œí™” ë°ì´í„° ì‚­ì œ ì¤‘...")
        clean_normalized_data(platform, date)
    
    print(f"ğŸ” ìµœê·¼ raw ë°ì´í„° ê²€ìƒ‰ ì¤‘...")
    
    raw_files = find_latest_raw_data(platform, date)
    if not raw_files:
        print("âŒ ì²˜ë¦¬í•  raw ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        return False
    
    print(f"ğŸ“ ë°œê²¬ëœ raw íŒŒì¼: {len(raw_files)}ê°œ")
    for file in raw_files:
        print(f"  - {file}")
    
    success_count = 0
    
    for raw_file in raw_files:
        print(f"\nğŸ”„ ì²˜ë¦¬ ì¤‘: {raw_file}")
        
        # ì •ê·œí™”
        if normalize_data(str(raw_file)):
            success_count += 1
            print(f"âœ… ì •ê·œí™” ì™„ë£Œ: {raw_file}")
            
            # DB ì €ì¥ ì˜µì…˜ì´ í™œì„±í™”ëœ ê²½ìš°
            if save_to_db:
                print(f"ğŸ’¾ DB ì €ì¥ ì¤‘: {raw_file}")
                if load_to_db(str(raw_file)):
                    print(f"âœ… DB ì €ì¥ ì™„ë£Œ: {raw_file}")
                else:
                    print(f"âŒ DB ì €ì¥ ì‹¤íŒ¨: {raw_file}")
        else:
            print(f"âŒ ì •ê·œí™” ì‹¤íŒ¨: {raw_file}")
    
    print(f"\nğŸ“Š ì²˜ë¦¬ ê²°ê³¼: {success_count}/{len(raw_files)} ì„±ê³µ")
    return success_count > 0

def main():
    # í™˜ê²½ ë³€ìˆ˜ ì„¤ì • (ê¸°ë³¸ê°’)
    import os
    os.environ.setdefault("PG_USER", "postgres")
    os.environ.setdefault("PG_PASSWORD", "post1234")
    os.environ.setdefault("PG_DB", "rey")
    os.environ.setdefault("DATABASE_URL", "postgresql+psycopg://postgres:post1234@localhost:5432/rey")
    
    parser = argparse.ArgumentParser(description="ì •ê·œí™” CLI")
    parser.add_argument("command", choices=["process"], 
                       help="ì‹¤í–‰í•  ëª…ë ¹ì–´")
    parser.add_argument("--platform", help="íŠ¹ì • í”Œë«í¼ë§Œ ì²˜ë¦¬ (sohouse, cohouse, youth, sh, lh)")
    parser.add_argument("--date", help="íŠ¹ì • ë‚ ì§œë§Œ ì²˜ë¦¬ (YYYY-MM-DD)")
    parser.add_argument("--db", action="store_true", help="ì •ê·œí™” í›„ DBì— ì €ì¥")
    parser.add_argument("--db-url", help="ë°ì´í„°ë² ì´ìŠ¤ URL")
    parser.add_argument("--fresh", action="store_true", help="ê¸°ì¡´ ì •ê·œí™” ë°ì´í„°ë¥¼ ì‚­ì œí•˜ê³  ìƒˆë¡œ ìƒì„±")
    parser.add_argument("--verbose", "-v", action="store_true", help="ìƒì„¸ ë¡œê·¸ ì¶œë ¥")
    
    args = parser.parse_args()
    
    # ë¡œê¹… ì„¤ì •
    log_level = logging.DEBUG if args.verbose else logging.INFO
    logging.basicConfig(level=log_level, format='%(asctime)s - %(levelname)s - %(message)s')
    
    # ëª…ë ¹ì–´ ì‹¤í–‰
    if args.command == "process":
        success = process_latest_data(args.platform, args.date, args.db, args.fresh)
    
    sys.exit(0 if success else 1)

if __name__ == "__main__":
    main()

#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
ì •ê·œí™” CLI
ìµœê·¼ ë‚ ì§œì˜ raw ë°ì´í„°ë¥¼ ì •ê·œí™”ëœ í…Œì´ë¸” êµ¬ì¡°ë¡œ ë³€í™˜
"""

import sys
import argparse
import logging
from pathlib import Path
from typing import List, Dict, Any
import json
import pandas as pd
from datetime import datetime

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python pathì— ì¶”ê°€
project_root = Path(__file__).parent.parent.parent.parent.parent
sys.path.insert(0, str(project_root))

from backend.services.data_collection.curated.normalizer import DataNormalizer
from backend.services.data_collection.curated.db_loader import NormalizedDataLoader
from backend.db.db_utils_pg import get_engine

HELP = """data-collection normalized <command> [args]

Commands:
  process              ìµœê·¼ ë‚ ì§œì˜ ëª¨ë“  raw ë°ì´í„°ë¥¼ ì •ê·œí™”
  process --platform <name>  íŠ¹ì • í”Œë«í¼ë§Œ ì •ê·œí™”
  process --date <date>      íŠ¹ì • ë‚ ì§œë§Œ ì •ê·œí™”
  process --db              ì •ê·œí™” í›„ DBì— ì €ì¥

Examples:
  data-collection normalized process
  data-collection normalized process --platform sohouse
  data-collection normalized process --date 2025-09-15
  data-collection normalized process --db
"""

def find_latest_raw_data(platform: str = None, date: str = None) -> List[Path]:
    """ìµœê·¼ ë‚ ì§œì˜ raw ë°ì´í„° íŒŒì¼ë“¤ì„ ì°¾ê¸°"""
    raw_dir = Path("backend/data/raw")
    if not raw_dir.exists():
        return []
    
    raw_files = []
    
    # í”Œë«í¼ë³„ë¡œ ê²€ìƒ‰
    platforms = [platform] if platform else ['sohouse', 'cohouse', 'youth', 'sh', 'lh']
    
    for platform_name in platforms:
        platform_dir = raw_dir / platform_name
        if not platform_dir.exists():
            continue
            
        # ë‚ ì§œë³„ ë””ë ‰í† ë¦¬ ì°¾ê¸°
        date_dirs = [d for d in platform_dir.iterdir() if d.is_dir()]
        if not date_dirs:
            continue
            
        # ë‚ ì§œë³„ë¡œ ì •ë ¬ (ìµœì‹ ìˆœ)
        date_dirs.sort(key=lambda x: x.name, reverse=True)
        
        # íŠ¹ì • ë‚ ì§œê°€ ì§€ì •ëœ ê²½ìš°
        if date:
            target_dirs = [d for d in date_dirs if d.name.startswith(date)]
        else:
            # ìµœì‹  ë‚ ì§œë§Œ
            target_dirs = [date_dirs[0]]
        
        # ê° ë‚ ì§œ ë””ë ‰í† ë¦¬ì—ì„œ raw.csv ì°¾ê¸°
        for date_dir in target_dirs:
            raw_csv = date_dir / "raw.csv"
            if raw_csv.exists():
                raw_files.append(raw_csv)
    
    return raw_files

def get_normalized_output_path(raw_file: Path) -> Path:
    """ì •ê·œí™”ëœ ë°ì´í„° ì¶œë ¥ ê²½ë¡œ ìƒì„±: data/normalized/í¬ë¡¤ë§ë‚ ì§œ/í”Œë«í¼ëª…/"""
    # raw ê²½ë¡œì—ì„œ ë‚ ì§œì™€ í”Œë«í¼ëª… ì¶”ì¶œ
    # ì˜ˆ: data/raw/sohouse/2025-09-15__20250915T021038/raw.csv
    path_parts = raw_file.parts
    platform_name = None
    date_str = None
    
    for i, part in enumerate(path_parts):
        if part in ['sohouse', 'cohouse', 'youth', 'sh', 'lh']:
            platform_name = part
            # ë‹¤ìŒ ë””ë ‰í† ë¦¬ê°€ ë‚ ì§œì¼ ê°€ëŠ¥ì„±ì´ ë†’ìŒ
            if i + 1 < len(path_parts):
                date_str = path_parts[i + 1]
            break
    
    if not platform_name or not date_str:
        raise ValueError(f"í”Œë«í¼ëª… ë˜ëŠ” ë‚ ì§œë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {raw_file}")
    
    # backend/data/normalized/í¬ë¡¤ë§ë‚ ì§œ/í”Œë«í¼ëª…/ êµ¬ì¡°ë¡œ ìƒì„±
    output_path = Path("backend") / "data" / "normalized" / date_str / platform_name
    output_path.mkdir(parents=True, exist_ok=True)
    
    return output_path

def normalize_data(raw_csv_path: str) -> bool:
    """raw ë°ì´í„°ë¥¼ ì •ê·œí™”ëœ ë°ì´í„°ë¡œ ë³€í™˜"""
    raw_path = Path(raw_csv_path)
    if not raw_path.exists():
        print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {raw_csv_path}")
        return False
    
    try:
        # ì¶œë ¥ ê²½ë¡œ ìƒì„±
        output_path = get_normalized_output_path(raw_path)
        print(f"ğŸ“ ì¶œë ¥ ê²½ë¡œ: {output_path}")
        
        print(f"ğŸ”„ ì •ê·œí™” ì‹œì‘: {raw_csv_path}")
        
        normalizer = DataNormalizer()
        normalized_data = normalizer.normalize_raw_data(raw_path)
        
        # ì •ê·œí™”ëœ ë°ì´í„°ë¥¼ JSONìœ¼ë¡œ ì €ì¥
        for table_name, data in normalized_data.items():
            output_file = output_path / f"{table_name}.json"
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2, default=str)
            print(f"âœ… {table_name}: {len(data)}ê°œ ë ˆì½”ë“œ ì €ì¥ â†’ {output_file}")
        
        # ì‹¤íŒ¨í•œ ì£¼ì†Œ í†µê³„ ì¶œë ¥
        if hasattr(normalizer, 'failed_addresses') and normalizer.failed_addresses:
            print(f"âš ï¸  ì£¼ì†Œ ì •ê·œí™” ì‹¤íŒ¨: {len(normalizer.failed_addresses)}ê±´")
            print(f"ğŸ“„ ì‹¤íŒ¨ ë°ì´í„° ì €ì¥: backend/data/normalized/failed_addresses_*.csv")
        else:
            print("âœ… ì£¼ì†Œ ì •ê·œí™” ì‹¤íŒ¨ ì—†ìŒ")
        
        print(f"âœ… ì •ê·œí™” ì™„ë£Œ: {output_path}")
        return True
        
    except Exception as e:
        print(f"âŒ ì •ê·œí™” ì‹¤íŒ¨: {e}")
        return False

def load_to_db(raw_csv_path: str, db_url: str = None) -> bool:
    """ì •ê·œí™” í›„ DBì— ì €ì¥"""
    raw_path = Path(raw_csv_path)
    if not raw_path.exists():
        print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {raw_csv_path}")
        return False
    
    print(f"ğŸ”„ ì •ê·œí™” ë° DB ì €ì¥ ì‹œì‘: {raw_csv_path}")
    
    try:
        # ì •ê·œí™”
        normalizer = DataNormalizer()
        normalized_data = normalizer.normalize_raw_data(raw_path)
        
        # ì‹¤íŒ¨í•œ ì£¼ì†Œ í†µê³„ ì¶œë ¥
        if hasattr(normalizer, 'failed_addresses') and normalizer.failed_addresses:
            print(f"âš ï¸  ì£¼ì†Œ ì •ê·œí™” ì‹¤íŒ¨: {len(normalizer.failed_addresses)}ê±´")
            print(f"ğŸ“„ ì‹¤íŒ¨ ë°ì´í„° ì €ì¥: backend/data/normalized/failed_addresses_*.csv")
        else:
            print("âœ… ì£¼ì†Œ ì •ê·œí™” ì‹¤íŒ¨ ì—†ìŒ")
        
        # DB URL ì„¤ì •
        if not db_url:
            db_url = get_engine().url
        
        # DBì— ì €ì¥
        with NormalizedDataLoader(db_url) as loader:
            success = loader.load_normalized_data(normalized_data)
            
            if success:
                print("âœ… DB ì €ì¥ ì™„ë£Œ")
                return True
            else:
                print("âŒ DB ì €ì¥ ì‹¤íŒ¨")
                return False
                
    except Exception as e:
        print(f"âŒ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        return False

def process_latest_data(platform: str = None, date: str = None, save_to_db: bool = False) -> bool:
    """ìµœê·¼ ë‚ ì§œì˜ ëª¨ë“  raw ë°ì´í„°ë¥¼ ì •ê·œí™”"""
    print(f"ğŸ” ìµœê·¼ raw ë°ì´í„° ê²€ìƒ‰ ì¤‘...")
    
    raw_files = find_latest_raw_data(platform, date)
    if not raw_files:
        print("âŒ ì²˜ë¦¬í•  raw ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        return False
    
    print(f"ğŸ“ ë°œê²¬ëœ raw íŒŒì¼: {len(raw_files)}ê°œ")
    for file in raw_files:
        print(f"  - {file}")
    
    success_count = 0
    
    for raw_file in raw_files:
        print(f"\nğŸ”„ ì²˜ë¦¬ ì¤‘: {raw_file}")
        
        # ì •ê·œí™”
        if normalize_data(str(raw_file)):
            success_count += 1
            print(f"âœ… ì •ê·œí™” ì™„ë£Œ: {raw_file}")
            
            # DB ì €ì¥ ì˜µì…˜ì´ í™œì„±í™”ëœ ê²½ìš°
            if save_to_db:
                print(f"ğŸ’¾ DB ì €ì¥ ì¤‘: {raw_file}")
                if load_to_db(str(raw_file)):
                    print(f"âœ… DB ì €ì¥ ì™„ë£Œ: {raw_file}")
                else:
                    print(f"âŒ DB ì €ì¥ ì‹¤íŒ¨: {raw_file}")
        else:
            print(f"âŒ ì •ê·œí™” ì‹¤íŒ¨: {raw_file}")
    
    print(f"\nğŸ“Š ì²˜ë¦¬ ê²°ê³¼: {success_count}/{len(raw_files)} ì„±ê³µ")
    return success_count > 0

def main():
    # í™˜ê²½ ë³€ìˆ˜ ì„¤ì • (ê¸°ë³¸ê°’)
    import os
    os.environ.setdefault("PG_USER", "postgres")
    os.environ.setdefault("PG_PASSWORD", "post1234")
    os.environ.setdefault("PG_DB", "rey")
    os.environ.setdefault("DATABASE_URL", "postgresql+psycopg://postgres:post1234@localhost:5432/rey")
    
    parser = argparse.ArgumentParser(description="ì •ê·œí™” CLI")
    parser.add_argument("command", choices=["process"], 
                       help="ì‹¤í–‰í•  ëª…ë ¹ì–´")
    parser.add_argument("--platform", help="íŠ¹ì • í”Œë«í¼ë§Œ ì²˜ë¦¬ (sohouse, cohouse, youth, sh, lh)")
    parser.add_argument("--date", help="íŠ¹ì • ë‚ ì§œë§Œ ì²˜ë¦¬ (YYYY-MM-DD)")
    parser.add_argument("--db", action="store_true", help="ì •ê·œí™” í›„ DBì— ì €ì¥")
    parser.add_argument("--db-url", help="ë°ì´í„°ë² ì´ìŠ¤ URL")
    parser.add_argument("--verbose", "-v", action="store_true", help="ìƒì„¸ ë¡œê·¸ ì¶œë ¥")
    
    args = parser.parse_args()
    
    # ë¡œê¹… ì„¤ì •
    log_level = logging.DEBUG if args.verbose else logging.INFO
    logging.basicConfig(level=log_level, format='%(asctime)s - %(levelname)s - %(message)s')
    
    # ëª…ë ¹ì–´ ì‹¤í–‰
    if args.command == "process":
        success = process_latest_data(args.platform, args.date, args.db)
    
    sys.exit(0 if success else 1)

if __name__ == "__main__":
    main()

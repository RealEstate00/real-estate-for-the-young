#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
===========================================
ë°ì´í„° ì •ê·œí™” ëª¨ë“ˆ (Data Normalizer)
===========================================
raw ë°ì´í„°ë¥¼ ì •ê·œí™”ëœ í…Œì´ë¸” ë°ì´í„°ë¡œ ë³€í™˜í•˜ëŠ” ëª¨ë“ˆ
"""

import json
import logging
import pandas as pd
import re
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Tuple

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
from dotenv import load_dotenv
load_dotenv()

# ì£¼ì†Œ API í™œì„±í™”
from backend.services.data_collection.housing.normalized.address_api import normalize_address, AddressNormalizerError

logger = logging.getLogger(__name__)

# ===========================================
# 1) ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
# ===========================================

def preprocess_address(addr_raw: str) -> str:
    """ì£¼ì†Œ ì „ì²˜ë¦¬ - JUSO API ë§¤ì¹­ë¥  í–¥ìƒì„ ìœ„í•œ ì •ë¦¬"""
    if not addr_raw:
        return ""
    
    # 1. ê¸°ë³¸ ì •ë¦¬
    addr_raw = str(addr_raw).strip()
    
    # 2. ë¶ˆí•„ìš”í•œ ì ‘ë¯¸ì‚¬ ì œê±°
    addr_raw = re.sub(r'\s*\([^)]*\)\s*$', '', addr_raw)  # ê´„í˜¸ ì•ˆì˜ ë‚´ìš© ì œê±°
    addr_raw = re.sub(r'\s*\[[^\]]*\]\s*$', '', addr_raw)  # ëŒ€ê´„í˜¸ ì•ˆì˜ ë‚´ìš© ì œê±°
    
    # 3. ë¶ˆí•„ìš”í•œ ì¸µìˆ˜ ì •ë³´ ì œê±°
    addr_raw = re.sub(r'\s+\d+\s*ì¸µ.*$', '', addr_raw)
    addr_raw = re.sub(r'\s+ì „ì¸µ.*$', '', addr_raw)
    addr_raw = re.sub(r'\s+\d+\s*~\s*\d+\s*ì¸µ.*$', '', addr_raw)
    
    # 4. ê±´ë¬¼ëª… ì œê±° (ê´„í˜¸ ì•ˆì˜ ë‚´ìš©)
    addr_raw = re.sub(r'\s*\([^)]*\)\s*', ' ', addr_raw)
    addr_raw = re.sub(r'\s+[ê°€-í£]+ìƒí™œ.*$', '', addr_raw)
    addr_raw = re.sub(r'\s+[ê°€-í£]+ë¹Œ.*$', '', addr_raw)
    
    # 5. ê³µë°± ì •ë¦¬
    addr_raw = re.sub(r'\s+', ' ', addr_raw).strip()
    
    return addr_raw


# ===========================================
# 2) DataNormalizer í´ë˜ìŠ¤
# ===========================================

class DataNormalizer:
    """ë°ì´í„° ì •ê·œí™”ë¥¼ ë‹´ë‹¹í•˜ëŠ” ë©”ì¸ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.platforms = []
        self.addresses = []
        self.notices = []
        self.units = []
        self.unit_features = []
        self.notice_tags = []
        self.address_id_map = {}  # ì£¼ì†Œ ì¤‘ë³µ ì²´í¬ë¥¼ ìœ„í•œ ë§µ
        self.raw_data_dir = None
        
        # ìƒˆë¡œìš´ ì •ê·œí™” í´ë˜ìŠ¤ë“¤ (notice_normalizer ì œê±° - í•¨ìˆ˜ë§Œ ì‚¬ìš©)
        
    # ===========================================
    # 2-1) ë©”ì¸ ì •ê·œí™” ë©”ì„œë“œ
    # ===========================================
        
    def normalize_raw_data(self, raw_csv_path: Path, save_callback=None) -> Dict[str, List[Dict]]:
        """
        raw CSV ë°ì´í„°ë¥¼ ì •ê·œí™”ëœ í…Œì´ë¸” ë°ì´í„°ë¡œ ë³€í™˜
        
        Args:
            raw_csv_path: raw CSV íŒŒì¼ ê²½ë¡œ
            save_callback: ì‹¤ì‹œê°„ ì €ì¥ì„ ìœ„í•œ ì½œë°± í•¨ìˆ˜
            
        Returns:
            ì •ê·œí™”ëœ í…Œì´ë¸” ë°ì´í„° ë”•ì…”ë„ˆë¦¬
        """
        logger.info(f"ì •ê·œí™” ì‹œì‘: {raw_csv_path}")
        
        # raw_data_dir ì„¤ì • (ê°€ì¥ ìµœê·¼ ë‚ ì§œì˜ raw ë°ì´í„° ë””ë ‰í† ë¦¬)
        self.raw_data_dir = self._get_latest_raw_data_dir()
        
        # CSV ë°ì´í„° ë¡œë“œ
        df = pd.read_csv(raw_csv_path)
        logger.info(f"ë¡œë“œëœ ë ˆì½”ë“œ ìˆ˜: {len(df)}")
        
        # ê° í–‰ ì •ê·œí™”
        for idx, row in df.iterrows():
            try:
                self._normalize_row(row)
                
                # ì‹¤ì‹œê°„ ì €ì¥ (ì½œë°±ì´ ì œê³µëœ ê²½ìš°)
                if save_callback and (idx + 1) % 100 == 0:
                    normalized_data = {
                        'platforms': self.platforms,
                        'addresses': self.addresses,
                        'notices': self.notices,
                        'units': self.units,
                        'unit_features': self.unit_features,
                        'notice_tags': self.notice_tags
                    }
                    save_callback(normalized_data, f"batch_{idx+1}")
                    
            except Exception as e:
                logger.error(f"í–‰ {idx} ì •ê·œí™” ì‹¤íŒ¨: {e}")
                continue
        
        # ì •ê·œí™”ëœ ë°ì´í„° ë°˜í™˜
        normalized_data = {
            'platforms': self.platforms,
            'addresses': self.addresses,
            'notices': self.notices,
            'units': self.units,
            'unit_features': self.unit_features,
            'notice_tags': self.notice_tags
        }
        
        # ìƒˆë¡œìš´ ê¸°ëŠ¥ë“¤ ì ìš© (unit_normalizer ì œê±° - normalizer.pyì—ì„œ ì§ì ‘ ì²˜ë¦¬)
        
        # update_notices_with_kv_data ì œê±° - building_typeì€ ì´ë¯¸ _normalize_notice_with_rawì—ì„œ ì˜¬ë°”ë¥´ê²Œ ì„¤ì •ë¨
        # logger.info("ğŸ”§ Notices ë°ì´í„°ë¥¼ KV JSON ë°ì´í„°ë¡œ ì—…ë°ì´íŠ¸ ì¤‘...")
        # self.notice_normalizer.update_notices_with_kv_data(normalized_data, self.raw_data_dir)
        
        # ì‹¤ì‹œê°„ ì €ì¥ (ì½œë°±ì´ ì œê³µëœ ê²½ìš°) - ëª¨ë“  ì—…ë°ì´íŠ¸ í›„ ì €ì¥
        if save_callback:
            # ê° í…Œì´ë¸”ë³„ë¡œ ì €ì¥
            for table_name, data in normalized_data.items():
                save_callback(table_name, data)
        
        logger.info(f"ì •ê·œí™” ì™„ë£Œ: í”Œë«í¼ {len(self.platforms)}ê°œ, ì£¼ì†Œ {len(self.addresses)}ê°œ, ê³µê³  {len(self.notices)}ê°œ, ìœ ë‹› {len(self.units)}ê°œ")
        return normalized_data
    
    def _normalize_row(self, row: pd.Series):
        """ë‹¨ì¼ í–‰ ì •ê·œí™”"""
        # 1. í”Œë«í¼ ì •ê·œí™”
        platform_id = self._normalize_platform(row)
        
        # 2. ì£¼ì†Œ ì •ê·œí™” (JUSO API ì‚¬ìš©)
        address_id = self._normalize_address_from_raw(row.get('address', ''), row.get('notice_id', ''))
        
        # 3. ê³µê³  ì •ê·œí™”
        notice_id = self._normalize_notice_with_raw(row, address_id, row.to_dict())
        
        # 4. ìœ ë‹› ì •ê·œí™” (occupancy ë°ì´í„° ì‚¬ìš©)
        kv_data = self._load_kv_data(row.get('kv_json_path', ''))
        unit_index = int(row.get('unit_index', 0))
        units = self._normalize_units_from_occupancy(row.get('notice_id', ''), notice_id, kv_data, unit_index)
        self.units.extend(units)
        
        # 5. ê³µê³  ë²”ìœ„ ì—…ë°ì´íŠ¸ (ìœ ë‹› ë°ì´í„° ê¸°ë°˜)
        self._update_notice_ranges(notice_id, units)
        
        # 6. íƒœê·¸ ì •ê·œí™”
        self._normalize_tags(row, notice_id)
        
        # 7. ì¸ê·¼ ì‹œì„¤ ì •ë³´ íƒœê·¸ ì •ê·œí™” (KV ë°ì´í„° í¬í•¨)
        # KV ë°ì´í„°ë¥¼ recordì— ì¶”ê°€
        record = row.to_dict()
        record['kv_data'] = kv_data
        facilities_tags = normalize_facilities_tags(record, notice_id)
        self.notice_tags.extend(facilities_tags)

    # ===========================================
    # 2-2) ì£¼ì†Œ ì •ê·œí™” ê´€ë ¨ (JUSO API ì‚¬ìš©)
    # ===========================================
    
    def _normalize_address_from_raw(self, address_raw: str, notice_id: str) -> Optional[str]:
        """raw.csvì—ì„œ ì¶”ì¶œí•œ ì£¼ì†Œë¥¼ ì •ê·œí™” (JUSO API ì‚¬ìš©)"""
        if not address_raw:
            return None
        
        # ì¤‘ë³µ ì²´í¬
        if address_raw in self.address_id_map:
            return self.address_id_map[address_raw]
        
        try:
            # JUSO APIë¥¼ ì‚¬ìš©í•œ ì£¼ì†Œ ì •ê·œí™”
            normalized_data = normalize_address(address_raw)
            
            if normalized_data and 'normalized' in normalized_data:
                # ì •ê·œí™”ëœ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ì£¼ì†Œ ID ìƒì„±
                address_id = self._normalize_address_with_juso(normalized_data)
                return address_id
            else:
                # API ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ì²˜ë¦¬
                logger.warning(f"ì£¼ì†Œ ì •ê·œí™” API ì‹¤íŒ¨: {address_raw}")
                return self._normalize_address_fallback(address_raw)
                
        except AddressNormalizerError as e:
            logger.warning(f"ì£¼ì†Œ ì •ê·œí™” ì˜¤ë¥˜: {address_raw} - {e}")
            return self._normalize_address_fallback(address_raw)
        except Exception as e:
            logger.error(f"ì£¼ì†Œ ì •ê·œí™” ì˜ˆì™¸: {address_raw} - {e}")
            return self._normalize_address_fallback(address_raw)
    
    def _normalize_address_fallback(self, address_raw: str) -> str:
        """API ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ì£¼ì†Œ ì •ê·œí™”"""
        # ì¤‘ë³µ ì²´í¬
        if address_raw in self.address_id_map:
            return self.address_id_map[address_raw]
        
        # ê³ ìœ í•œ address_id ìƒì„±
        import hashlib
        address_id = hashlib.md5(address_raw.encode('utf-8')).hexdigest()[:16]
        
        # ì£¼ì†Œ ì •ë³´ êµ¬ì„± (ê°„ë‹¨í•œ í˜•íƒœ)
        address_info = {
            'address_id': address_id,
            'address_raw': address_raw,
            'ctpv_nm': None,
            'sgg_nm': None,
            'emd_nm': None,
            'emd_cd': None,
            'building_name': None,
            'road_name_full': None,
            'jibun_name_full': None,
            'latitude': None,
            'longitude': None
        }
        
        self.addresses.append(address_info)
        self.address_id_map[address_raw] = address_id
        
        logger.info(f"ì£¼ì†Œ ì •ê·œí™” ì™„ë£Œ (fallback): {address_raw} -> ID {address_id}")
        return address_id
    
    def _normalize_address_with_juso(self, address_data: Dict) -> int:
        """JUSO API ê²°ê³¼ë¥¼ ì‚¬ìš©í•œ ì£¼ì†Œ ì •ê·œí™”"""
        address_raw = address_data['address_raw']
        normalized = address_data['normalized']
        
        # ì¤‘ë³µ ì²´í¬ë¥¼ ìœ„í•œ í‚¤ ìƒì„±
        dedup_key = f"{normalized.get('emd_cd', '')}_{normalized.get('road_name_full', '')}_{normalized.get('building_main_no', '')}"
        dedup_key = dedup_key.lower().strip()
        
        if dedup_key in self.addresses:
            return self.addresses[dedup_key]['address_id']
        
        # ê³ ìœ í•œ address_id ìƒì„± (ì£¼ì†Œ ì›ë³¸ í…ìŠ¤íŠ¸ì˜ MD5 í•´ì‹œê°’)
        import hashlib
        address_id = hashlib.md5(address_raw.encode('utf-8')).hexdigest()[:16]
        
        # ì£¼ì†Œ ì •ê·œí™” ë°ì´í„° êµ¬ì„± (vworld API ê²°ê³¼ ì‚¬ìš©)
        address_info = {
            'address_id': address_id,
            'address_raw': address_raw,
            'ctpv_nm': normalized.get('ctpv_nm', ''),  # ì‹œë„ëª…
            'sgg_nm': normalized.get('sgg_nm', ''),    # ì‹œêµ°êµ¬ëª…
            'emd_nm': normalized.get('emd_nm', ''),    # ìë©´ë™ëª…
            'emd_cd': normalized.get('emd_cd', ''),    # ìë©´ë™ì½”ë“œ
            'road_name_full': normalized.get('road_name_full', ''),  # ë„ë¡œëª…ì£¼ì†Œ ì „ì²´
            'jibun_name_full': normalized.get('jibun_name_full', ''),  # ì§€ë²ˆì£¼ì†Œ ì „ì²´
            'main_jibun': normalized.get('main_jibun', ''),  # ì£¼ì§€ë²ˆ
            'sub_jibun': normalized.get('sub_jibun', ''),    # ë¶€ì§€ë²ˆ
            'building_name': normalized.get('building_name', ''),  # ê±´ë¬¼ëª…
            'building_main_no': normalized.get('building_main_no', ''),  # ê±´ë¬¼ì£¼ë²ˆí˜¸
            'building_sub_no': normalized.get('building_sub_no', ''),    # ê±´ë¬¼ë¶€ë²ˆí˜¸
            'lat': normalized.get('lat'),  # ìœ„ë„
            'lon': normalized.get('lon')   # ê²½ë„
        }
        
        self.addresses[dedup_key] = address_info
        self.address_id_map[address_raw] = address_id
        
        logger.info(f"ì£¼ì†Œ ì •ê·œí™” ì™„ë£Œ: {address_raw} -> ID {address_id}")
        logger.info(f"  ì‹œë„ëª…: {normalized.get('ctpv_nm', '')}")
        logger.info(f"  ì‹œêµ°êµ¬ëª…: {normalized.get('sgg_nm', '')}")
        logger.info(f"  ìë©´ë™ì½”ë“œ: {normalized.get('emd_cd', '')}, ìë©´ë™ëª…: {normalized.get('emd_nm', '')}")
        logger.info(f"  ë„ë¡œëª…ì£¼ì†Œ: {normalized.get('road_name_full', '')}")
        logger.info(f"  ì§€ë²ˆì£¼ì†Œ: {normalized.get('jibun_name_full', '')}")
        logger.info(f"  ì£¼ì§€ë²ˆ: {normalized.get('main_jibun', '')}, ë¶€ì§€ë²ˆ: {normalized.get('sub_jibun', '')}")
        logger.info(f"  ì¢Œí‘œ: ({normalized.get('lat', '')}, {normalized.get('lon', '')})")
        return address_id
    
    # ===========================================
    # 2-3) ê°€ê²©/ë©´ì  ì¶”ì¶œ ê´€ë ¨
    # ===========================================
    

    # ===========================================
    # 2-4) ê³µê³  ì •ê·œí™” ê´€ë ¨
    # ===========================================

    def _normalize_notice_with_raw(self, row: pd.Series, address_id: Optional[int], record: Dict) -> str:
        """raw ë°ì´í„°ë¥¼ ì‚¬ìš©í•œ ê³µê³  ì •ê·œí™”"""
        notice_id = row.get('notice_id', '')  # ì‹¤ì œ notice_id ì‚¬ìš©
        
        # í”Œë«í¼ ID ê²°ì • (ë¬¸ìì—´ ì½”ë“œ ì‚¬ìš©) - í†µì¼ëœ ì½”ë“œ ì‚¬ìš©
        platform_id = 'sohouse' if 'sohouse' in str(row.get('notice_id', '')) else 'cohouse'
        
        # KV JSON ë°ì´í„° ë¡œë“œ
        kv_data = self._load_kv_data(record.get('kv_json_path', ''))
        
        # ì´ë¯¸ì§€, í”Œë¡œì–´í”Œëœ, ë¬¸ì„œ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        image_paths = row.get('image_paths', '')
        floorplan_paths = row.get('floorplan_paths', '')
        document_paths = row.get('document_paths', '')
        
        # floorplan_pathsëŠ” raw.csvì—ì„œ ì§ì ‘ ê°€ì ¸ì˜´ (KVì—ì„œ ì¶”ì¶œ ë¶ˆí•„ìš”)
        
        has_images = bool(image_paths and str(image_paths).strip() and str(image_paths) != 'nan')
        has_floorplan = bool(floorplan_paths and str(floorplan_paths).strip() and str(floorplan_paths) != 'nan')
        has_documents = bool(document_paths and str(document_paths).strip() and str(document_paths) != 'nan')
        
        # occupancy ë°ì´í„°ì—ì„œ ê°€ê²© ë° ë©´ì  ì •ë³´ ì¶”ì¶œ
        # building_typeì€ raw.csvì—ì„œ ì§ì ‘ ê°€ì ¸ì˜¤ê¸°
        final_building_type = row.get('building_type', 'unknown')
        
        # ê³ ìœ í•œ address_id ìƒì„±
        import hashlib
        address_raw = record.get('address', '')
        unique_address_id = hashlib.md5(address_raw.encode('utf-8')).hexdigest()[:16] if address_raw else None
        
        # ê³µê³  ì •ë³´ êµ¬ì„± 
        notice_data = {
            'notice_id': row.get('notice_id', ''), 
            'platform_id': platform_id,
            'title': record.get('house_name', '') or record.get('building_details', {}).get('address', ''),
            'status': 'open',
            'address_raw': address_raw,
            'address_id': unique_address_id,  # ê³ ìœ í•œ address_id ì‚¬ìš©
            'building_type': final_building_type,
            'notice_extra': {
                'image_paths': image_paths,
                'floorplan_paths': floorplan_paths,
                'document_paths': document_paths
            },
            'has_images': has_images,
            'has_floorplan': has_floorplan,
            'has_documents': has_documents,
            'list_url': record.get('list_url', ''),
            'posted_at': self._parse_datetime(record.get('last_modified')),
            'last_modified': self._parse_datetime(record.get('last_modified')),
            'apply_start_at': None,
            'apply_end_at': None,
            'created_at': self._parse_datetime(record.get('crawl_date')),
            'updated_at': self._parse_datetime(record.get('crawl_date'))
        }
        
        self.notices.append(notice_data)
        return notice_id
    
    def _normalize_tags(self, row: pd.Series, notice_id: str):
        """íƒœê·¸ ì •ê·œí™”"""
        tags = normalize_tags(row, notice_id)
        self.notice_tags.extend(tags)

    # ===========================================
    # 2-5) ìœ ë‹› ì •ê·œí™” ê´€ë ¨
    # ===========================================

    def _normalize_units_from_occupancy(self, notice_id: str, notice_id_int: int, kv_data: Dict, unit_index: int = 0) -> List[Dict]:
        """occupancy ë°ì´í„°ì—ì„œ units ì •ê·œí™”"""
        occupancy_df = self._load_occupancy_data(notice_id, unit_index)
        units = []
        
        logger.info(f"[DEBUG] _normalize_units_from_occupancy: notice_id={notice_id}, occupancy_df.empty={occupancy_df.empty}")
        
        if occupancy_df.empty:
            # occupancy ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
            logger.warning(f"[DEBUG] occupancy ë°ì´í„° ì—†ìŒ: {notice_id}")
            return []
        
        logger.info(f"[DEBUG] occupancy ë°ì´í„° {len(occupancy_df)}ê°œ ë ˆì½”ë“œ ì²˜ë¦¬ ì‹œì‘")
        
        # raw CSVì—ì„œ í•´ë‹¹ notice_idì˜ unit_type ê°€ì ¸ì˜¤ê¸°
        raw_unit_type = self._get_raw_unit_type(notice_id)
        
        for idx, unit_row in occupancy_df.iterrows():
            logger.info(f"[DEBUG] unit_row {idx}: {unit_row.to_dict()}")
            unit_id = len(self.units) + len(units) + 1
            
            # ë©´ì  íŒŒì‹± (ì˜ˆ: "17.84mÂ²" -> 17.84)
            area_str = str(unit_row.get('ë©´ì ', ''))
            area_m2 = None
            if area_str and area_str != 'nan':
                area_match = re.search(r'(\d+\.?\d*)', area_str)
                if area_match:
                    area_m2 = float(area_match.group(1))
            
            # ë³´ì¦ê¸ˆ, ì›”ì„ëŒ€ë£Œ, ê´€ë¦¬ë¹„ íŒŒì‹±
            deposit = self._parse_krw_amount(str(unit_row.get('ë³´ì¦ê¸ˆ', '')))
            rent = self._parse_krw_amount(str(unit_row.get('ì›”ì„ëŒ€ë£Œ', '')))
            maintenance_fee = self._parse_krw_amount(str(unit_row.get('ê´€ë¦¬ë¹„', '')))
            
            # ì¸µìˆ˜ íŒŒì‹±
            floor = self._parse_int(unit_row.get('ì¸µ'))
            
            # ì…ì£¼ê°€ëŠ¥ì¼ íŒŒì‹±
            occupancy_date = unit_row.get('ì…ì£¼ê°€ëŠ¥ì¼', '')
            occupancy_available_at = None
            if occupancy_date and str(occupancy_date) != 'nan' and str(occupancy_date).strip():
                try:
                    occupancy_available_at = pd.to_datetime(occupancy_date).strftime('%Y-%m-%d')
                except:
                    pass
            
            # ì¸ì› ìˆ˜ íŒŒì‹±
            capacity = self._parse_int(unit_row.get('ì¸ì›'))
            
            # ì…ì£¼ê°€ëŠ¥ ì—¬ë¶€ íŒŒì‹±
            occupancy_available = bool(unit_row.get('ì…ì£¼ê°€ëŠ¥', 0))
            
            # í˜¸ìˆ˜ ì •ë³´ íŒŒì‹± (ë°©ì´ë¦„ì—ì„œ ì¶”ì¶œ)
            room_name = unit_row.get('ë°©ì´ë¦„', '')
            if room_name and str(room_name).strip() and str(room_name).strip() != 'nan':
                room_number = str(room_name).strip()
                # "í˜¸"ê°€ ìˆìœ¼ë©´ ì œê±°
                if room_number.endswith('í˜¸'):
                    room_number = room_number[:-1]
                # ìˆ«ìë§Œ ìˆëŠ” ê²½ìš° ê·¸ëŒ€ë¡œ ì‚¬ìš©
                if room_number.isdigit():
                    room_number = room_number
            else:
                room_number = None
            
            unit_data = {
                'unit_id': unit_row.get('space_id', f'unit_{unit_id}'),
                'notice_id': notice_id,
                'unit_type': raw_unit_type, 
                'deposit': deposit,
                'rent': rent,
                'maintenance_fee': maintenance_fee,
                'area_m2': area_m2,
                'floor': floor,
                'room_number': room_number,  # í˜¸ìˆ˜ ì •ë³´ ì¶”ê°€
                'occupancy_available': occupancy_available,  # ì…ì£¼ê°€ëŠ¥ ì—¬ë¶€
                'occupancy_available_at': occupancy_available_at,
                'capacity': capacity  # ì¸ì› ìˆ˜
            }
            
            units.append(unit_data)
            
            # unit_featuresì— ì¶”ê°€ (room_count, bathroom_count, direction)
            unit_type = raw_unit_type  # raw CSVì—ì„œ ê°€ì ¸ì˜¨ unit_type ì‚¬ìš©
            unit_features = {
                'unit_id': unit_row.get('space_id', f'unit_{unit_id}'),  # unit_codeë¥¼ unit_idë¡œ ì‚¬ìš©
                'room_count': self._extract_room_count(unit_type),
                'bathroom_count': None,  # occupancy ë°ì´í„°ì—ëŠ” ì—†ìŒ
                'direction': None
            }
            self.unit_features.append(unit_features)
        
        return units

    # ===========================================
    # 2-6) ë°ì´í„° ë¡œë“œ ê´€ë ¨
    # ===========================================
    
    def _load_occupancy_data(self, notice_id: str, unit_index: int = 0) -> pd.DataFrame:
        """occupancy CSV íŒŒì¼ ë¡œë“œ"""
        try:
            if 'sohouse' in notice_id:
                # sohouseì˜ ê²½ìš° sohouse ë””ë ‰í† ë¦¬ì—ì„œ occupancy íŒŒì¼ ê²€ìƒ‰
                sohouse_data_dir = Path("backend/data/housing/sohouse")
                # ê°€ì¥ ìµœê·¼ ë‚ ì§œì˜ sohouse ë””ë ‰í† ë¦¬ ì°¾ê¸°
                if sohouse_data_dir.exists():
                    date_dirs = [d for d in sohouse_data_dir.iterdir() if d.is_dir() and re.match(r'\d{4}-\d{2}-\d{2}', d.name)]
                    if date_dirs:
                        latest_sohouse_dir = max(date_dirs, key=lambda x: x.name)
                        tables_dir = latest_sohouse_dir / 'tables'
                    else:
                        logger.warning(f"sohouse ë‚ ì§œ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {sohouse_data_dir}")
                        return pd.DataFrame()
                else:
                    logger.warning(f"sohouse ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {sohouse_data_dir}")
                    return pd.DataFrame()
                occupancy_files = list(tables_dir.glob("detail_*_occupancy.csv"))
                occupancy_files.sort(key=lambda x: int(x.stem.split('_')[1]))
                
                logger.info(f"[DEBUG] sohouse occupancy íŒŒì¼ ê²€ìƒ‰: {len(occupancy_files)}ê°œ íŒŒì¼")
                
                for occupancy_file in occupancy_files:
                    try:
                        df = pd.read_csv(occupancy_file)
                        if not df.empty and 'notice_id' in df.columns:
                            # ë”°ì˜´í‘œ ì œê±°í•˜ì—¬ ë¹„êµ
                            df['notice_id_clean'] = df['notice_id'].astype(str).str.strip('"')
                            logger.info(f"[DEBUG] sohouse occupancy íŒŒì¼ {occupancy_file} - notice_id ìƒ˜í”Œ: {df['notice_id_clean'].iloc[0] if not df.empty else 'None'}")
                            logger.info(f"[DEBUG] ì°¾ëŠ” notice_id: {notice_id}")
                            matching_rows = df[df['notice_id_clean'] == notice_id]
                            if not matching_rows.empty:
                                logger.info(f"[DEBUG] sohouse occupancy ë°ì´í„° ë°œê²¬: {occupancy_file} - {len(matching_rows)}ê°œ ë ˆì½”ë“œ")
                                return matching_rows
                    except Exception as e:
                        logger.warning(f"[DEBUG] occupancy íŒŒì¼ ì½ê¸° ì‹¤íŒ¨ {occupancy_file}: {e}")
                        continue
                
                logger.warning(f"[DEBUG] sohouse notice_id {notice_id}ì— í•´ë‹¹í•˜ëŠ” occupancy ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                return pd.DataFrame()
            else:
                # cohouseì˜ ê²½ìš° cohouse ë””ë ‰í† ë¦¬ì—ì„œ occupancy íŒŒì¼ ê²€ìƒ‰
                cohouse_data_dir = Path("backend/data/housing/cohouse")
                # ê°€ì¥ ìµœê·¼ ë‚ ì§œì˜ cohouse ë””ë ‰í† ë¦¬ ì°¾ê¸°
                if cohouse_data_dir.exists():
                    date_dirs = [d for d in cohouse_data_dir.iterdir() if d.is_dir() and re.match(r'\d{4}-\d{2}-\d{2}', d.name)]
                    if date_dirs:
                        latest_cohouse_dir = max(date_dirs, key=lambda x: x.name)
                        tables_dir = latest_cohouse_dir / 'tables'
                    else:
                        logger.warning(f"cohouse ë‚ ì§œ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {cohouse_data_dir}")
                        return pd.DataFrame()
                else:
                    logger.warning(f"cohouse ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {cohouse_data_dir}")
                    return pd.DataFrame()
                occupancy_files = list(tables_dir.glob("detail_*_occupancy.csv"))
                occupancy_files.sort(key=lambda x: int(x.stem.split('_')[1]))
                
                logger.info(f"[DEBUG] cohouse occupancy íŒŒì¼ ê²€ìƒ‰: {len(occupancy_files)}ê°œ íŒŒì¼")
                
                for occupancy_file in occupancy_files:
                    try:
                        df = pd.read_csv(occupancy_file)
                        if not df.empty and 'notice_id' in df.columns:
                            # ë”°ì˜´í‘œ ì œê±°í•˜ì—¬ ë¹„êµ
                            df['notice_id_clean'] = df['notice_id'].astype(str).str.strip('"')
                            matching_rows = df[df['notice_id_clean'] == notice_id]
                            if not matching_rows.empty:
                                logger.info(f"[DEBUG] cohouse occupancy ë°ì´í„° ë°œê²¬: {occupancy_file} - {len(matching_rows)}ê°œ ë ˆì½”ë“œ")
                                return matching_rows
                    except Exception as e:
                        logger.warning(f"[DEBUG] occupancy íŒŒì¼ ì½ê¸° ì‹¤íŒ¨ {occupancy_file}: {e}")
                        continue
                
                logger.warning(f"[DEBUG] cohouse notice_id {notice_id}ì— í•´ë‹¹í•˜ëŠ” occupancy ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
        except Exception as e:
            logger.warning(f"occupancy ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {notice_id}, ì˜¤ë¥˜: {e}")
        
        return pd.DataFrame()

    def _load_kv_data(self, kv_json_path: str) -> Dict:
        """KV JSON íŒŒì¼ ë¡œë“œ"""
        if not kv_json_path or str(kv_json_path) == 'nan':
            return {}
        
        try:
            # raw ë°ì´í„° ë””ë ‰í† ë¦¬ì—ì„œ KV íŒŒì¼ ê²½ë¡œ êµ¬ì„±
            kv_file_path = Path(self.raw_data_dir) / kv_json_path
            if kv_file_path.exists():
                with open(kv_file_path, 'r', encoding='utf-8') as f:
                    return json.load(f)
        except Exception as e:
            logger.warning(f"KV JSON íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {kv_json_path}, ì˜¤ë¥˜: {e}")
        
        return {}

    def _get_latest_raw_csv_paths(self) -> List[Path]:
        """ê°€ì¥ ìµœê·¼ ë‚ ì§œì˜ housing CSV íŒŒì¼ ê²½ë¡œë“¤ì„ ë°˜í™˜"""
        housing_data_dir = Path("backend/data/housing")
        csv_paths = []
        
        # cohouseì™€ sohouse ë””ë ‰í† ë¦¬ì—ì„œ ê°€ì¥ ìµœê·¼ ë‚ ì§œ ì°¾ê¸°
        for platform in ['cohouse', 'sohouse']:
            platform_dir = housing_data_dir / platform
            if platform_dir.exists():
                # ë‚ ì§œ ë””ë ‰í† ë¦¬ë“¤ ì°¾ê¸° (YYYY-MM-DD í˜•ì‹)
                date_dirs = [d for d in platform_dir.iterdir() if d.is_dir() and re.match(r'\d{4}-\d{2}-\d{2}', d.name)]
                if date_dirs:
                    # ê°€ì¥ ìµœê·¼ ë‚ ì§œ ì„ íƒ
                    latest_date_dir = max(date_dirs, key=lambda x: x.name)
                    csv_path = latest_date_dir / "raw.csv"
                    if csv_path.exists():
                        csv_paths.append(csv_path)
                        logger.info(f"ìµœê·¼ housing CSV íŒŒì¼ ë°œê²¬: {csv_path}")
        
        return csv_paths

    def _get_latest_raw_data_dir(self) -> Path:
        """ê°€ì¥ ìµœê·¼ ë‚ ì§œì˜ housing ë°ì´í„° ë””ë ‰í† ë¦¬ë¥¼ ë°˜í™˜"""
        housing_data_dir = Path("backend/data/housing")
        
        # cohouseì™€ sohouse ë””ë ‰í† ë¦¬ì—ì„œ ê°€ì¥ ìµœê·¼ ë‚ ì§œ ì°¾ê¸°
        latest_date = None
        for platform in ['cohouse', 'sohouse']:
            platform_dir = housing_data_dir / platform
            if platform_dir.exists():
                # ë‚ ì§œ ë””ë ‰í† ë¦¬ë“¤ ì°¾ê¸° (YYYY-MM-DD í˜•ì‹)
                date_dirs = [d for d in platform_dir.iterdir() if d.is_dir() and re.match(r'\d{4}-\d{2}-\d{2}', d.name)]
                if date_dirs:
                    platform_latest = max(date_dirs, key=lambda x: x.name)
                    if latest_date is None or platform_latest.name > latest_date.name:
                        latest_date = platform_latest
        
        if latest_date:
            logger.info(f"ìµœê·¼ housing ë°ì´í„° ë””ë ‰í† ë¦¬: {latest_date}")
            return latest_date
        else:
            # ê¸°ë³¸ê°’ìœ¼ë¡œ cohouse/2025-09-25 ì‚¬ìš© (fallback)
            fallback_dir = housing_data_dir / "cohouse" / "2025-09-25"
            logger.warning(f"ìµœê·¼ ë‚ ì§œë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ê¸°ë³¸ ë””ë ‰í† ë¦¬ ì‚¬ìš©: {fallback_dir}")
            return fallback_dir

    def _get_raw_unit_type(self, notice_id: str) -> str:
        """raw CSVì—ì„œ notice_idì— í•´ë‹¹í•˜ëŠ” unit_typeì„ ê°€ì ¸ì˜¤ê¸°"""
        # ê°€ì¥ ìµœê·¼ ë‚ ì§œì˜ raw CSV íŒŒì¼ë“¤ ì°¾ê¸°
        raw_csv_paths = self._get_latest_raw_csv_paths()
        
        for csv_path in raw_csv_paths:
            if csv_path.exists():
                try:
                    import pandas as pd
                    df = pd.read_csv(csv_path)
                    matching_rows = df[df['notice_id'] == notice_id]
                    if not matching_rows.empty:
                        unit_type = matching_rows.iloc[0].get('unit_type', '')
                        if unit_type and str(unit_type) != 'nan':
                            logger.info(f"[DEBUG] raw CSVì—ì„œ unit_type ì°¾ìŒ: {notice_id} -> {unit_type}")
                            return str(unit_type)
                except Exception as e:
                    logger.error(f"[ERROR] {csv_path} ì½ê¸° ì‹¤íŒ¨: {e}")
        
        logger.warning(f"[DEBUG] raw CSVì—ì„œ unit_typeì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {notice_id}")
        return 'unknown'

    # ===========================================
    # 2-7) íŒŒì‹± ìœ í‹¸ë¦¬í‹°
    # ===========================================

    def _parse_krw_amount(self, amount_str: str) -> Optional[int]:
        """í•œêµ­ ì›í™” ê¸ˆì•¡ íŒŒì‹± (ì˜ˆ: "15,000,000ì›" -> 15000000, "5500ë§Œì›" -> 55000000)"""
        if not amount_str or str(amount_str) == 'nan':
            return None
        
        amount_str = str(amount_str).strip()
        
        # ë§Œì› ë‹¨ìœ„ ì²˜ë¦¬
        if 'ë§Œì›' in amount_str:
            # "5500ë§Œì›" -> 55000000
            numbers = re.findall(r'[\d,]+', amount_str.replace('ë§Œì›', ''))
            if numbers:
                try:
                    return int(numbers[0].replace(',', '')) * 10000
                except:
                    pass
        
        # ì–µì› ë‹¨ìœ„ ì²˜ë¦¬
        elif 'ì–µì›' in amount_str:
            # "1ì–µì›" -> 100000000
            numbers = re.findall(r'[\d,]+', amount_str.replace('ì–µì›', ''))
            if numbers:
                try:
                    return int(numbers[0].replace(',', '')) * 100000000
                except:
                    pass
        
        # ì¼ë°˜ ì› ë‹¨ìœ„ ì²˜ë¦¬
        else:
            # ìˆ«ìë§Œ ì¶”ì¶œ
            numbers = re.findall(r'[\d,]+', amount_str.replace('ì›', ''))
            if numbers:
                try:
                    return int(numbers[0].replace(',', ''))
                except:
                    pass
        
        return None

    def _parse_datetime(self, value) -> Optional[datetime]:
        """ë‚ ì§œ/ì‹œê°„ íŒŒì‹±"""
        if pd.isna(value) or not value:
            return None
        
        try:
            if isinstance(value, str):
                # ISO í˜•ì‹ íŒŒì‹± ì‹œë„
                if 'T' in value:
                    return datetime.fromisoformat(value.replace('Z', '+00:00'))
                # ê¸°íƒ€ í˜•ì‹ë“¤
                return pd.to_datetime(value)
            return pd.to_datetime(value)
        except:
            return None

    def _parse_numeric(self, value) -> Optional[float]:
        """ìˆ«ì íŒŒì‹±"""
        if pd.isna(value) or not value:
            return None
        
        try:
            if isinstance(value, str):
                # ì‰¼í‘œ ì œê±°
                value = value.replace(',', '')
                # ìˆ«ìë§Œ ì¶”ì¶œ
                import re
                numbers = re.findall(r'\d+', value)
                return float(numbers[0]) if numbers else None
            return float(value)
        except:
            return None

    def _parse_int(self, value) -> Optional[int]:
        """ì •ìˆ˜ íŒŒì‹±"""
        numeric = self._parse_numeric(value)
        return int(numeric) if numeric is not None else None

    def _extract_room_count(self, unit_type: str) -> int:
        """ìœ ë‹› íƒ€ì…ì—ì„œ ë°© ê°œìˆ˜ ì¶”ì¶œ"""
        if 'ì›ë£¸' in unit_type:
            return 1
        elif 'íˆ¬ë£¸' in unit_type:
            return 2
        elif 'ì“°ë¦¬ë£¸' in unit_type or '3ë£¸' in unit_type:
            return 3
        else:
            return 1  # ê¸°ë³¸ê°’

    # ===========================================
    # 2-8) ê¸°íƒ€ ìœ í‹¸ë¦¬í‹°
    # ===========================================
    
    def _normalize_platform(self, row: pd.Series) -> str:
        """í”Œë«í¼ ì •ê·œí™” (ë¬¸ìì—´ ì½”ë“œ ë°˜í™˜)"""
        platform_code = row.get('platform', 'unknown')
        platform_name = self._get_platform_name(platform_code)
        platform_url = self._get_platform_url(platform_code)
        
        # ì½”ë“œë§ˆìŠ¤í„°ì™€ í˜¸í™˜ë˜ëŠ” ì½”ë“œë¡œ ë³€í™˜
        code_master_code = f'platform_{platform_code}'
        
        # ì¤‘ë³µ ì²´í¬
        for platform in self.platforms:
            if platform['code'] == platform_code:
                return platform['code']  # code ë°˜í™˜
        
        platform_data = {
            'code': platform_code,
            'name': platform_name,
            'url': platform_url,
            'is_active': True,
            'platform_code': code_master_code  # ì½”ë“œë§ˆìŠ¤í„° ì½”ë“œ ì¶”ê°€
        }
        
        self.platforms.append(platform_data)
        return platform_code

    def _update_notice_ranges(self, notice_id: int, units: List[Dict]):
        """ê³µê³ ì˜ ë²”ìœ„ ì •ë³´ ì—…ë°ì´íŠ¸ (ìœ ë‹› ë°ì´í„° ê¸°ë°˜)"""
        if not units:
            return
        
        # ë©´ì  ë²”ìœ„ ê³„ì‚°
        areas = [unit.get('area_m2') for unit in units if unit.get('area_m2')]
        if areas:
            area_min = min(areas)
            area_max = max(areas)
        else:
            area_min = area_max = None
        
        # ì¸µìˆ˜ ë²”ìœ„ ê³„ì‚°
        floors = [unit.get('floor') for unit in units if unit.get('floor')]
        if floors:
            floor_min = min(floors)
            floor_max = max(floors)
        else:
            floor_min = floor_max = None
        
        # ê³µê³  ë°ì´í„° ì—…ë°ì´íŠ¸
        for notice in self.notices:
            if notice['notice_id'] == notice_id:
                # min/max í•„ë“œëŠ” ì œê±°ë˜ì—ˆìœ¼ë¯€ë¡œ ì—…ë°ì´íŠ¸í•˜ì§€ ì•ŠìŒ
                break

    def _get_platform_name(self, code: str) -> str:
        """í”Œë«í¼ ì½”ë“œë¡œë¶€í„° ì´ë¦„ ì¶”ì¶œ"""
        platform_names = {
            'cohouse': 'ì„œìš¸ì‹œ ê³µë™ì²´ì£¼íƒ',
            'sohouse': 'ì„œìš¸ì‹œ ì‚¬íšŒì£¼íƒ',
            'lh': 'LHê³µì‚¬',
            'sh': 'SHê³µì‚¬',
            'youth': 'ì²­ë…„ì•ˆì‹¬ì£¼íƒ'
        }
        return platform_names.get(code, code)

    def _get_platform_url(self, code: str) -> str:
        """í”Œë«í¼ ì½”ë“œë¡œë¶€í„° URL ì¶”ì¶œ"""
        platform_urls = {
            'cohouse': 'https://soco.seoul.go.kr/coHouse',
            'sohouse': 'https://soco.seoul.go.kr/soHouse',
            'lh': 'https://www.lh.or.kr',
            'sh': 'https://www.sh.co.kr',
            'youth': 'https://soco.seoul.go.kr/youth'
        }
        return platform_urls.get(code, '')

    
# ===========================================
# 3) íƒœê·¸ ì •ê·œí™” í•¨ìˆ˜ë“¤
# ===========================================

def normalize_tags(row: pd.Series, notice_id: str) -> List[Dict]:
    """íƒœê·¸ ë°ì´í„° ì •ê·œí™” (notice_id, tag, description êµ¬ì¡°)"""
    tags = []
    
    # í…Œë§ˆë¥¼ íƒœê·¸ë¡œ ì¶”ê°€
    theme = str(row.get('theme', '')).strip()
    if theme and theme != 'nan':
        tags.append({
            'notice_id': notice_id,
            'tag': 'í…Œë§ˆ',
            'description': theme
        })
    
    # ì§€í•˜ì² ì—­ì„ íƒœê·¸ë¡œ ì¶”ê°€
    subway = str(row.get('subway_station', '')).strip()
    if subway and subway != 'nan':
        tags.append({
            'notice_id': notice_id,
            'tag': 'ì§€í•˜ì² ',
            'description': subway
        })
    
    # ìê²©ìš”ê±´ì„ íƒœê·¸ë¡œ ì¶”ê°€
    eligibility = str(row.get('eligibility', '')).strip()
    if eligibility and eligibility != 'nan':
        tags.append({
            'notice_id': notice_id,
            'tag': 'ìê²©ìš”ê±´',
            'description': eligibility
        })
    
    return tags

def normalize_facilities_tags(record: Dict, notice_id: str) -> List[Dict]:
    """KV/JSON íŒŒì¼ì˜ ì‹œì„¤ ì •ë³´ë¥¼ íƒœê·¸ë¡œ ì •ê·œí™” (notice_id, tag, description êµ¬ì¡°)"""
    tags = []
    added_tags = set()  # ì¤‘ë³µ ë°©ì§€ë¥¼ ìœ„í•œ ì§‘í•©
    
    if not record:
        logger.warning(f"[DEBUG] normalize_facilities_tags: recordì´ ë¹„ì–´ìˆìŒ - notice_id: {notice_id}")
        return tags
    
    # KV ë°ì´í„°ì—ì„œ ì‹œì„¤ ì •ë³´ ì¶”ì¶œ
    kv_data = record.get('kv_data', {})
    facilities = kv_data.get('facilities', {})
    
    logger.info(f"[DEBUG] normalize_facilities_tags: notice_id={notice_id}, kv_data keys={list(kv_data.keys())}, facilities keys={list(facilities.keys())}")
    
    # facilities ë”•ì…”ë„ˆë¦¬ì˜ ëª¨ë“  í‚¤-ê°’ ìŒì„ íƒœê·¸ë¡œ ë³€í™˜
    for key, value in facilities.items():
        if value and str(value).strip() and str(value).strip() != 'nan':
            # í‚¤ ì´ë¦„ì„ í•œê¸€ë¡œ ë§¤í•‘
            key_mapping = {
                'subway_station': 'ì§€í•˜ì² ',
                'subway': 'ì§€í•˜ì² ',  # subway_stationê³¼ ë™ì¼í•˜ê²Œ ì²˜ë¦¬
                'bus': 'ë²„ìŠ¤',
                'nearby_ë§ˆíŠ¸': 'ë§ˆíŠ¸',
                'nearby_ë³‘ì›': 'ë³‘ì›',
                'nearby_í•™êµ': 'í•™êµ',
                'nearby_ì‹œì„¤': 'ì‹œì„¤',
                'nearby_ì¹´í˜': 'ì¹´í˜'
            }
            
            tag_key = key_mapping.get(key, key)
            tag_value = str(value).strip()
            
            # ì¤‘ë³µ ì²´í¬ (tag + description ì¡°í•©)
            tag_combination = f"{tag_key}:{tag_value}"
            if tag_combination not in added_tags:
                tags.append({
                    'notice_id': notice_id,
                    'tag': tag_key,
                    'description': tag_value
                })
                added_tags.add(tag_combination)
    
    logger.info(f"[DEBUG] normalize_facilities_tags: notice_id={notice_id}, ìƒì„±ëœ íƒœê·¸ ìˆ˜={len(tags)}")
    return tags
#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
ì¸í”„ë¼ ë°ì´í„° ì •ê·œí™” ëª¨ë“ˆ
ì„œìš¸ ì—´ë¦°ë°ì´í„°ê´‘ì¥ API ë°ì´í„°ë¥¼ infra ìŠ¤í‚¤ë§ˆì— ë§ê²Œ ì •ê·œí™”
"""

import pandas as pd
from pathlib import Path
import os
import json
import re
import requests
import time
from typing import List, Dict, Optional, Any, Tuple
import logging
from dotenv import load_dotenv
import chardet
from datetime import datetime

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# JUSO API ëŒ€ì‹  ì¢Œí‘œ APIë§Œ ì‚¬ìš©

logger = logging.getLogger(__name__)

# ----- API í´ë¼ì´ì–¸íŠ¸ í´ë˜ìŠ¤ë“¤ -----

class CoordinateAPI:
    """ì¢Œí‘œ ë³€í™˜ API í´ë¼ì´ì–¸íŠ¸ (ì£¼ì†Œ ì •ê·œí™” + ì¢Œí‘œ ë³€í™˜ í†µí•©)"""
    
    def __init__(self, api_key: str):
        self.api_key = api_key
        self.base_url = "https://api.vworld.kr/req/address"
        self.bcode_data = self._load_bcode_data()
    
    def _load_bcode_data(self) -> Dict[str, str]:
        """ë²•ì •ë™ì½”ë“œ ì „ì²´ìë£Œ.txt íŒŒì¼ì„ ë¡œë“œí•˜ì—¬ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
        bcode_file = Path(__file__).resolve().parents[5] / "backend" / "data" / "rtms" / "ë²•ì •ë™ì½”ë“œ ì „ì²´ìë£Œ.txt"
        
        if not bcode_file.exists():
            logger.error(f"ë²•ì •ë™ì½”ë“œ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {bcode_file}")
            return {}
        
        bcode_dict = {}
        try:
            with open(bcode_file, 'r', encoding='utf-8') as f:
                lines = f.readlines()
                
            # í—¤ë” ìŠ¤í‚µí•˜ê³  ë°ì´í„° íŒŒì‹±
            for line in lines[1:]:
                parts = line.strip().split('\t')
                if len(parts) >= 3:
                    bcode, bname, status = parts[0], parts[1], parts[2]
                    if status == 'ì¡´ì¬':  # íì§€ì—¬ë¶€ê°€ 'ì¡´ì¬'ì¸ ê²½ìš°ë§Œ
                        bcode_dict[bname] = bcode
            
            logger.info(f"ë²•ì •ë™ì½”ë“œ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(bcode_dict)}ê°œ")
            return bcode_dict
            
        except Exception as e:
            logger.error(f"ë²•ì •ë™ì½”ë“œ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return {}
    
    def normalize_address(self, address: str, address_type: str = 'ROAD') -> Dict[str, Any]:
        """ì£¼ì†Œë¥¼ ì •ê·œí™”í•˜ê³  ì¢Œí‘œ, ë²•ì •ë™ì½”ë“œë¥¼ ì¶”ì¶œ"""
        if not address:
            return {
                'address_raw': address,
                'address_nm': None,
                'address_id': None,
                'lat': None,
                'lon': None,
                'normalization_success': False
            }
            
        try:
            params = {
                'service': 'address',
                'request': 'getcoord',
                'version': '2.0',
                'crs': 'epsg:4326',
                'address': address,
                'refine': 'true',
                'simple': 'false',
                'format': 'xml',
                'type': address_type,
                'key': self.api_key
            }
            
            logger.info(f"ğŸŒ ì¢Œí‘œ ë³€í™˜ API ìš”ì²­: {address}")
            logger.info(f"ğŸ“‹ ì¢Œí‘œ ë³€í™˜ API ìš”ì²­ íŒŒë¼ë¯¸í„°: {params}")
            
            response = requests.get(self.base_url, params=params, timeout=10)
            logger.info(f"ğŸ” ì¢Œí‘œ ë³€í™˜ API ì‘ë‹µ ìƒíƒœ: {response.status_code}")
            logger.info(f"ğŸ“„ ì¢Œí‘œ ë³€í™˜ API ì „ì²´ ì‘ë‹µ ë‚´ìš©:")
            logger.info(f"{response.text}")
            
            response.raise_for_status()
            
            # XML íŒŒì‹±í•˜ì—¬ ì¢Œí‘œ, ì£¼ì†Œëª…, ë²•ì •ë™ì½”ë“œ ì¶”ì¶œ
            import xml.etree.ElementTree as ET
            root = ET.fromstring(response.content)
            
            # 1. ì¢Œí‘œ ì¶”ì¶œ
            lat, lon = None, None
            point = root.find('.//point')
            if point is not None:
                x = point.find('x')
                y = point.find('y')
                if x is not None and y is not None and x.text and y.text:
                    try:
                        lon = float(x.text)
                        lat = float(y.text)
                        logger.info(f"âœ… ì¢Œí‘œ ì¶”ì¶œ ì„±ê³µ: lat={lat}, lon={lon}")
                    except ValueError as e:
                        logger.error(f"âŒ ì¢Œí‘œ ë³€í™˜ ì‹¤íŒ¨ (ê°’ ë³€í™˜ ì˜¤ë¥˜): {e}")
            
            # 2. refined structureì—ì„œ ì£¼ì†Œëª… ì¶”ì¶œ
            address_nm = None
            structure = root.find('.//structure')
            if structure is not None:
                level1 = structure.find('level1')
                level2 = structure.find('level2')
                level3 = structure.find('level3')
                
                if level1 is not None and level2 is not None and level3 is not None:
                    if level1.text and level2.text and level3.text:
                        address_nm = f"{level1.text} {level2.text} {level3.text}"
                        logger.info(f"âœ… ì£¼ì†Œëª… ì¶”ì¶œ ì„±ê³µ: {address_nm}")
            
            # 3. ë²•ì •ë™ì½”ë“œ ì¶”ì¶œ
            address_id = None
            if address_nm and self.bcode_data:
                # ë²•ì •ë™ì½”ë“œ ë”•ì…”ë„ˆë¦¬ì—ì„œ ê²€ìƒ‰
                address_id = self.bcode_data.get(address_nm)
                if address_id:
                    logger.info(f"âœ… ë²•ì •ë™ì½”ë“œ ì¶”ì¶œ ì„±ê³µ: {address_id}")
                else:
                    logger.warning(f"âš ï¸ ë²•ì •ë™ì½”ë“œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {address_nm}")
            
            # ì„±ê³µ ì—¬ë¶€ íŒë‹¨
            success = lat is not None and lon is not None
            
            result = {
                'address_raw': address,
                'address_nm': address_nm,
                'address_id': address_id,
                'lat': lat,
                'lon': lon,
                'normalization_success': success
            }
            
            logger.info(f"ğŸ¯ ìµœì¢… ì •ê·œí™” ê²°ê³¼:")
            logger.info(f"   - address_raw: {result['address_raw']}")
            logger.info(f"   - address_nm: {result['address_nm']}")
            logger.info(f"   - address_id: {result['address_id']}")
            logger.info(f"   - lat: {result['lat']}")
            logger.info(f"   - lon: {result['lon']}")
            logger.info(f"   - normalization_success: {result['normalization_success']}")
            
            return result
            
        except Exception as e:
            logger.error(f"ì£¼ì†Œ ì •ê·œí™” ì‹¤íŒ¨: {address} - {e}")
            logger.error(f"ìš”ì²­ URL: {self.base_url}")
            logger.error(f"ìš”ì²­ íŒŒë¼ë¯¸í„°: {params}")
            return {
                'address_raw': address,
                'address_nm': None,
                'address_id': None,
                'lat': None,
                'lon': None,
                'normalization_success': False
            }

def detect_address_type(address: str) -> str:
    """ì£¼ì†Œê°€ ë„ë¡œëª…ì£¼ì†Œì¸ì§€ ì§€ë²ˆì£¼ì†Œì¸ì§€ ê°ì§€"""
    if not address:
        return "unknown"
    
    # ì§€ë²ˆì£¼ì†Œ íŒ¨í„´: ~ë™ + ìˆ«ì (ìš°ì„ ìˆœìœ„ ë†’ìŒ)
    jibun_pattern = r'[ê°€-í£]+ë™\s+\d+[-\d]*'
    if re.search(jibun_pattern, address):
        return "jibun"
    
    # ë„ë¡œëª…ì£¼ì†Œ íŒ¨í„´: ~ë¡œ/ê¸¸/ëŒ€ë¡œ/ê°€ + ìˆ«ì (ë™ëª… ì œì™¸)
    # ë™ëª…ì— "ê¸¸"ì´ í¬í•¨ëœ ê²½ìš°(ì˜ˆ: ì‹ ê¸¸ë™)ëŠ” ì§€ë²ˆì£¼ì†Œë¡œ ì²˜ë¦¬
    road_pattern = r'[ê°€-í£]+(?:ë¡œ|ê¸¸|ëŒ€ë¡œ|ê°€)\d*[ê°€-í£]*\s+\d+[-\d]*'
    if re.search(road_pattern, address):
        return "road"
    
    # 'ì§€í•˜'ê°€ í¬í•¨ëœ ë„ë¡œëª…ì£¼ì†Œ íŒ¨í„´
    if re.search(r'[ê°€-í£]+ë¡œ\s+ì§€í•˜\d+', address) or re.search(r'[ê°€-í£]+ê¸¸\s+ì§€í•˜\d+', address):
        return "road"
    else:
        return "jibun"

def detect_file_encoding(file_path: Path) -> str:
    """íŒŒì¼ì˜ ì¸ì½”ë”©ì„ ìë™ìœ¼ë¡œ ê°ì§€"""
    try:
        with open(file_path, 'rb') as f:
            raw_data = f.read()
            encoding_result = chardet.detect(raw_data)
            detected_encoding = encoding_result['encoding']
            confidence = encoding_result['confidence']
            
        logger.info(f"ğŸ” íŒŒì¼ ì¸ì½”ë”© ê°ì§€: {file_path.name} -> {detected_encoding} (ì‹ ë¢°ë„: {confidence:.2f})")
        
        # ì‹ ë¢°ë„ê°€ ë‚®ê±°ë‚˜ Noneì¸ ê²½ìš° UTF-8ë¡œ í´ë°±
        if not detected_encoding or confidence < 0.7:
            logger.warning(f"âš ï¸ ì¸ì½”ë”© ê°ì§€ ì‹ ë¢°ë„ ë‚®ìŒ, UTF-8ë¡œ í´ë°±: {file_path.name}")
            return 'utf-8'
            
        return detected_encoding
    except Exception as e:
        logger.error(f"âŒ ì¸ì½”ë”© ê°ì§€ ì‹¤íŒ¨: {file_path.name} - {e}")
        return 'utf-8'

def read_csv_with_auto_encoding(file_path: Path, **kwargs) -> pd.DataFrame:
    """ì¸ì½”ë”©ì„ ìë™ìœ¼ë¡œ ê°ì§€í•˜ì—¬ CSV íŒŒì¼ì„ ì½ê¸°"""
    detected_encoding = detect_file_encoding(file_path)
    
    try:
        # ê°ì§€ëœ ì¸ì½”ë”©ìœ¼ë¡œ íŒŒì¼ ì½ê¸°
        df = pd.read_csv(file_path, encoding=detected_encoding, **kwargs)
        logger.info(f"âœ… CSV íŒŒì¼ ì½ê¸° ì„±ê³µ: {file_path.name} (ì¸ì½”ë”©: {detected_encoding})")
        return df
    except UnicodeDecodeError:
        # ê°ì§€ëœ ì¸ì½”ë”©ìœ¼ë¡œ ì‹¤íŒ¨í•˜ë©´ UTF-8ë¡œ ì¬ì‹œë„
        logger.warning(f"âš ï¸ {detected_encoding} ì¸ì½”ë”© ì‹¤íŒ¨, UTF-8ë¡œ ì¬ì‹œë„: {file_path.name}")
        df = pd.read_csv(file_path, encoding='utf-8', **kwargs)
        return df
    except Exception as e:
        logger.error(f"âŒ CSV íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {file_path.name} - {e}")
        raise

def preprocess_subway_address(addr_raw: str) -> str:
    """ì§€í•˜ì² ì—­ ì£¼ì†Œ ì „ì²˜ë¦¬ - ì§€í•˜ì² ì—­ íŠ¹í™” ì „ì²˜ë¦¬"""
    if not addr_raw:
        return addr_raw
    
    import re
    
    # ì§€í•˜ì² ì—­ ê´€ë ¨ ì „ì²˜ë¦¬
    # ì—­ëª…ê³¼ ê´„í˜¸ ì œê±° (ì˜ˆ: "ì‹ ì„¤ë™ì—­(2í˜¸ì„ )" â†’ "")
    addr_raw = re.sub(r'\s+[ê°€-í£]+ì—­\([^)]*\)', '', addr_raw).strip()
    
    # ë‚¨ì€ ê´„í˜¸ ì œê±° (ì˜ˆ: "(2í˜¸ì„ )" â†’ "")
    addr_raw = re.sub(r'\s*\([^)]*\)', '', addr_raw).strip()
    
    return addr_raw

def preprocess_park_address(addr_raw: str) -> str:
    """ê³µì› ì£¼ì†Œ ì „ì²˜ë¦¬ - ê³µì› íŠ¹í™” ì „ì²˜ë¦¬"""
    if not addr_raw:
        return addr_raw
    
    import re
    
    # ê³µì› íŠ¹í™” ì „ì²˜ë¦¬
    # ê´„í˜¸ ì•ˆì˜ ë™ ì •ë³´ ì œê±° (ì˜ˆ: "(ì˜ˆì¥ë™)" â†’ "")
    addr_raw = re.sub(r'\s*\([^)]*ë™[^)]*\)', '', addr_raw).strip()
    
    # ê³µì›ëª… ì œê±° (ì˜ˆ: "ê¸¸ë™ìƒíƒœê³µì›" â†’ "")
    addr_raw = re.sub(r'\s+[ê°€-í£]*ê³µì›[ê°€-í£]*', '', addr_raw).strip()
    
    # ë¶ˆí•„ìš”í•œ ê³µë°± ì •ë¦¬
    addr_raw = re.sub(r'\s+', ' ', addr_raw).strip()
    
    return addr_raw

def detect_address_type_enhanced(address: str) -> str:
    """ì£¼ì†Œ íƒ€ì…ì„ ë” ì •í™•í•˜ê²Œ ê°ì§€"""
    if not address:
        return "unknown"
    
    import re
    
    # ì§€ë²ˆì£¼ì†Œ íŒ¨í„´: ~ë™ + ìˆ«ì (ìš°ì„ ìˆœìœ„ ë†’ìŒ)
    jibun_pattern = r'[ê°€-í£]+ë™\s+\d+[-\d]*'
    if re.search(jibun_pattern, address):
        return "jibun"
    
    # ë„ë¡œëª…ì£¼ì†Œ íŒ¨í„´: ~ë¡œ/ê¸¸/ëŒ€ë¡œ/ê°€ + ìˆ«ì (ë™ëª… ì œì™¸)
    # ë™ëª…ì— "ê¸¸"ì´ í¬í•¨ëœ ê²½ìš°(ì˜ˆ: ì‹ ê¸¸ë™)ëŠ” ì§€ë²ˆì£¼ì†Œë¡œ ì²˜ë¦¬
    road_pattern = r'[ê°€-í£]+(?:ë¡œ|ê¸¸|ëŒ€ë¡œ|ê°€)\d*[ê°€-í£]*\s+\d+[-\d]*'
    if re.search(road_pattern, address):
        return "road"
    
    # ì¼ë°˜ì£¼ì†Œ íŒ¨í„´: ì‹œ/êµ¬/ë™ë§Œ ìˆëŠ” ê²½ìš° (ë™ ë’¤ì— ìˆ«ì ì—†ìŒ)
    general_pattern = r'[ê°€-í£]+\s+[ê°€-í£]+\s+[ê°€-í£]+ë™(?:\s|$)'
    if re.search(general_pattern, address):
        return "general"
    
    # ë” ê°„ë‹¨í•œ ì¼ë°˜ì£¼ì†Œ íŒ¨í„´: ì‹œ êµ¬ ë™ í˜•íƒœ
    simple_general_pattern = r'[ê°€-í£]+\s+[ê°€-í£]+\s+[ê°€-í£]+ë™$'
    if re.search(simple_general_pattern, address):
        return "general"
    
    return "unknown"

def preprocess_address(addr_raw: str) -> str:
    """ê°œì„ ëœ ì£¼ì†Œ ì „ì²˜ë¦¬ - ì¢Œí‘œ API ë§¤ì¹­ë¥  í–¥ìƒì„ ìœ„í•œ ì •ë¦¬"""
    if not addr_raw:
        return addr_raw
    
    import re
    
    # 0. ì¤‘ë³µ ì£¼ì†Œ ì œê±° (ê°€ì¥ ë¨¼ì € ì ìš©)
    # ì˜ˆ: "ì„œìš¸íŠ¹ë³„ì‹œ ì–‘ì²œêµ¬ ëª©ë™ì¤‘ì•™ë³¸ë¡œ20ê¸¸33 (ëª©ë™, ëª©ë™ê³¨ë“ ë¹Œ) ì„œìš¸íŠ¹ë³„ì‹œ ì–‘ì²œêµ¬ ëª©ë™ì¤‘ì•™ë³¸ë¡œ20ê¸¸33 (ëª©ë™, ëª©ë™ê³¨ë“ ë¹Œ)" 
    # -> "ì„œìš¸íŠ¹ë³„ì‹œ ì–‘ì²œêµ¬ ëª©ë™ì¤‘ì•™ë³¸ë¡œ20ê¸¸33 (ëª©ë™, ëª©ë™ê³¨ë“ ë¹Œ)"
    # ë” ì •í™•í•œ ì¤‘ë³µ ì œê±°: ìµœì†Œ 5ê¸€ì ì´ìƒì˜ ì˜ë¯¸ìˆëŠ” ì¤‘ë³µë§Œ ì œê±°
    addr_raw = re.sub(r'(.{5,}?)\s+\1\s*.*$', r'\1', addr_raw)  # ê³µë°± ìˆëŠ” ì¤‘ë³µ (5ê¸€ì ì´ìƒ)
    # ê³µë°± ì—†ëŠ” ì¤‘ë³µì€ ë” ì—„ê²©í•˜ê²Œ: ìµœì†Œ 10ê¸€ì ì´ìƒ
    addr_raw = re.sub(r'(.{10,}?)\1.*$', r'\1', addr_raw)  # ê³µë°± ì—†ëŠ” ì¤‘ë³µ (10ê¸€ì ì´ìƒ)
    
    # 0-1. ì¤‘ë³µ ì£¼ì†Œ ì œê±° (ë” ê°„ë‹¨í•˜ê³  íš¨ê³¼ì ì¸ ë°©ë²•)
    # ì˜ˆ: "ì„œìš¸íŠ¹ë³„ì‹œ ì–‘ì²œêµ¬ ëª©ë™ì¤‘ì•™ë³¸ë¡œ20ê¸¸33 (ëª©ë™, ëª©ë™ê³¨ë“ ë¹Œ) ì„œìš¸íŠ¹ë³„ì‹œ ì–‘ì²œêµ¬ ëª©ë™ì¤‘ì•™ë³¸ë¡œ20ê¸¸33 (ëª©ë™, ëª©ë™ê³¨ë“ ë¹Œ)"
    # -> "ì„œìš¸íŠ¹ë³„ì‹œ ì–‘ì²œêµ¬ ëª©ë™ì¤‘ì•™ë³¸ë¡œ20ê¸¸33 (ëª©ë™, ëª©ë™ê³¨ë“ ë¹Œ)"
    
    # ì„œìš¸íŠ¹ë³„ì‹œë¡œ ì‹œì‘í•˜ëŠ” ì£¼ì†Œì˜ ì¤‘ë³µ ì œê±°
    addr_raw = re.sub(r'(ì„œìš¸íŠ¹ë³„ì‹œ\s+[^ì„œìš¸íŠ¹ë³„ì‹œ]*?)\s+ì„œìš¸íŠ¹ë³„ì‹œ\s+.*$', r'\1', addr_raw)
    
    # ì„œìš¸ë¡œ ì‹œì‘í•˜ëŠ” ì£¼ì†Œì˜ ì¤‘ë³µ ì œê±°
    addr_raw = re.sub(r'(ì„œìš¸\s+[^ì„œìš¸]*?)\s+ì„œìš¸\s+.*$', r'\1', addr_raw)
    
    # 1. ë„ë¡œëª…ì£¼ì†Œ ëì˜ ê´„í˜¸ ì œê±° (ëŒ€í•™êµ ì£¼ì†Œ ë“±) - ê±´ë¬¼ëª…ë§Œ ì œê±°í•˜ê³  ë²ˆì§€ëŠ” ë³´ì¡´
    # ì˜ˆ: "ì„œìš¸ ì¤‘ë‘êµ¬ ì„œì¼ëŒ€í•™ê¸¸ 22(ë©´ëª©ë™ 49-3) ì„œì¼ëŒ€í•™êµ" -> "ì„œìš¸ ì¤‘ë‘êµ¬ ì„œì¼ëŒ€í•™ê¸¸ 22"
    # ~ë¡œ + ìˆ«ì + ê´„í˜¸ íŒ¨í„´ (ê±´ë¬¼ëª…ì´ í¬í•¨ëœ ê´„í˜¸ë§Œ ì œê±°)
    addr_raw = re.sub(r'([ê°€-í£]+ë¡œ\d*[ê°€-í£]*\s+\d+[-\d]*)\s*\([^)]*(?:ëŒ€í•™êµ|ë¹Œë”©|íƒ€ì›Œ|ì„¼í„°|ë§ˆíŠ¸|ëª°|ì•„íŒŒíŠ¸|ì˜¤í”¼ìŠ¤)[^)]*\).*$', r'\1', addr_raw)
    # ~ê¸¸ + ìˆ«ì + ê´„í˜¸ íŒ¨í„´ (ê±´ë¬¼ëª…ì´ í¬í•¨ëœ ê´„í˜¸ë§Œ ì œê±°)
    addr_raw = re.sub(r'([ê°€-í£]+ê¸¸\d*[ê°€-í£]*\s+\d+[-\d]*)\s*\([^)]*(?:ëŒ€í•™êµ|ë¹Œë”©|íƒ€ì›Œ|ì„¼í„°|ë§ˆíŠ¸|ëª°|ì•„íŒŒíŠ¸|ì˜¤í”¼ìŠ¤)[^)]*\).*$', r'\1', addr_raw)
    
    # 2. ê±´ë¬¼ë²ˆí˜¸ê°€ ë¹„ì •ìƒì ìœ¼ë¡œ í° ê²½ìš° ìˆ˜ì • (ì˜ˆ: 217 912 -> 217)
    # ë„ë¡œëª… + ê±´ë¬¼ë²ˆí˜¸ íŒ¨í„´ ì°¾ê¸°
    road_pattern = r'([ê°€-í£]+ë¡œ\d*[ê°€-í£]*)\s+(\d+)\s+(\d{3,})'
    match = re.search(road_pattern, addr_raw)
    if match:
        road_name = match.group(1)
        building_num = match.group(2)
        large_num = match.group(3)
        # í° ë²ˆí˜¸ë¥¼ ì œê±°í•˜ê³  ë„ë¡œëª… + ê±´ë¬¼ë²ˆí˜¸ë§Œ ì‚¬ìš©
        addr_raw = re.sub(road_pattern, f'{road_name} {building_num}', addr_raw)
        logger.info(f"ì£¼ì†Œ ì „ì²˜ë¦¬: ê±´ë¬¼ë²ˆí˜¸ ì •ë¦¬ -> {addr_raw}")
    
    # 2. ì§€ë²ˆ ì£¼ì†ŒëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€ (JUSO APIê°€ ì§€ë²ˆ ì£¼ì†Œë„ ì²˜ë¦¬ ê°€ëŠ¥)
    # ë‹¤ë§Œ ë¶ˆí•„ìš”í•œ ì •ë³´ë§Œ ì œê±°
    
    # 2-1. êµ¬ì²´ì  ê±´ë¬¼ëª… íŒ¨í„´ ì œê±° (ì¸µìˆ˜ ì •ë³´ ì œê±° ì „ì— ì ìš©) - ìƒì„¸ ë²ˆì§€ëŠ” ë³´ì¡´
    # ë„ë¡œëª…+ë²ˆì§€ ì´í›„ì˜ ê±´ë¬¼ëª…ë§Œ ì œê±° (ìƒì„¸ ë²ˆì§€ ì •ë³´ëŠ” ë³´ì¡´)
    # ì˜ˆ: "í•œê°•ë¡œ2ê°€ 112-3 ìš©ì‚°íŒŒí¬ìì´" -> "í•œê°•ë¡œ2ê°€ 112-3"
    # í•œê¸€+ìˆ«ì+ê°€ íŒ¨í„´ê³¼ ìƒì„¸ ë²ˆì§€ ì´í›„ì˜ ê±´ë¬¼ëª… ì œê±° (ë¶€ì§€ë²ˆ ë³´ì¡´)
    # ì£¼ì˜: ë¶€ì§€ë²ˆì´ ìˆëŠ” ê²½ìš°ëŠ” ê±´ë¬¼ëª…ë§Œ ì œê±°í•˜ê³  ë¶€ì§€ë²ˆì€ ë³´ì¡´
    # ì´ íŒ¨í„´ì€ ìƒˆë¡œìš´ ê·œì¹™ì—ì„œ ì²˜ë¦¬í•˜ë¯€ë¡œ ì£¼ì„ ì²˜ë¦¬
    # addr_raw = re.sub(r'([ê°€-í£]+ë¡œ\d*[ê°€-í£]*ê°€\s+\d+[-\d]*)\s+[ê°€-í£A-Z]{3,}(?:ë¹Œ|íƒ€ì›Œ|ì„¼í„°|ë§ˆíŠ¸|ëª°|ì•„íŒŒíŠ¸|ì˜¤í”¼ìŠ¤|ìì´|íŒŒí¬|íìŠ¤|ìœ„ë¸Œ|í‘¸ë¥´ì§€ì˜¤).*$', r'\1', addr_raw)
    addr_raw = re.sub(r'([ê°€-í£]+ê¸¸\s+\d+[-\d]*)\s+[ê°€-í£A-Z]+(?:ë¹Œ|íƒ€ì›Œ|ì„¼í„°|ë§ˆíŠ¸|ëª°|ì•„íŒŒíŠ¸|ì˜¤í”¼ìŠ¤|ìì´|íŒŒí¬|íìŠ¤|ìœ„ë¸Œ|í‘¸ë¥´ì§€ì˜¤).*$', r'\1', addr_raw)
    addr_raw = re.sub(r'([ê°€-í£]+ë¡œ\s+\d+[-\d]*)\s+[ê°€-í£A-Z]+(?:ë¹Œ|íƒ€ì›Œ|ì„¼í„°|ë§ˆíŠ¸|ëª°|ì•„íŒŒíŠ¸|ì˜¤í”¼ìŠ¤|ìì´|íŒŒí¬|íìŠ¤|ìœ„ë¸Œ|í‘¸ë¥´ì§€ì˜¤).*$', r'\1', addr_raw)
    
    # íŠ¹ì • ê±´ë¬¼ëª… íŒ¨í„´ ì œê±°
    addr_raw = re.sub(r'\s+[A-Z]+íƒ€ì›Œ.*$', '', addr_raw)  # "KRíƒ€ì›Œ" ë“±
    addr_raw = re.sub(r'\s+[ê°€-í£]+ì£¼íƒ.*$', '', addr_raw)  # "ì‚¼ì„±ì£¼íƒ" ë“±
    addr_raw = re.sub(r'\s+[ê°€-í£]+ë¹Œë”©.*$', '', addr_raw)  # "ì‚¼ì„±ë¹Œë”©" ë“±
    addr_raw = re.sub(r'\s+[ê°€-í£]+ì„¼í„°.*$', '', addr_raw)  # "ë¬¸í™”ì„¼í„°" ë“±
    
    # ì˜ë¬¸+ìˆ«ì ì¡°í•© ê±´ë¬¼ëª… ì œê±°
    addr_raw = re.sub(r'\s+[A-Z]+\d+.*$', '', addr_raw)  # "KR4", "AB123" ë“±
    
    # 3. ë¶ˆí•„ìš”í•œ ì¸µìˆ˜ ì •ë³´ ì œê±°
    addr_raw = re.sub(r'\s+\d+\s*ì¸µ.*$', '', addr_raw)
    addr_raw = re.sub(r'\s+ì „ì¸µ.*$', '', addr_raw)
    addr_raw = re.sub(r'\s+\d+\s*~\s*\d+\s*ì¸µ.*$', '', addr_raw)
    addr_raw = re.sub(r'\s+\d+,\d+.*ì¸µ.*$', '', addr_raw)  # 2,3,4,5ì¸µ íŒ¨í„´
    addr_raw = re.sub(r'\s+\d+[Ff].*$', '', addr_raw)  # 1F, 2f íŒ¨í„´
    # ì¶”ê°€ ì¸µìˆ˜ íŒ¨í„´ë“¤
    addr_raw = re.sub(r'\s+ì§€ìƒ\d+\s*~\s*\d+\s*ì¸µ.*$', '', addr_raw)  # ì§€ìƒ1ì¸µ~3ì¸µ
    addr_raw = re.sub(r'\s+ì§€í•˜\d+\s*~\s*\d+\s*ì¸µ.*$', '', addr_raw)  # ì§€í•˜1ì¸µ~3ì¸µ
    addr_raw = re.sub(r'\s+ì§€ìƒ\d+\s*ì¸µ.*$', '', addr_raw)  # ì§€ìƒ1ì¸µ
    addr_raw = re.sub(r'\s+ì§€í•˜\d+\s*ì¸µ.*$', '', addr_raw)  # ì§€í•˜1ì¸µ
    # ìƒˆë¡œ ì¶”ê°€: ë” ê°•ë ¥í•œ ì¸µìˆ˜ ì •ë³´ ì œê±°
    addr_raw = re.sub(r'\s+ì§€í•˜\d*ì¸µ.*$', '', addr_raw)  # ì§€í•˜1ì¸µ, ì§€í•˜ì¸µ ë“±
    addr_raw = re.sub(r'\s+ì§€ìƒ\d*ì¸µ.*$', '', addr_raw)  # ì§€ìƒ2ì¸µ, ì§€ìƒì¸µ ë“±
    addr_raw = re.sub(r'\s+B\d+.*$', '', addr_raw)  # B1, B2 ë“±
    
    # 4. ê±´ë¬¼ëª… ì œê±° (ê´„í˜¸ ì•ˆì˜ ë‚´ìš©)
    addr_raw = re.sub(r'\s*\([^)]*\)\s*', ' ', addr_raw)
    addr_raw = re.sub(r'\s+[ê°€-í£]+ìƒí™œ.*$', '', addr_raw)
    addr_raw = re.sub(r'\s+[ê°€-í£]+ë¹Œ.*$', '', addr_raw)
    # ì¶”ê°€ ê±´ë¬¼ëª… íŒ¨í„´ë“¤
    addr_raw = re.sub(r'\s+ì• ìŠ¤íŠ¸ë¦¬\d+.*$', '', addr_raw)  # ì• ìŠ¤íŠ¸ë¦¬23
    addr_raw = re.sub(r'\s+ì‚¬ëŠ”ìë¦¬.*$', '', addr_raw)  # ì‚¬ëŠ”ìë¦¬
    addr_raw = re.sub(r'\s+ì¨ë“œí”Œë ˆì´ìŠ¤.*$', '', addr_raw)  # ì¨ë“œí”Œë ˆì´ìŠ¤ í™ì€7
    addr_raw = re.sub(r'\s+ì½”ì´ë…¸ë‹ˆì•„.*$', '', addr_raw)  # ì½”ì´ë…¸ë‹ˆì•„ìŠ¤í…Œì´
    addr_raw = re.sub(r'\s+ë§‘ì€êµ¬ë¦„ì§‘.*$', '', addr_raw)  # ë§‘ì€êµ¬ë¦„ì§‘
    addr_raw = re.sub(r'\s+ë„ˆë‚˜ë“¤ì´.*$', '', addr_raw)  # ë„ˆë‚˜ë“¤ì´
    addr_raw = re.sub(r'\s+í™”ê³¡ë™.*$', '', addr_raw)  # í™”ê³¡ë™ ê³µë™ì²´ì£¼íƒ
    # ì¶”ê°€ ê±´ë¬¼ëª… íŒ¨í„´ë“¤ (sohouse ì‹¤íŒ¨ ì¼€ì´ìŠ¤)
    addr_raw = re.sub(r'\s+ë…¹ìƒ‰ì¹œêµ¬ë“¤.*$', '', addr_raw)  # ë…¹ìƒ‰ì¹œêµ¬ë“¤ ëŒ€ì¡°, ë…¹ìƒ‰ì¹œêµ¬ë“¤ ì°½ì²œ
    addr_raw = re.sub(r'\s+í•¨ê»˜ì£¼íƒ.*$', '', addr_raw)  # í•¨ê»˜ì£¼íƒ6í˜¸, í•¨ê»˜ì£¼íƒ5í˜¸
    addr_raw = re.sub(r'\s+ì£¼ìƒë³µí•©ê±´ë¬¼.*$', '', addr_raw)  # ì£¼ìƒë³µí•©ê±´ë¬¼
    
    
    # 5. ë™/í˜¸ìˆ˜ ì œê±° ê°•í™”
    addr_raw = re.sub(r'\s+\d+ë™(\s+\d+í˜¸)?.*$', '', addr_raw)  # "101ë™", "101ë™ 102í˜¸" ë“±
    addr_raw = re.sub(r'\s+\d+í˜¸.*$', '', addr_raw)  # "102í˜¸" ë“±
    addr_raw = re.sub(r'\s+[A-Z]\d+í˜¸.*$', '', addr_raw)  # "A101í˜¸", "B205í˜¸" ë“±
    # ì¶”ê°€: ëª¨ë“  ë™/í˜¸ìˆ˜ íŒ¨í„´ ì œê±° (ë‚˜í˜¸, ë‹¤í˜¸ ë“± í¬í•¨)
    addr_raw = re.sub(r'\s+[ê°€-í£]+í˜¸.*$', '', addr_raw)  # "ë‚˜í˜¸", "ë‹¤í˜¸" ë“± ëª¨ë“  í˜¸ìˆ˜
    
    # 6. ì§€ë²ˆì£¼ì†Œ ì •ë¦¬ (~ë™ ìˆ«ì ë˜ëŠ” ~ë™ ìˆ«ì-ìˆ«ì ë’¤ì˜ ëª¨ë“  ë¬¸ì ì œê±°)
    # ì˜ˆ: "ì„œìš¸ ì¢…ë¡œêµ¬ ë™ìˆ­ë™ 192-6 1" -> "ì„œìš¸ ì¢…ë¡œêµ¬ ë™ìˆ­ë™ 192-6"
    # ì˜ˆ: "ì„œìš¸ ê´‘ì§„êµ¬ ëŠ¥ë™ 25 ì„ í™”ì˜ˆìˆ ì¤‘ê³ ë“±í•™êµ" -> "ì„œìš¸ ê´‘ì§„êµ¬ ëŠ¥ë™ 25"
    # ì£¼ì˜: ë¶€ì§€ë²ˆì´ ìˆëŠ” ê²½ìš°ëŠ” ì œì™¸ (ì˜ˆ: ì„±ìˆ˜ë™1ê°€ 685-20)
    # ì´ íŒ¨í„´ì€ ìƒˆë¡œìš´ ê·œì¹™ì—ì„œ ì²˜ë¦¬í•˜ë¯€ë¡œ ì£¼ì„ ì²˜ë¦¬
    # addr_raw = re.sub(r'([ê°€-í£]+ë™\s+\d+)\s+[ê°€-í£].*$', r'\1', addr_raw)  # ë¶€ì§€ë²ˆ ì—†ëŠ” ê²½ìš°ë§Œ
    
    # ìƒˆë¡œ ì¶”ê°€: ë„ë¡œëª…+ë²ˆì§€ ì´í›„ì˜ ê±´ë¬¼ëª… ì œê±° ê°•í™”
    # ë„ë¡œëª…ê¸¸+ìˆ«ì ì´í›„ ëª¨ë“  í•œê¸€ ì œê±°
    addr_raw = re.sub(r'([ê°€-í£]+ê¸¸\s+\d+[-\d]*)\s+[ê°€-í£]+.*$', r'\1', addr_raw)
    # ë„ë¡œëª…ë¡œ+ìˆ«ì ì´í›„ ëª¨ë“  í•œê¸€ ì œê±° (ê¸¸ì´ ì—†ëŠ” ê²½ìš°)
    if 'ê¸¸' not in addr_raw:
        addr_raw = re.sub(r'([ê°€-í£]+ë¡œ\s+\d+[-\d]*)\s+[ê°€-í£]+.*$', r'\1', addr_raw)
    
    # ìˆ«ì-ìˆ«ì íŒ¨í„´ ì´í›„ ëª¨ë“  ê²ƒ ì œê±° (ë‹¨, ì§€ë²ˆì£¼ì†ŒëŠ” ë³´ì¡´)
    # ì˜ˆ: "385-16 ì‚¼ì„±ì£¼íƒ" â†’ "385-16"
    # ë„ë¡œëª…ì´ ì•„ë‹Œ ê²½ìš°ì—ë§Œ ì ìš© (ì§€ë²ˆì£¼ì†Œ ë³´ì¡´)
    if not re.search(r'[ê°€-í£]+ë¡œ\s+\d+[-\d]*', addr_raw) and not re.search(r'[ê°€-í£]+ê¸¸\s+\d+[-\d]*', addr_raw):
        addr_raw = re.sub(r'(\d+-\d+)\s+[ê°€-í£A-Z].*$', r'\1', addr_raw)
    
    # 6. ë„ë¡œëª…+ê±´ë¬¼ë²ˆí˜¸ ë¶„ë¦¬ (ì¼ë°˜í™”ëœ íŒ¨í„´) - ìƒì„¸ ë²ˆì§€ ë³´ì¡´
    # 6-1. *ë¡œ*ê¸¸+ìˆ«ì -> *ë¡œ*ê¸¸ ìˆ«ì (ì˜ˆ: ë¶ì•…ì‚°ë¡œ3ê¸¸44 -> ë¶ì•…ì‚°ë¡œ3ê¸¸ 44)
    # ë” ì •í™•í•œ íŒ¨í„´ìœ¼ë¡œ ìˆ˜ì •: ~ë¡œë¡œ ëë‚˜ê³  ~ê¸¸ë¡œ ëë‚˜ëŠ” íŒ¨í„´ (ìƒì„¸ ë²ˆì§€ ë³´ì¡´)
    # ì£¼ì˜: ì´ë¯¸ ê³µë°±ì´ ìˆëŠ” ê²½ìš°ëŠ” ì œì™¸
    addr_raw = re.sub(r'([ê°€-í£]+ë¡œ\d*[ê°€-í£]*ê¸¸)(\d+[-\d]*)', r'\1 \2', addr_raw)
    
    # 6-2. *ë¡œ+ìˆ«ì -> *ë¡œ ìˆ«ì (ê¸¸ì´ ì—†ëŠ” ê²½ìš°ë§Œ) - ìƒì„¸ ë²ˆì§€ ë³´ì¡´
    # ë‹¨, ~ê¸¸ì´ í¬í•¨ëœ ê²½ìš°ëŠ” ì œì™¸ (ì—°í¬ë¡œ18ê¸¸ -> ì—°í¬ë¡œ18ê¸¸ ìœ ì§€)
    # ìƒì„¸ ë²ˆì§€(ì˜ˆ: 112-3, 581, 685-20) ë³´ì¡´
    # ì£¼ì˜: ì´ë¯¸ ê³µë°±ì´ ìˆëŠ” ê²½ìš°ëŠ” ì œì™¸
    if 'ê¸¸' not in addr_raw:
        addr_raw = re.sub(r'([ê°€-í£]+ë¡œ)(\d+[-\d]*)', r'\1 \2', addr_raw)
    
    # 7. ë„ë¡œëª…ì£¼ì†Œ ì •ë¦¬ - ê±´ë¬¼ëª…ë§Œ ì œê±°í•˜ê³  ìƒì„¸ ë²ˆì§€ëŠ” ë³´ì¡´
    # ì˜ˆ: "ì„œìš¸ ì„œëŒ€ë¬¸êµ¬ ì—°í¬ë¡œ18ê¸¸ 36 ì• ìŠ¤íŠ¸ë¦¬23" -> "ì„œìš¸ ì„œëŒ€ë¬¸êµ¬ ì—°í¬ë¡œ18ê¸¸ 36"
    # ì£¼ì˜: "ë‹¤ê¸¸", "ë‚˜ê¸¸" ë“±ì€ ì‹¤ì œ ë„ë¡œëª…ì´ë¯€ë¡œ ê±´ë¬¼ë²ˆí˜¸ë¥¼ ì œê±°í•˜ë©´ ì•ˆë¨
    # ~ê¸¸ìˆ«ì(-ìˆ«ì) íŒ¨í„´ ë’¤ì˜ ê±´ë¬¼ëª…ë§Œ ì œê±° (ìƒì„¸ ë²ˆì§€ëŠ” ë³´ì¡´)
    if not re.search(r'[ë‹¤ë‚˜]ê¸¸', addr_raw):
        # ê±´ë¬¼ëª… íŒ¨í„´ë§Œ ì œê±° (ìƒì„¸ ë²ˆì§€ ì •ë³´ëŠ” ë³´ì¡´)
        addr_raw = re.sub(r'([ê°€-í£]+ê¸¸\d+[-\d]*)\s+[ê°€-í£A-Z]+(?:ë¹Œ|íƒ€ì›Œ|ì„¼í„°|ë§ˆíŠ¸|ëª°|ì•„íŒŒíŠ¸|ì˜¤í”¼ìŠ¤|ìì´|íŒŒí¬|íìŠ¤|ìœ„ë¸Œ|í‘¸ë¥´ì§€ì˜¤|ìƒí™œ|ê´€ë¦¬|í‚¤ì¦ˆ).*$', r'\1', addr_raw)
    # ~ê¸¸ ìˆ«ì(-ìˆ«ì) íŒ¨í„´ ë’¤ì˜ ê±´ë¬¼ëª…ë§Œ ì œê±°
    if not re.search(r'[ë‹¤ë‚˜]ê¸¸', addr_raw):
        addr_raw = re.sub(r'([ê°€-í£]+ê¸¸\s+\d+[-\d]*)\s+[ê°€-í£A-Z]+(?:ë¹Œ|íƒ€ì›Œ|ì„¼í„°|ë§ˆíŠ¸|ëª°|ì•„íŒŒíŠ¸|ì˜¤í”¼ìŠ¤|ìì´|íŒŒí¬|íìŠ¤|ìœ„ë¸Œ|í‘¸ë¥´ì§€ì˜¤|ìƒí™œ|ê´€ë¦¬|í‚¤ì¦ˆ).*$', r'\1', addr_raw)
    # ~ë¡œ ìˆ«ì(-ìˆ«ì) íŒ¨í„´ ë’¤ì˜ ê±´ë¬¼ëª…ë§Œ ì œê±° (ê¸¸ì´ ì—†ëŠ” ê²½ìš°)
    addr_raw = re.sub(r'([ê°€-í£]+ë¡œ\s+\d+[-\d]*)\s+[ê°€-í£A-Z]+(?:ë¹Œ|íƒ€ì›Œ|ì„¼í„°|ë§ˆíŠ¸|ëª°|ì•„íŒŒíŠ¸|ì˜¤í”¼ìŠ¤|ìì´|íŒŒí¬|íìŠ¤|ìœ„ë¸Œ|í‘¸ë¥´ì§€ì˜¤|ìƒí™œ|ê´€ë¦¬|í‚¤ì¦ˆ).*$', r'\1', addr_raw)
    
    # 8. ~ë¡œ ì£¼ì†Œì—ì„œ ~ë¡œ ì œê±° (ê¸¸ì´ ì—†ëŠ” ê²½ìš°)
    # ì˜ˆ: "ì„œìš¸íŠ¹ë³„ì‹œ ë§ˆí¬êµ¬ ì™€ìš°ì‚°ë¡œ ìƒìˆ˜ë™ 321-6" -> "ì„œìš¸íŠ¹ë³„ì‹œ ë§ˆí¬êµ¬ ìƒìˆ˜ë™ 321-6"
    # ~ê¸¸ì´ ì—†ëŠ” ì£¼ì†Œì—ì„œë§Œ ~ë¡œ ì œê±°
    if 'ê¸¸' not in addr_raw and 'ë¡œ' in addr_raw:
        addr_raw = re.sub(r'([ê°€-í£]+êµ¬)\s+([ê°€-í£]+ë¡œ)\s+([ê°€-í£]+ë™)', r'\1 \3', addr_raw)
    
    # 9. ì§€ë²ˆ ì£¼ì†Œ ë’¤ ì¶”ê°€ ì •ë³´ ì œê±° (ì¦ì‚°ë™ 202-25 ë“±)
    # ì˜ˆ: "ì¦ì‚°ë¡œ9ê¸¸ 26-21 ì¦ì‚°ë™ 202-25" -> "ì¦ì‚°ë¡œ9ê¸¸ 26-21"
    addr_raw = re.sub(r'([ê°€-í£]+ë¡œ\d*[ê°€-í£]*ê¸¸\s+\d+[-\d]*)\s+[ê°€-í£]+ë™\s+\d+[-\d]*.*$', r'\1', addr_raw)
    # ì¶”ê°€ ì§€ë²ˆ ì£¼ì†Œ ì œê±° (ì„œìš¸ì‹œ ê¸ˆì²œêµ¬ ë…ì‚°ë™ 964-42 ë“±)
    addr_raw = re.sub(r'([ê°€-í£]+ë¡œ\d*[ê°€-í£]*ê¸¸\s+\d+[-\d]*)\s+ì„œìš¸[ì‹œíŠ¹ë³„ì‹œ]*\s+[ê°€-í£]+êµ¬\s+[ê°€-í£]+ë™\s+\d+[-\d]*.*$', r'\1', addr_raw)
    
    # 10. ë§ˆì¹¨í‘œ ë° ë¶ˆí•„ìš”í•œ ë¬¸ì ì œê±°
    addr_raw = re.sub(r'\.+$', '', addr_raw)  # ëì˜ ë§ˆì¹¨í‘œ ì œê±°
    addr_raw = re.sub(r'\s+$', '', addr_raw)  # ëì˜ ê³µë°± ì œê±°
    
    # 12. ì•„íŒŒíŠ¸/ê±´ë¬¼ ë‚´ë¶€ ì •ë³´ ì œê±° (infra ì‹¤íŒ¨ ì¼€ì´ìŠ¤ ëŒ€ì‘)
    # ë™/í˜¸ìˆ˜ ì •ë³´ ì œê±° (ì˜ˆ: 101ë™ 102í˜¸, 13ë™ 204í˜¸)
    addr_raw = re.sub(r'\s+\d+ë™\s+\d+í˜¸.*$', '', addr_raw)
    addr_raw = re.sub(r'\s+\d+ë™\s+\d+,\s*\d+í˜¸.*$', '', addr_raw)
    # ì¸µìˆ˜ ì •ë³´ ì œê±° (ì˜ˆ: 4ì¸µ, 3ì¸µ, 1ì¸µ)
    addr_raw = re.sub(r'\s+\d+ì¸µ.*$', '', addr_raw)
    # ë‹¨ì§€/ì•„íŒŒíŠ¸ ë‚´ë¶€ ì •ë³´ ì œê±°
    addr_raw = re.sub(r'\s+ë‹¨ì§€\s*ë‚´.*$', '', addr_raw)
    addr_raw = re.sub(r'\s+ê´€ë¦¬ë™.*$', '', addr_raw)
    addr_raw = re.sub(r'\s+ê´€ë¦¬ì‹¤.*$', '', addr_raw)
    addr_raw = re.sub(r'\s+í‚¤ì¦ˆì„¼í„°\d+ì¸µ.*$', '', addr_raw)
    addr_raw = re.sub(r'\s+í‚¤ì¦ˆí´ëŸ½.*$', '', addr_raw)
    # ìƒì„¸ ìœ„ì¹˜ ì •ë³´ ì œê±° (ì˜ˆ: B113í˜¸, 101-2í˜¸, 206-2í˜¸)
    addr_raw = re.sub(r'\s+[A-Z]\d+í˜¸.*$', '', addr_raw)  # B113í˜¸, A101í˜¸ ë“±
    addr_raw = re.sub(r'\s+\d+-\d+í˜¸.*$', '', addr_raw)  # 101-2í˜¸, 206-2í˜¸ ë“±
    addr_raw = re.sub(r'\s+\d+,\d+í˜¸.*$', '', addr_raw)  # 105,106í˜¸ ë“±
    # ì§€í•˜/ì§€ìƒ ì •ë³´ ì œê±°
    addr_raw = re.sub(r'\s+ì§€í•˜\d+.*$', '', addr_raw)
    addr_raw = re.sub(r'\s+ì§€ìƒ\s*\d+.*$', '', addr_raw)
    # B1ì¸µ, B2ì¸µ ë“± ì œê±°
    addr_raw = re.sub(r'\s+B\d+ì¸µ.*$', '', addr_raw)
    # ìƒê°€/ë¹Œë”© ë‚´ë¶€ ì •ë³´ ì œê±°
    addr_raw = re.sub(r'\s+ìƒê°€ë™.*$', '', addr_raw)
    addr_raw = re.sub(r'\s+ë¹Œë”©.*$', '', addr_raw)
    addr_raw = re.sub(r'\s+íƒ€ì›Œ.*$', '', addr_raw)
    # ì£¼ë¯¼ê³µë™ì‹œì„¤, ì¢…í•©ìƒê°€ ë“± ì œê±°
    addr_raw = re.sub(r'\s+ì£¼ë¯¼ê³µë™ì‹œì„¤.*$', '', addr_raw)
    addr_raw = re.sub(r'\s+ì¢…í•©ìƒê°€.*$', '', addr_raw)
    # ì„¼í„°/ê´€ë ¨ ì •ë³´ ì œê±°
    addr_raw = re.sub(r'\s+ê³ ê°ì„¼í„°\d+ì¸µ.*$', '', addr_raw)
    addr_raw = re.sub(r'\s+ê·¼ë¡œë³µì§€ê´€\d+ì¸µ.*$', '', addr_raw)
    addr_raw = re.sub(r'\s+êµìœ¡ê´€.*$', '', addr_raw)
    addr_raw = re.sub(r'\s+ë¬¸í™”ê±´ê°•ì„¼í„°.*$', '', addr_raw)
    addr_raw = re.sub(r'\s+ì—´ë¦°ë¬¸í™”ì„¼í„°.*$', '', addr_raw)
    
    # 13. ë³µì¡í•œ ê±´ë¬¼ëª… íŒ¨í„´ ì œê±° (infra íŠ¹í™”)
    # ì•„íŒŒíŠ¸ ë‹¨ì§€ëª… ì œê±°
    addr_raw = re.sub(r'\s+[ê°€-í£]+ì•„íŒŒíŠ¸.*$', '', addr_raw)
    addr_raw = re.sub(r'\s+[ê°€-í£]+íƒ€ìš´.*$', '', addr_raw)
    addr_raw = re.sub(r'\s+[ê°€-í£]+ì‹œí‹°.*$', '', addr_raw)
    addr_raw = re.sub(r'\s+[ê°€-í£]+íŒŒí¬.*$', '', addr_raw)
    addr_raw = re.sub(r'\s+[ê°€-í£]+íìŠ¤.*$', '', addr_raw)
    addr_raw = re.sub(r'\s+[ê°€-í£]+ìœ„ë¸Œ.*$', '', addr_raw)
    addr_raw = re.sub(r'\s+[ê°€-í£]+ìì´.*$', '', addr_raw)
    addr_raw = re.sub(r'\s+[ê°€-í£]+í‘¸ë¥´ì§€ì˜¤.*$', '', addr_raw)
    addr_raw = re.sub(r'\s+[ê°€-í£]+í”„ë ˆìŠ¤í‹°ì§€.*$', '', addr_raw)
    addr_raw = re.sub(r'\s+[ê°€-í£]+íƒ€ìš´.*$', '', addr_raw)
    addr_raw = re.sub(r'\s+[ê°€-í£]+íƒ€ì›Œ.*$', '', addr_raw)
    addr_raw = re.sub(r'\s+[ê°€-í£]+ë¸Œì´ì›.*$', '', addr_raw)
    addr_raw = re.sub(r'\s+[ê°€-í£]+ì—ì½”.*$', '', addr_raw)
    addr_raw = re.sub(r'\s+[ê°€-í£]+ë¦¬ë²„.*$', '', addr_raw)
    addr_raw = re.sub(r'\s+[ê°€-í£]+ë”í”„ë ˆí‹°ì›€.*$', '', addr_raw)
    addr_raw = re.sub(r'\s+[ê°€-í£]+ìŠ¤ìœ„ì²¸.*$', '', addr_raw)
    addr_raw = re.sub(r'\s+[ê°€-í£]+ë¦¬ì²¸ì‹œì•„.*$', '', addr_raw)
    addr_raw = re.sub(r'\s+[ê°€-í£]+í”Œë ˆì´ìŠ¤.*$', '', addr_raw)
    addr_raw = re.sub(r'\s+[ê°€-í£]+ì„¼íŠ¸ëŸ´.*$', '', addr_raw)
    addr_raw = re.sub(r'\s+[ê°€-í£]+ê·¸ë¼ì‹œì›€.*$', '', addr_raw)
    addr_raw = re.sub(r'\s+[ê°€-í£]+ì•„ì´íŒŒí¬.*$', '', addr_raw)
    addr_raw = re.sub(r'\s+[ê°€-í£]+ì•„ì¹´ë°ë¯¸.*$', '', addr_raw)
    addr_raw = re.sub(r'\s+[ê°€-í£]+ë¸Œì´ì›.*$', '', addr_raw)
    addr_raw = re.sub(r'\s+[ê°€-í£]+ì—”ì§€ë‹ˆì–´ë§.*$', '', addr_raw)
    addr_raw = re.sub(r'\s+[ê°€-í£]+ë©”ë””ì»¬.*$', '', addr_raw)
    addr_raw = re.sub(r'\s+[ê°€-í£]+í™ˆìºìŠ¤íŠ¸.*$', '', addr_raw)
    addr_raw = re.sub(r'\s+[ê°€-í£]+ìŠ¤ìœ„íŠ¸.*$', '', addr_raw)
    
    # 14. ê´„í˜¸ ì•ˆì˜ ìƒì„¸ ì •ë³´ ì œê±° (ì˜ˆ: (ë§ìš°ë³¸ë™), (ì •ë¦‰ë™))
    addr_raw = re.sub(r'\s*\([^)]*ë™[^)]*\)\s*', ' ', addr_raw)
    addr_raw = re.sub(r'\s*\([^)]*ê°€[^)]*\)\s*', ' ', addr_raw)
    
    # 15. ëŒ€í•™êµëª… ì œê±° (ê´„í˜¸ ì œê±° í›„)
    # ì˜ˆ: "ì„œìš¸ êµ¬ë¡œêµ¬ í•­ë™ ì„±ê³µíšŒëŒ€í•™êµ" â†’ "ì„œìš¸ êµ¬ë¡œêµ¬ í•­ë™"
    # ë™ ë’¤ì— ëŒ€í•™êµëª…ì´ ìˆëŠ” ê²½ìš°ë§Œ ì œê±°
    addr_raw = re.sub(r'(.*ë™)\s+.*(?:ëŒ€í•™êµ|ëŒ€í•™|í•™êµ).*$', r'\1', addr_raw)
    
    # 16. ê³µë°± ì •ë¦¬
    addr_raw = re.sub(r'\s+', ' ', addr_raw).strip()

    # --- ğŸ”¥ [ì¶”ê°€: ì£¼íƒ ì •ê·œí™” ì „ìš© ê·œì¹™] ---
    # 1. ì‰¼í‘œ(,) ë’¤ ìƒì„¸ì •ë³´ ì œê±° â†’ "ì„œìš¸ ì„±ë¶êµ¬ ë™ì†Œë¬¸ë¡œ 305, ì§€ì¸µ" â†’ "ì„œìš¸ ì„±ë¶êµ¬ ë™ì†Œë¬¸ë¡œ 305"
    addr_raw = re.sub(r',\s*[^,]+$', '', addr_raw)

    # 2. 'ì§€ìƒ', 'ì§€í•˜', 'ì¸µ', 'í˜¸' ë“± ì„¸ë¶€ìœ„ì¹˜ ì œê±°
    addr_raw = re.sub(r'\s*ì§€[ìƒí•˜]\d*ì¸µ?', '', addr_raw)
    addr_raw = re.sub(r'\s*\d+ì¸µ', '', addr_raw)
    addr_raw = re.sub(r'\s*\d+í˜¸', '', addr_raw)
    addr_raw = re.sub(r'\s*\d+ë™', '', addr_raw)

    # 3. ê´„í˜¸ ì•ˆì— ìˆëŠ” ë™/ê°€/ê±´ë¬¼ëª… ì œê±°
    addr_raw = re.sub(r'\([^)]*(ë™|ê°€|ì•„íŒŒíŠ¸|ë¹Œë¼|ë‹¨ì§€|ìƒê°€|íƒ€ì›Œ)[^)]*\)', '', addr_raw)

    # 4. "ì•", "ë‚´", "ê´€ë¦¬ë™", "ê´€ë¦¬ì‹¤" ë“± ë¶ˆí•„ìš”í•œ ë‹¨ì–´ ì œê±°
    addr_raw = re.sub(r'\s*(ì•|ë‚´|ê´€ë¦¬ë™|ê´€ë¦¬ì‹¤|ì£¼ë¯¼ì„¼í„°|í‚¤ì¦ˆì„¼í„°\d*ì¸µ?|ë¬¸í™”ì„¼í„°).*$', '', addr_raw)

    # 5. ì£¼ì†Œ ë ë¶ˆí•„ìš”í•œ ë¬¸ì ì œê±°
    addr_raw = re.sub(r'[-,\s]+$', '', addr_raw)

    # 6. ë„ë¡œëª… + ë„ë¡œëª…ì£¼ì†Œë¶€ë²ˆê¹Œì§€ë§Œ ì¸ì‹í•˜ê³  ê·¸ ë’¤ëŠ” ëª¨ë‘ ì œê±° - ê±´ë¬¼ëª…ë§Œ ì œê±°
    # ~ê¸¸ + ìˆ«ì íŒ¨í„´ (ë„ë¡œëª…ì£¼ì†Œë¶€ë²ˆê¹Œì§€) - ê±´ë¬¼ëª…ë§Œ ì œê±°, ìƒì„¸ ë²ˆì§€ëŠ” ë³´ì¡´
    addr_raw = re.sub(r'([ê°€-í£]+ê¸¸\d*[ê°€-í£]*\s+\d+[-\d]*)\s+[ê°€-í£A-Z]+(?:ë¹Œ|íƒ€ì›Œ|ì„¼í„°|ë§ˆíŠ¸|ëª°|ì•„íŒŒíŠ¸|ì˜¤í”¼ìŠ¤|ìì´|íŒŒí¬|íìŠ¤|ìœ„ë¸Œ|í‘¸ë¥´ì§€ì˜¤|ìƒí™œ|ê´€ë¦¬|í‚¤ì¦ˆ).*$', r'\1', addr_raw)
    # ~ë¡œ + ìˆ«ì íŒ¨í„´ (ë„ë¡œëª…ì£¼ì†Œë¶€ë²ˆê¹Œì§€) - ê¸¸ì´ ì—†ëŠ” ê²½ìš°ë§Œ, ê±´ë¬¼ëª…ë§Œ ì œê±°
    if 'ê¸¸' not in addr_raw:
        addr_raw = re.sub(r'([ê°€-í£]+ë¡œ\d*[ê°€-í£]*\s+\d+[-\d]*)\s+[ê°€-í£A-Z]+(?:ë¹Œ|íƒ€ì›Œ|ì„¼í„°|ë§ˆíŠ¸|ëª°|ì•„íŒŒíŠ¸|ì˜¤í”¼ìŠ¤|ìì´|íŒŒí¬|íìŠ¤|ìœ„ë¸Œ|í‘¸ë¥´ì§€ì˜¤|ìƒí™œ|ê´€ë¦¬|í‚¤ì¦ˆ).*$', r'\1', addr_raw)
    
    # 6-1. íŠ¹ìˆ˜ ì¼€ì´ìŠ¤: ë„ë¡œëª…ì´ ë„ˆë¬´ êµ¬ì²´ì ì¸ ê²½ìš° ë” ê°„ë‹¨í•˜ê²Œ
    # ì£¼ì˜: "8ê¸¸", "9ê¸¸" ë“±ì€ ì‹¤ì œ ë„ë¡œëª…ì´ë¯€ë¡œ ì œê±°í•˜ë©´ ì•ˆë¨
    # ê´‘í‰ë¡œ34ê¸¸ -> ê´‘í‰ë¡œë¡œ ë‹¨ìˆœí™” (ë‹¨, ì¼ë°˜ì ì¸ ìˆ«ìê¸¸ì€ ì œì™¸)
    # addr_raw = re.sub(r'([ê°€-í£]+ë¡œ)\d+ê¸¸', r'\1', addr_raw)  # ì£¼ì„ ì²˜ë¦¬
    # ë„ë¡œëª… + ìˆ«ì ë’¤ì˜ ê±´ë¬¼ëª…ë§Œ ì œê±° (ê¸¸ì´ ì—†ëŠ” ê²½ìš°ë§Œ) - ìƒì„¸ ë²ˆì§€ëŠ” ë³´ì¡´
    if 'ê¸¸' not in addr_raw:
        addr_raw = re.sub(r'([ê°€-í£]+ë¡œ\s+\d+[-\d]*)\s+[ê°€-í£A-Z]+(?:ë¹Œ|íƒ€ì›Œ|ì„¼í„°|ë§ˆíŠ¸|ëª°|ì•„íŒŒíŠ¸|ì˜¤í”¼ìŠ¤|ìì´|íŒŒí¬|íìŠ¤|ìœ„ë¸Œ|í‘¸ë¥´ì§€ì˜¤|ìƒí™œ|ê´€ë¦¬|í‚¤ì¦ˆ).*$', r'\1', addr_raw)
    
    # 7. ì–´ë¦°ì´ì§‘/ìœ ì¹˜ì› ë“± ì‹œì„¤ëª… ì œê±° (ë” êµ¬ì²´ì ì¸ íŒ¨í„´)
    # ~ê¸¸/ë¡œ + ìˆ«ì + ì‹œì„¤ëª… íŒ¨í„´
    addr_raw = re.sub(r'([ê°€-í£]+ê¸¸\d*[ê°€-í£]*\s+\d+)\s+[ê°€-í£]*(ì–´ë¦°ì´ì§‘|ìœ ì¹˜ì›|í•™êµ|ë³‘ì›|ì•½êµ­|ë§ˆíŠ¸|í¸ì˜ì |ê³µì›|ì²´ìœ¡ê´€|ë¬¸í™”ì„¼í„°|ì£¼ë¯¼ì„¼í„°|ì„¼í„°|ê´€ë¦¬ë™|ê´€ë¦¬ì‹¤|í‚¤ì¦ˆì„¼í„°|í‚¤ì¦ˆí´ëŸ½).*$', r'\1', addr_raw)
    addr_raw = re.sub(r'([ê°€-í£]+ë¡œ\d*[ê°€-í£]*\s+\d+)\s+[ê°€-í£]*(ì–´ë¦°ì´ì§‘|ìœ ì¹˜ì›|í•™êµ|ë³‘ì›|ì•½êµ­|ë§ˆíŠ¸|í¸ì˜ì |ê³µì›|ì²´ìœ¡ê´€|ë¬¸í™”ì„¼í„°|ì£¼ë¯¼ì„¼í„°|ì„¼í„°|ê´€ë¦¬ë™|ê´€ë¦¬ì‹¤|í‚¤ì¦ˆì„¼í„°|í‚¤ì¦ˆí´ëŸ½).*$', r'\1', addr_raw)
    
    # ì¼ë°˜ì ì¸ ì‹œì„¤ëª… ì œê±° (ê¸¸/ë¡œ íŒ¨í„´ì´ ì—†ëŠ” ê²½ìš°)
    addr_raw = re.sub(r'\s+[ê°€-í£]*(ì–´ë¦°ì´ì§‘|ìœ ì¹˜ì›|í•™êµ|ë³‘ì›|ì•½êµ­|ë§ˆíŠ¸|í¸ì˜ì |ê³µì›|ì²´ìœ¡ê´€|ë¬¸í™”ì„¼í„°|ì£¼ë¯¼ì„¼í„°|ì„¼í„°|ê´€ë¦¬ë™|ê´€ë¦¬ì‹¤|í‚¤ì¦ˆì„¼í„°|í‚¤ì¦ˆí´ëŸ½).*$', '', addr_raw)
    
    # 7. ê³µë°± ì •ë¦¬
    addr_raw = re.sub(r'\s+', ' ', addr_raw).strip()

    # 8. ì¶”ê°€ ì£¼ì†Œì •ê·œí™” ê·œì¹™ (ì‚° í‘œê¸°, ë™/í˜¸ìˆ˜, ë‹¨ì§€ëª… ì œê±° ë“±)
    # 8-1. 'ì‚°' ë’¤ ìˆ«ì ê³µë°± ì‚½ì… (ì˜ˆ: ì‚°19 -> ì‚° 19)
    addr_raw = re.sub(r"(ì‚°)(\d+)", r"\1 \2", addr_raw)

    # 8-2. ì„¸ë¶€ ë™Â·í˜¸ìˆ˜ ì œê±° (ì˜ˆ: 106-101, 101-102)
    # ì£¼ì˜: ê±´ë¬¼ë²ˆí˜¸ëŠ” ì œê±°í•˜ë©´ ì•ˆë¨ (ì˜ˆ: 22-5ëŠ” ê±´ë¬¼ë²ˆí˜¸)
    # í° ë²ˆí˜¸ì˜ ì„¸ë¶€ ë™Â·í˜¸ìˆ˜ë§Œ ì œê±° (ì˜ˆ: 106-101, 101-102)
    addr_raw = re.sub(r"\d{3,}-\d+$", "", addr_raw)

    # 8-3. ì¸µ/í˜¸/ìƒì„¸ ìœ„ì¹˜ ì œê±°
    addr_raw = re.sub(r"\d+ì¸µ", "", addr_raw)      # 3ì¸µ, 2ì¸µ
    addr_raw = re.sub(r"\d+í˜¸", "", addr_raw)      # 101í˜¸, 201í˜¸
    addr_raw = re.sub(r"B\d+", "", addr_raw)       # B113í˜¸ ê°™ì€ íŒ¨í„´
    addr_raw = re.sub(r"(ìƒê°€|ë³¸ê´€|ê´€ë¦¬ë™).*", "", addr_raw)  # ê´€ë¦¬ë™, ìƒê°€ 2ì¸µ ë“±

    # 8-4. ë‹¨ì§€ëª…/ì•„íŒŒíŠ¸ëª… ì œê±° (ê³µí†µ ì•„íŒŒíŠ¸ ë¸Œëœë“œëª…)
    apt_keywords = [
        "ìì´", "í‘¸ë¥´ì§€ì˜¤", "íìŠ¤í…Œì´íŠ¸", "ë˜ë¯¸ì•ˆ",
        "ì•„ì´íŒŒí¬", "ë¦¬ì²¸ì‹œì•„", "ë¡¯ë°ìºìŠ¬", "eí¸í•œì„¸ìƒ",
        "ì„¼íŠ¸ë ˆë¹Œ", "ìœ„ë¸Œ", "ë”ìƒµ", "SKë·°", "ì— ë²¨ë¦¬"
    ]
    for kw in apt_keywords:
        addr_raw = re.sub(rf"\s*{kw}\S*", "", addr_raw)

    # 8-5. ê´„í˜¸/ì‰¼í‘œ ì•ˆì˜ ë¶€ê°€ì„¤ëª… ì œê±°
    addr_raw = re.sub(r"\(.*?\)", "", addr_raw)
    addr_raw = re.sub(r",.*", "", addr_raw)

    # 8-6. ê±´ë¬¼ëª…/ì‹œì„¤ëª… ì œê±° (E3, Aë™, Bë™ ë“±)
    addr_raw = re.sub(r"\s+[A-Z]\d*$", "", addr_raw)  # E3, A1, B2 ë“±
    addr_raw = re.sub(r"\s+[ê°€-í£]*ë™$", "", addr_raw)  # Aë™, Bë™ ë“±
    
    # 8-7. ë¶ˆí•„ìš”í•œ ê³µë°± ì •ë¦¬
    addr_raw = re.sub(r"\s+", " ", addr_raw).strip()

    # âœ… ìµœì¢… ì •ë¦¬ (ìƒì„¸ ë²ˆì§€ ì •ë³´ ë³´ì¡´)
    # -------------------
    # 1. ê´„í˜¸ ì œê±° (ê±´ë¬¼ëª…ì´ í¬í•¨ëœ ê´„í˜¸ë§Œ)
    addr = re.sub(r'\([^)]*(?:ë¹Œ|íƒ€ì›Œ|ì„¼í„°|ë§ˆíŠ¸|ëª°|ì•„íŒŒíŠ¸|ì˜¤í”¼ìŠ¤|ìì´|íŒŒí¬|íìŠ¤|ìœ„ë¸Œ|í‘¸ë¥´ì§€ì˜¤)[^)]*\)', '', addr_raw)

    # 2. ì‰¼í‘œ ë’¤ ìƒì„¸ì •ë³´ ì œê±° (ì¸µìˆ˜, í˜¸ìˆ˜ ì •ë³´ë§Œ)
    addr = re.sub(r',\s*(?:ì¸µ|í˜¸|ë™|ì§€í•˜|ì§€ìƒ).*$', '', addr)

    # 3. "ì¸µ", "í˜¸", "ë™" ê°™ì€ ì„¸ë¶€ ìœ„ì¹˜ ì œê±° (ìƒì„¸ ë²ˆì§€ëŠ” ë³´ì¡´)
    # ì£¼ì˜: ê±´ë¬¼ë²ˆí˜¸ëŠ” ì œê±°í•˜ì§€ ì•ŠìŒ (ì˜ˆ: 112-3, 581, 21-31 ë“±)
    addr = re.sub(r'\s*\d+ì¸µ.*$', '', addr)
    addr = re.sub(r'\s*\d+í˜¸.*$', '', addr)
    addr = re.sub(r'\s*\d+ë™.*$', '', addr)
    addr = re.sub(r'\s*ì§€í•˜\d*ì¸µ?', '', addr)
    addr = re.sub(r'\s*ì§€ìƒ\d*ì¸µ?', '', addr)

    # 4. ê³µë°± ì •ë¦¬
    addr = re.sub(r'\s+', ' ', addr).strip()

    # 5. ìƒˆë¡œìš´ ì „ì²˜ë¦¬ ê·œì¹™ë“¤ ì¶”ê°€
    
    # 5-1. '*ë¡œnê°€' íŒ¨í„´ì—ì„œ ë„ì–´ì“°ê¸° ì œê±° (ë™ì´ë¦„ì´ë¯€ë¡œ ë„ì–´ì“°ë©´ ì•ˆë¨)
    # ì˜ˆ: "ì„ì§€ë¡œ 6ê°€" -> "ì„ì§€ë¡œ6ê°€"
    addr = re.sub(r'([ê°€-í£]+ë¡œ)\s+(\d+ê°€)', r'\1\2', addr)
    
    # 5-2. ì§€ë²ˆì£¼ì†Œì—ì„œ ì§€ë²ˆ-ë¶€ì§€ë²ˆ ë’¤ì˜ ëª¨ë“  í•œê¸€ ì œê±°
    # ì˜ˆ: "ì„±ìˆ˜ë™1ê°€ 685-20 ì„œìš¸ìˆ² ê´€ë¦¬ì‚¬ë¬´ì†Œ" -> "ì„±ìˆ˜ë™1ê°€ 685-20"
    # ì˜ˆ: "ê°€ì–‘ë™ 56-2ë²ˆì§€ ê°•ì„œì˜¤í† í”Œë™ìŠ¤ ìë™ì°¨ë§¤ë§¤ì„¼í„° 102í˜¸" -> "ê°€ì–‘ë™ 56-2"
    # ì˜ˆ: "í•˜ì™•ì‹­ë¦¬ë™ 998ë²ˆì§€ ì™•ì‹­ë¦¬KCCìŠ¤ìœ„ì²¸" -> "í•˜ì™•ì‹­ë¦¬ë™ 998"
    # ì˜ˆ: "ë¶ì•„í˜„ë™ 136-21ë²ˆì§€ ì´í¸í•œì„¸ìƒì‹ ì´Œ 119í˜¸" -> "ë¶ì•„í˜„ë™ 136-21"
    # ì˜ˆ: "ì‹ ì‚¬ë™ 162-16ë²ˆì§€ .17" -> "ì‹ ì‚¬ë™ 162-16"
    
    # ì§€ë²ˆ-ë¶€ì§€ë²ˆ íŒ¨í„´ ë’¤ì˜ ëª¨ë“  í•œê¸€ ì œê±° (ë²ˆì§€ í¬í•¨)
    # ì£¼ì˜: ë¶€ì§€ë²ˆì´ ìˆëŠ” ê²½ìš°ëŠ” ì œì™¸ (ì˜ˆ: ì„±ìˆ˜ë™1ê°€ 685-20)
    # ì´ íŒ¨í„´ì€ ìƒˆë¡œìš´ ê·œì¹™ì—ì„œ ì²˜ë¦¬í•˜ë¯€ë¡œ ì£¼ì„ ì²˜ë¦¬
    # addr = re.sub(r'([ê°€-í£]+ë™\s+\d+[-\d]*)\s*ë²ˆì§€.*$', r'\1', addr)
    
    # ì§€ë²ˆ-ë¶€ì§€ë²ˆ íŒ¨í„´ ë’¤ì˜ ëª¨ë“  í•œê¸€ ì œê±° (ë²ˆì§€ ì—†ì´, 2ê¸€ì ì´ìƒì˜ í•œê¸€ë§Œ)
    # ì£¼ì˜: ë¶€ì§€ë²ˆì´ ìˆëŠ” ê²½ìš°ëŠ” ì œì™¸ (ì˜ˆ: ì„±ìˆ˜ë™1ê°€ 685-20)
    # ì´ íŒ¨í„´ì€ ìƒˆë¡œìš´ ê·œì¹™ì—ì„œ ì²˜ë¦¬í•˜ë¯€ë¡œ ì£¼ì„ ì²˜ë¦¬
    # addr = re.sub(r'([ê°€-í£]+ë™\s+\d+[-\d]*)\s+[ê°€-í£]{2,}.*$', r'\1', addr)
    
    # 5-3. ë™ì´ë¦„ ë’¤ì˜ ìˆ«ì(ì§€ë²ˆ)ëŠ” ë³´ì¡´í•˜ê³  ê·¸ ë’¤ì˜ í•œê¸€ë§Œ ì œê±°
    # ì˜ˆ: "í•œê°•ë¡œ1ê°€ 64 ëŒ€ìš° ì›”ë“œë§ˆí¬ ìš©ì‚°" -> "í•œê°•ë¡œ1ê°€ 64"
    # ì˜ˆ: "í•œê°•ë¡œ3ê°€ 40-999 ìš©ì‚°ì—­" -> "í•œê°•ë¡œ3ê°€ 40-999"
    # í•œê¸€+ë¡œ+ìˆ«ì+ê°€ íŒ¨í„´ì—ì„œ ì§€ë²ˆ ë’¤ì˜ í•œê¸€ ì œê±°
    # ì£¼ì˜: ë¶€ì§€ë²ˆì´ ìˆëŠ” ê²½ìš°ëŠ” ì œì™¸ (ì˜ˆ: í•œê°•ë¡œ2ê°€ 112-3)
    # ì´ íŒ¨í„´ì€ ìƒˆë¡œìš´ ê·œì¹™ì—ì„œ ì²˜ë¦¬í•˜ë¯€ë¡œ ì£¼ì„ ì²˜ë¦¬
    # addr = re.sub(r'([ê°€-í£]+ë¡œ\d+ê°€\s+\d+[-\d]*)\s+[ê°€-í£]{2,}.*$', r'\1', addr)
    
    # 5-4. ë§ˆì¹¨í‘œì™€ ì  ì œê±°
    # ì˜ˆ: "ì‹ ì‚¬ë™ 162-16ë²ˆì§€ .17" -> "ì‹ ì‚¬ë™ 162-16"
    addr = re.sub(r'\.\d+.*$', '', addr)
    addr = re.sub(r'\s*\.\s*.*$', '', addr)
    
    # 5-5. ìµœì¢… ê³µë°± ì •ë¦¬
    addr = re.sub(r'\s+', ' ', addr).strip()

    return addr

class InfraNormalizer:
    """ì¸í”„ë¼ ë°ì´í„° ì •ê·œí™” í´ë˜ìŠ¤"""
    
    def __init__(self, data_dir: Path):
        self.data_dir = data_dir
        self.normalized_facilities: List[Dict] = []
        self.normalized_subway_stations: List[Dict] = []
        self.normalized_bus_stops: List[Dict] = []  # ë²„ìŠ¤ì •ë¥˜ì†Œ ë°ì´í„°
        self.failed_addresses: List[Dict] = []  # ì‹¤íŒ¨í•œ ì£¼ì†Œ ì •ê·œí™” ë°ì´í„°
        self.failed_output_dir: Optional[Path] = None  # ì‹¤íŒ¨ ë°ì´í„° ì €ì¥ ê²½ë¡œ
        self.output_dir: Optional[Path] = None  # ì¶œë ¥ ë””ë ‰í† ë¦¬
        self.realtime_mode: bool = False  # ì‹¤ì‹œê°„ ì €ì¥ ëª¨ë“œ
        
        # API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        coordinate_api_key = os.getenv("TOLOLA_API_KEY")
        
        if not coordinate_api_key:
            raise ValueError("TOLOLA_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            
        self.coordinate_api = CoordinateAPI(coordinate_api_key)

        
        # facility_id ìƒì„±ìš© ì¹´í…Œê³ ë¦¬ë³„ ì ‘ë‘ì‚¬ ë§¤í•‘
        self.facility_id_prefix_map = {
            'childSchool': 'chsch',    # ìœ ì¹˜ì›
            'childCare': 'child',     # ì–´ë¦°ì´ì§‘
            'school': 'sch',            # ì´ˆì¤‘ê³ 
            'college': 'col',           # ëŒ€í•™
            'pharmacy': 'pha',          # ì•½êµ­
            'hospital': 'hos',          # ë³‘ì›
            'mart': 'mt',               # ë§ˆíŠ¸
            'convenience': 'con',       # í¸ì˜ì 
            'gym': 'gym',               # ê³µê³µì²´ìœ¡ì‹œì„¤
            'park': 'park',             # ê³µì›
            'subway': 'sub',            # ì§€í•˜ì² ì—­
            'busstop': 'bus',           # ë²„ìŠ¤ì •ë¥˜ì†Œ
            'bus': 'bus'                # ë²„ìŠ¤ì •ë¥˜ì†Œ (ìƒˆë¡œìš´ í˜•ì‹)
        }
        
        # ì¹´í…Œê³ ë¦¬ë³„ ì¹´ìš´í„° (facility_id ìƒì„±ìš©)
        self.facility_counters = {prefix: 0 for prefix in self.facility_id_prefix_map.values()}
        
        # JUSO API ëŒ€ì‹  ì¢Œí‘œ API ì‚¬ìš©
        
        # í•„ìš”í•œ ì¹´í…Œê³ ë¦¬ í™•ì¸ ë° ì¶”ê°€
        self._ensure_categories_exist()

    def _ensure_categories_exist(self):
        """í•„ìš”í•œ ì¹´í…Œê³ ë¦¬ê°€ DBì— ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•˜ê³  ì—†ìœ¼ë©´ ì¶”ê°€"""
        try:
            from backend.db.db_utils_pg import get_engine
            from sqlalchemy import text
            
            engine = get_engine()
            with engine.connect() as conn:
                # ID 11 (childSchool - ìœ ì¹˜ì›) í™•ì¸
                result = conn.execute(text("""
                    SELECT id FROM infra.facility_categories 
                    WHERE id = 11 AND code = 'childSchool'
                """)).fetchone()
                
                if not result:
                    # ID 11 ìœ ì¹˜ì› ì¹´í…Œê³ ë¦¬ ì¶”ê°€
                    conn.execute(text("""
                        INSERT INTO infra.facility_categories (id, code, name, description, created_at) 
                        VALUES (11, 'childSchool', 'ìœ ì¹˜ì›', 'êµìœ¡ì‹œì„¤', NOW())
                    """))
                    conn.commit()
                    logger.info("âœ… ìœ ì¹˜ì› ì¹´í…Œê³ ë¦¬(ID: 11) ì¶”ê°€ ì™„ë£Œ")
                else:
                    logger.info("âœ… ìœ ì¹˜ì› ì¹´í…Œê³ ë¦¬(ID: 11) ì´ë¯¸ ì¡´ì¬")
                    
        except Exception as e:
            logger.warning(f"âš ï¸ ì¹´í…Œê³ ë¦¬ í™•ì¸/ì¶”ê°€ ì¤‘ ì˜¤ë¥˜: {e}")

    def _get_category_id(self, code: str) -> Optional[int]:
        """ì‹œì„¤ ì¹´í…Œê³ ë¦¬ ì½”ë“œë¥¼ ê¸°ë°˜ìœ¼ë¡œ IDë¥¼ ì¡°íšŒ"""
        return self.category_map.get(code)
    
    def _generate_facility_id(self, facility_type: str) -> str:
        """ì‹œì„¤ íƒ€ì…ì— ë”°ë¼ ê³ ìœ í•œ facility_id ìƒì„±"""
        prefix = self.facility_id_prefix_map.get(facility_type, 'unk')
        
        # ì¹´ìš´í„° ì¦ê°€
        self.facility_counters[prefix] += 1
        
        # 4ìë¦¬ ìˆ«ìë¡œ í¬ë§·íŒ… (ì˜ˆ: child0001, sch0001)
        return f"{prefix}{self.facility_counters[prefix]:04d}"
    
    def _get_facility_cd(self, facility_type: str) -> str:
        """ì‹œì„¤ íƒ€ì…ì— ë”°ë¼ cd (ì ‘ë‘ì‚¬) ë°˜í™˜"""
        return self.facility_id_prefix_map.get(facility_type, 'unk')

    def _generate_transport_id(self, transport_type: str) -> str:
        """êµí†µ ì‹œì„¤ ID ìƒì„± (sub000001, bus000001 í˜•íƒœ)"""
        if transport_type == 'subway':
            prefix = 'sub'
            digits = 6  # ì§€í•˜ì² ì€ 6ìë¦¬ (ìµœëŒ€ 999,999ê°œ)
        elif transport_type == 'bus':
            prefix = 'bus'
            digits = 6  # ë²„ìŠ¤ëŠ” 6ìë¦¬ (ìµœëŒ€ 999,999ê°œ)
        else:
            prefix = 'trp'
            digits = 6  # ê¸°íƒ€ êµí†µì‹œì„¤ë„ 6ìë¦¬
        
        # ì¹´ìš´í„° ì¦ê°€
        self.facility_counters[prefix] += 1
        
        # 6ìë¦¬ ìˆ«ìë¡œ í¬ë§·íŒ… (ì˜ˆ: sub000001, bus000001)
        return f"{prefix}{self.facility_counters[prefix]:0{digits}d}"

    def _safe_int(self, value) -> Optional[int]:
        """ì•ˆì „í•˜ê²Œ ì •ìˆ˜ë¡œ ë³€í™˜"""
        if pd.isna(value) or value == '' or value is None:
            return None
        try:
            return int(float(str(value)))
        except (ValueError, TypeError):
            return None

    def _safe_float(self, value) -> Optional[float]:
        """ì•ˆì „í•˜ê²Œ ì‹¤ìˆ˜ë¡œ ë³€í™˜"""
        if pd.isna(value) or value == '' or value is None:
            return None
        try:
            return float(str(value))
        except (ValueError, TypeError):
            return None


    def _save_failed_data_immediately(self, failed_data: Dict, output_dir: Path):
        """ì‹¤íŒ¨ ë°ì´í„°ë¥¼ ì¦‰ì‹œ íŒŒì¼ì— ì¶”ê°€ ì €ì¥ (ì¤‘ë³µ ë°©ì§€)"""
        failed_file = output_dir / "failed_addresses.jsonl"
        
        # ì¤‘ë³µ ì²´í¬ë¥¼ ìœ„í•œ í‚¤ ìƒì„± (facility_name + address_raw + facility_type)
        check_key = f"{failed_data.get('facility_name', '')}_{failed_data.get('address_raw', '')}_{failed_data.get('facility_type', '')}"
        
        # ì´ë¯¸ ì €ì¥ëœ ì‹¤íŒ¨ ë°ì´í„°ì¸ì§€ í™•ì¸
        if hasattr(self, '_saved_failed_keys'):
            if check_key in self._saved_failed_keys:
                return  # ì´ë¯¸ ì €ì¥ëœ ë°ì´í„°ì´ë¯€ë¡œ ê±´ë„ˆë›°ê¸°
        else:
            self._saved_failed_keys = set()
        
        # ì‹¤íŒ¨ ë°ì´í„° ì €ì¥ (ì¤‘ë³µ ì œê±°ë¥¼ ìœ„í•´ addresses_tried í•„ë“œ ì •ë¦¬)
        clean_failed_data = failed_data.copy()
        if 'addresses_tried' in clean_failed_data:
            # ì¤‘ë³µ ì œê±°
            clean_failed_data['addresses_tried'] = list(set(clean_failed_data['addresses_tried']))
        
        with open(failed_file, 'a', encoding='utf-8') as f:
            f.write(json.dumps(clean_failed_data, ensure_ascii=False) + '\n')
        
        # ì €ì¥ëœ í‚¤ ê¸°ë¡
        self._saved_failed_keys.add(check_key)

    def _save_success_data_immediately(self, facility_data: Dict, output_dir: Path, facility_type: str):
        """ì„±ê³µ ë°ì´í„°ë¥¼ ì¦‰ì‹œ íŒŒì¼ì— ì¶”ê°€ ì €ì¥ (ë©”ëª¨ë¦¬ ì ˆì•½)"""
        # ì‹œì„¤ íƒ€ì…ë³„ íŒŒì¼ ë¶„ë¦¬
        if facility_type in ['subway', 'busstop', 'bus']:  # êµí†µ ì‹œì„¤ í†µí•©
            success_file = output_dir / "transport_points.jsonl"
        else:
            # ë°ì´í„°ì…‹ë³„ ê°œë³„ íŒŒì¼ ìƒì„± (public_facilities_{prefix}.jsonl)
            prefix = self.facility_id_prefix_map.get(facility_type, 'unk')
            success_file = output_dir / f"public_facilities_{prefix}.jsonl"
        
        with open(success_file, 'a', encoding='utf-8') as f:
            f.write(json.dumps(facility_data, ensure_ascii=False) + '\n')

    def _save_progress_immediately(self, progress_data: Dict, output_dir: Path):
        """ì§„í–‰ ìƒí™©ì„ ì¦‰ì‹œ íŒŒì¼ì— ì €ì¥"""
        progress_file = output_dir / "progress.jsonl"
        
        with open(progress_file, 'a', encoding='utf-8') as f:
            f.write(json.dumps(progress_data, ensure_ascii=False) + '\n')

    def _initialize_realtime_files(self, output_dir: Path):
        """ì‹¤ì‹œê°„ ëª¨ë“œìš© JSONL íŒŒì¼ë“¤ ì´ˆê¸°í™” (ê¸°ì¡´ íŒŒì¼ ìœ ì§€)"""
        files_to_init = [
            "transport_points.jsonl",  # êµí†µ ë°ì´í„° í†µí•© íŒŒì¼
            "failed_addresses.jsonl",
            "progress.jsonl"
        ]
        
        # ê¸°ì¡´ íŒŒì¼ë“¤ ì´ˆê¸°í™”
        for filename in files_to_init:
            file_path = output_dir / filename
            # ê¸°ì¡´ íŒŒì¼ì´ ì—†ìœ¼ë©´ë§Œ ìƒì„± (ê¸°ì¡´ íŒŒì¼ ìœ ì§€)
            if not file_path.exists():
                file_path.touch()  # ë¹ˆ íŒŒì¼ ìƒì„±
        
        # ë°ì´í„°ì…‹ë³„ ê°œë³„ íŒŒì¼ë“¤ ì´ˆê¸°í™”
        for facility_type, prefix in self.facility_id_prefix_map.items():
            if facility_type not in ['subway', 'busstop', 'bus']:  # ì§€í•˜ì² ê³¼ ë²„ìŠ¤ì •ë¥˜ì†ŒëŠ” ì œì™¸
                filename = f"public_facilities_{prefix}.jsonl"
                file_path = output_dir / filename
                if not file_path.exists():
                    file_path.touch()  # ë¹ˆ íŒŒì¼ ìƒì„±

    def get_last_progress(self, output_dir: Path) -> Optional[Dict]:
        """ë§ˆì§€ë§‰ ì§„í–‰ ìƒí™© ì¡°íšŒ"""
        progress_file = output_dir / "progress.jsonl"
        
        if not progress_file.exists():
            return None
        
        last_progress = None
        with open(progress_file, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    last_progress = json.loads(line.strip())
        
        return last_progress

    def get_dataset_last_progress(self, output_dir: Path, facility_type: str) -> Optional[Dict]:
        """íŠ¹ì • ë°ì´í„°ì…‹ì˜ ë§ˆì§€ë§‰ ì§„í–‰ ìƒí™© ì¡°íšŒ"""
        progress_file = output_dir / "progress.jsonl"
        
        if not progress_file.exists():
            return None
        
        last_progress = None
        with open(progress_file, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    progress_data = json.loads(line.strip())
                    if progress_data.get('facility_type') == facility_type:
                        last_progress = progress_data
        
        return last_progress

    def get_resume_point(self, output_dir: Path, facility_type: str) -> int:
        """ì¤‘ë‹¨ëœ ì§€ì ë¶€í„° ì¬ê°œí•  ë¼ì¸ ë²ˆí˜¸ ë°˜í™˜ (ìë™ ê°ì§€)"""
        progress_file = output_dir / "progress.jsonl"
        
        if not progress_file.exists():
            return 0  # ì²˜ìŒë¶€í„° ì‹œì‘
        
        # í•´ë‹¹ facility_typeì˜ ëª¨ë“  progress í•­ëª©ì„ ìˆ˜ì§‘
        progress_entries = []
        with open(progress_file, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    progress_data = json.loads(line.strip())
                    if progress_data.get('facility_type') == facility_type:
                        progress_entries.append(progress_data)
        
        if not progress_entries:
            return 0  # ì²˜ìŒë¶€í„° ì‹œì‘
        
        # ê°€ì¥ í° row_indexë¥¼ ì°¾ì•„ì„œ +1 ë°˜í™˜
        max_row_index = max(entry.get('row_index', 0) for entry in progress_entries)
        resume_point = max_row_index + 1
        
        logger.info(f"ğŸ“Š {facility_type} ì¬ê°œ ì§€ì  ìë™ ê°ì§€: {resume_point}í–‰ (ë§ˆì§€ë§‰ ì²˜ë¦¬: {max_row_index}í–‰)")
        
        return resume_point

    def find_latest_csv_file(self, directory: Path, pattern: str) -> Optional[Path]:
        """ë””ë ‰í† ë¦¬ì—ì„œ íŒ¨í„´ì— ë§ëŠ” ê°€ì¥ ìµœì‹  CSV íŒŒì¼ ì°¾ê¸°"""
        if not directory.exists():
            return None
        
        # íŒ¨í„´ì— ë§ëŠ” ëª¨ë“  CSV íŒŒì¼ ì°¾ê¸°
        matching_files = []
        for file_path in directory.glob("*.csv"):
            if pattern in file_path.name:
                matching_files.append(file_path)
        
        if not matching_files:
            return None
        
        # íŒŒì¼ëª…ì—ì„œ ë‚ ì§œ ì¶”ì¶œí•˜ì—¬ ì •ë ¬
        def extract_date(file_path: Path) -> str:
            # íŒŒì¼ëª…ì—ì„œ ë‚ ì§œ íŒ¨í„´ ì¶”ì¶œ (ì˜ˆ: 20250928)
            import re
            date_match = re.search(r'(\d{8})', file_path.name)
            return date_match.group(1) if date_match else "00000000"
        
        # ë‚ ì§œ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ê°€ì¥ ìµœì‹  íŒŒì¼ ë°˜í™˜
        latest_file = max(matching_files, key=extract_date)
        logger.info(f"ğŸ“ ìµœì‹  íŒŒì¼ ìë™ ê°ì§€: {latest_file.name}")
        
        return latest_file

    def resume_from_progress(self, output_dir: Path) -> bool:
        """ì§„í–‰ ìƒí™©ì—ì„œ ì¬ì‹œì‘"""
        last_progress = self.get_last_progress(output_dir)
        
        if not last_progress:
            logger.info("ì¬ì‹œì‘í•  ì§„í–‰ ìƒí™©ì´ ì—†ìŠµë‹ˆë‹¤. ì²˜ìŒë¶€í„° ì‹œì‘í•©ë‹ˆë‹¤.")
            return False
        
        logger.info(f"ë§ˆì§€ë§‰ ì§„í–‰ ìƒí™© ë°œê²¬:")
        logger.info(f"  - íŒŒì¼: {last_progress['file']}")
        logger.info(f"  - í–‰ ë²ˆí˜¸: {last_progress['row_index']}")
        logger.info(f"  - ì‹œì„¤ íƒ€ì…: {last_progress['facility_type']}")
        logger.info(f"  - ì²˜ë¦¬ëœ ê°œìˆ˜: {last_progress['processed_count']}")
        
        return True

    def _load_existing_facility_ids(self, output_dir: Path) -> Dict[str, int]:
        """ê° ë°ì´í„°ì…‹ë³„ JSONL íŒŒì¼ì—ì„œ ë§ˆì§€ë§‰ facility_idë¥¼ í™•ì¸í•˜ì—¬ ì¹´ìš´í„° ì´ˆê¸°í™”"""
        counters = {prefix: 0 for prefix in self.facility_id_prefix_map.values()}
        
        # ê° ë°ì´í„°ì…‹ë³„ íŒŒì¼ í™•ì¸
        for facility_type, prefix in self.facility_id_prefix_map.items():
            if facility_type in ['subway', 'busstop', 'bus']:
                # ì§€í•˜ì² ê³¼ ë²„ìŠ¤ì •ë¥˜ì†ŒëŠ” transport_points.jsonlì—ì„œ í™•ì¸
                file_path = output_dir / "transport_points.jsonl"
            else:
                # ë‚˜ë¨¸ì§€ëŠ” public_facilities_{prefix}.jsonl í˜•ì‹
                file_path = output_dir / f"public_facilities_{prefix}.jsonl"
            
            if file_path.exists():
                try:
                    max_id = 0
                    with open(file_path, 'r', encoding='utf-8') as f:
                        for line in f:
                            if line.strip():
                                data = json.loads(line.strip())
                                facility_id = data.get('facility_id', '')
                                if facility_id and facility_id.startswith(prefix):
                                    # ìˆ«ì ë¶€ë¶„ ì¶”ì¶œ
                                    number_part = facility_id[len(prefix):]
                                    try:
                                        number = int(number_part)
                                        if number > max_id:
                                            max_id = number
                                    except ValueError:
                                        continue
                    
                    counters[prefix] = max_id
                    if max_id > 0:
                        logger.info(f"ğŸ“Š {prefix} ì¹´ìš´í„° ì´ˆê¸°í™”: {max_id} (íŒŒì¼: {file_path.name})")
                    
                except Exception as e:
                    logger.warning(f"âš ï¸ {file_path.name} ë¡œë“œ ì‹¤íŒ¨: {e}")
            else:
                logger.info(f"ğŸ“ {file_path.name} íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. {prefix} ì¹´ìš´í„°ë¥¼ 0ë¶€í„° ì‹œì‘í•©ë‹ˆë‹¤.")
        
        loaded_count = sum(1 for count in counters.values() if count > 0)
        logger.info(f"âœ… ê¸°ì¡´ facility_id ë¡œë“œ ì™„ë£Œ: {loaded_count}ê°œ ì ‘ë‘ì‚¬")
        
        return counters

    def convert_jsonl_to_json(self, output_dir: Path):
        """JSONL íŒŒì¼ë“¤ì„ JSON íŒŒì¼ë¡œ ë³€í™˜ (ìµœì¢… ê²°ê³¼ ìƒì„±)"""
        logger.info("JSONL íŒŒì¼ë“¤ì„ JSON íŒŒì¼ë¡œ ë³€í™˜ ì¤‘...")
        
        # 1. ëª¨ë“  public_facilities_{prefix}.jsonl íŒŒì¼ë“¤ì„ í†µí•©í•˜ì—¬ public_facilities.json ìƒì„±
        all_public_facilities = []
        
        # ë°ì´í„°ì…‹ë³„ ê°œë³„ íŒŒì¼ë“¤ í†µí•©
        for facility_type, prefix in self.facility_id_prefix_map.items():
            if facility_type not in ['subway', 'busstop']:  # ì§€í•˜ì² ê³¼ ë²„ìŠ¤ì •ë¥˜ì†ŒëŠ” ì œì™¸
                file_path = output_dir / f"public_facilities_{prefix}.jsonl"
                if file_path.exists():
                    try:
                        with open(file_path, 'r', encoding='utf-8') as f:
                            for line in f:
                                if line.strip():
                                    data = json.loads(line.strip())
                                    all_public_facilities.append(data)
                        logger.info(f"ğŸ“ {file_path.name}: {len([l for l in open(file_path, 'r', encoding='utf-8') if l.strip()])}ê°œ ë°ì´í„° ë¡œë“œ")
                    except Exception as e:
                        logger.warning(f"âš ï¸ {file_path.name} ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        # í†µí•©ëœ public_facilities.json ìƒì„± (ë¹„í™œì„±í™”)
        if all_public_facilities:
            logger.info(f"public_facilities.json ìƒì„± ê±´ë„ˆë›°ê¸°: {len(all_public_facilities)}ê°œ ë°ì´í„° (JSONL íŒŒì¼ë§Œ ì‚¬ìš©)")
        else:
            logger.info("í†µí•©í•  public_facilities ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        # 2. subway_stations.jsonl â†’ subway_stations.json (ë°ì´í„°ê°€ ìˆì„ ë•Œë§Œ)
        subway_file = output_dir / "subway_stations.jsonl"
        if subway_file.exists():
            stations = []
            with open(subway_file, 'r', encoding='utf-8') as f:
                for line in f:
                    if line.strip():
                        stations.append(json.loads(line.strip()))
            
            # ë°ì´í„°ê°€ ìˆì„ ë•Œë§Œ JSON íŒŒì¼ ìƒì„±
            if stations:
                stations_data = {
                    "subway_stations": stations,
                    "metadata": {
                        "normalized_at": datetime.now().isoformat(),
                        "subway_stations_count": len(stations)
                    }
                }
                
                output_file = output_dir / "subway_stations.json"
                with open(output_file, 'w', encoding='utf-8') as f:
                    json.dump(stations_data, f, ensure_ascii=False, indent=2)
                logger.info(f"subway_stations.json ìƒì„±: {len(stations)}ê°œ")
            else:
                logger.info("subway_stations.jsonlì´ ë¹„ì–´ìˆì–´ì„œ JSON íŒŒì¼ì„ ìƒì„±í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        
        # 3. bus_stops.jsonl â†’ bus_stops.json (ë°ì´í„°ê°€ ìˆì„ ë•Œë§Œ)
        bus_stops_file = output_dir / "bus_stops.jsonl"
        if bus_stops_file.exists():
            bus_stops = []
            with open(bus_stops_file, 'r', encoding='utf-8') as f:
                for line in f:
                    if line.strip():
                        bus_stops.append(json.loads(line.strip()))
            
            # ë°ì´í„°ê°€ ìˆì„ ë•Œë§Œ JSON íŒŒì¼ ìƒì„±
            if bus_stops:
                bus_stops_data = {
                    "bus_stops": bus_stops,
                    "metadata": {
                        "normalized_at": datetime.now().isoformat(),
                        "bus_stops_count": len(bus_stops)
                    }
                }
                
                output_file = output_dir / "bus_stops.json"
                with open(output_file, 'w', encoding='utf-8') as f:
                    json.dump(bus_stops_data, f, ensure_ascii=False, indent=2)
                logger.info(f"bus_stops.json ìƒì„±: {len(bus_stops)}ê°œ")
            else:
                logger.info("bus_stops.jsonlì´ ë¹„ì–´ìˆì–´ì„œ JSON íŒŒì¼ì„ ìƒì„±í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        
        # 4. failed_addresses.jsonl â†’ failed_addresses.json (ë¹„í™œì„±í™”)
        # JSONL íŒŒì¼ë§Œ ì‚¬ìš©í•˜ë¯€ë¡œ JSON íŒŒì¼ ìƒì„±í•˜ì§€ ì•ŠìŒ
        # failed_file = output_dir / "failed_addresses.jsonl"
        # if failed_file.exists():
        #     failed_addresses = []
        #     with open(failed_file, 'r', encoding='utf-8') as f:
        #         for line in f:
        #             if line.strip():
        #                 failed_addresses.append(json.loads(line.strip()))
        #     
        #     failed_data = {
        #         "failed_addresses": failed_addresses,
        #         "metadata": {
        #             "normalized_at": datetime.now().isoformat(),
        #             "failed_addresses_count": len(failed_addresses)
        #         }
        #     }
        #     
        #     output_file = output_dir / "failed_addresses.json"
        #     with open(output_file, 'w', encoding='utf-8') as f:
        #         json.dump(failed_data, f, ensure_ascii=False, indent=2)
        #     logger.info(f"failed_addresses.json ìƒì„±: {len(failed_addresses)}ê°œ")
        
        logger.info("JSONL â†’ JSON ë³€í™˜ ì™„ë£Œ!")

    def _normalize_address(self, address_raw: str, facility_name: str = "", facility_type: str = "", original_file: str = "", original_row_index: int = -1) -> Dict[str, Any]:
        """ì£¼ì†Œ ì •ê·œí™” - ì¢Œí‘œ APIë§Œ ì‚¬ìš©"""
        # ì‹¤íŒ¨ ë°ì´í„° ì €ì¥ í”Œë˜ê·¸ ì´ˆê¸°í™”
        self._already_saved_failed = False
        
        if not address_raw or address_raw.strip() == '':
            return {
                'address_raw': address_raw,
                'address_nm': None,
                'address_id': None,
                'lat': None,
                'lon': None,
                'normalization_success': False
            }
        
        logger.info(f"ğŸ  ì£¼ì†Œ ì •ê·œí™” ì‹œì‘")
        logger.info(f"ğŸ“ ì›ë³¸ ì£¼ì†Œ: {address_raw}")
        logger.info(f"ğŸ¢ ì‹œì„¤ëª…: {facility_name}")
        logger.info(f"ğŸ·ï¸ ì‹œì„¤íƒ€ì…: {facility_type}")
        
        # ê³µì› ì£¼ì†Œì˜ ê²½ìš° ë¯¸ë¦¬ ì²´í¬
        is_park = facility_name and 'ê³µì›' in facility_name
        
        # ì£¼ì†Œ ì „ì²˜ë¦¬ (ì‹œì„¤ íƒ€ì…ë³„ ì „ìš© ì „ì²˜ë¦¬ í•¨ìˆ˜ ì‚¬ìš©)
        if facility_type == 'subway':
            addr_processed = preprocess_subway_address(address_raw)
            logger.info(f"ğŸš‡ ì§€í•˜ì²  ì „ìš© ì „ì²˜ë¦¬ ì ìš©")
        elif is_park:
            addr_processed = preprocess_park_address(address_raw)
            logger.info(f"ğŸŒ³ ê³µì› ì „ìš© ì „ì²˜ë¦¬ ì ìš©")
        else:
            addr_processed = preprocess_address(address_raw)
            logger.info(f"ğŸ”§ ì¼ë°˜ ì „ì²˜ë¦¬ ì ìš©")
        logger.info(f"âœ¨ ì „ì²˜ë¦¬ëœ ì£¼ì†Œ: {addr_processed}")
        
        # ì¢Œí‘œ APIë¥¼ í†µí•œ ì£¼ì†Œ ì •ê·œí™” - ì—¬ëŸ¬ ë²„ì „ ì‹œë„
        addresses_to_try = []
        
        # 1. ì „ì²˜ë¦¬ëœ ì£¼ì†Œ
        if addr_processed and addr_processed.strip():
            addresses_to_try.append(addr_processed)
        
        # 2. ì›ë³¸ ì£¼ì†Œ
        if address_raw != addr_processed:
            addresses_to_try.append(address_raw)
        
        # 3. ì „ì²˜ë¦¬ëœ ì£¼ì†Œì—ì„œ ë²ˆì§€ ì •ë³´ ì œê±°í•œ ë²„ì „ (ë„ë¡œëª…ì£¼ì†Œì¸ ê²½ìš°)
        if addr_processed and detect_address_type(addr_processed) == "road":
            # ë„ë¡œëª… + ë²ˆì§€ íŒ¨í„´ì—ì„œ ë²ˆì§€ ì œê±°
            road_without_number = re.sub(r'(\d+[-\d]*)', '', addr_processed)
            road_without_number = re.sub(r'\s+', ' ', road_without_number).strip()
            if road_without_number != addr_processed and road_without_number not in addresses_to_try:
                addresses_to_try.append(road_without_number)
        
        # 4. ì›ë³¸ ì£¼ì†Œì—ì„œ ë²ˆì§€ ì •ë³´ ì œê±°í•œ ë²„ì „ (ì§€ë²ˆì£¼ì†Œì¸ ê²½ìš°)
        if detect_address_type(address_raw) == "jibun":
            jibun_without_number = re.sub(r'(\d+[-\d]*)', '', address_raw)
            jibun_without_number = re.sub(r'\s+', ' ', jibun_without_number).strip()
            if jibun_without_number != address_raw and jibun_without_number not in addresses_to_try:
                addresses_to_try.append(jibun_without_number)
        
        # ì—¬ëŸ¬ ë²„ì „ì˜ ì£¼ì†Œë¡œ ì‹œë„
        result = None
        for i, addr_to_try in enumerate(addresses_to_try):
            address_type = detect_address_type(addr_to_try)
            type_param = "ROAD" if address_type == "road" else "PARCEL"
            
            logger.info(f"ğŸ¯ ì¢Œí‘œ API ì‹œë„ {i+1}/{len(addresses_to_try)}: {addr_to_try} (íƒ€ì…: {type_param})")
            
            # ì¢Œí‘œ API í˜¸ì¶œ
            result = self.coordinate_api.normalize_address(addr_to_try, type_param)
            
            if result['normalization_success']:
                logger.info(f"âœ… ì¢Œí‘œ API ì„±ê³µ: {addr_to_try}")
                break
            else:
                logger.warning(f"âŒ ì¢Œí‘œ API ì‹¤íŒ¨: {addr_to_try}")
        
        # ë§ˆì§€ë§‰ ê²°ê³¼ê°€ Noneì¸ ê²½ìš° ë¹ˆ ê²°ê³¼ ìƒì„±
        if result is None:
            result = {
                'address_raw': address_raw,
                'address_nm': None,
                'address_id': None,
                'lat': None,
                'lon': None,
                'normalization_success': False
            }
        
        # ì„±ê³µí•œ ê²½ìš°
        if result['normalization_success']:
            logger.info(f"âœ… ì£¼ì†Œ ì •ê·œí™” ì„±ê³µ!")
            logger.info(f"ğŸ“ address_nm: {result['address_nm']}")
            logger.info(f"âœ… address_id: {result['address_id']}")
            logger.info(f"ğŸ“ ì¢Œí‘œ: lat={result['lat']}, lon={result['lon']}")
            
            # ì‹¤íŒ¨ ë°ì´í„° ì €ì¥ í”Œë˜ê·¸ ì´ˆê¸°í™”
            self._already_saved_failed = False
            
            return result
        else:
            # ëª¨ë“  API ì‹¤íŒ¨ ì‹œ ì‹¤íŒ¨ ë°ì´í„° ì €ì¥
            failed_data = {
                "facility_type": facility_type,
                "facility_name": facility_name,
                "address_raw": address_raw,
                "address_processed": addr_processed,
                "addresses_tried": addresses_to_try,
                "error_reason": "ì¢Œí‘œ ë³€í™˜ ì‹¤íŒ¨ (ì£¼ì†ŒAPI ì„±ê³µ, ì¢Œí‘œAPI ì‹¤íŒ¨)",
                "timestamp": pd.Timestamp.now().isoformat(),
                "original_file": original_file,
                "original_row_index": original_row_index
            }
            # ì¦‰ì‹œ íŒŒì¼ì— ì €ì¥ (ë©”ëª¨ë¦¬ ì ˆì•½)
            if hasattr(self, 'failed_output_dir') and self.failed_output_dir:
                self._save_failed_data_immediately(failed_data, self.failed_output_dir)
            
            logger.warning(f"âŒ ì£¼ì†Œ ì •ê·œí™” ì‹¤íŒ¨: {address_raw}")
            return {
                'address_raw': address_raw,
                'address_nm': None,
                'address_id': None,
                'lat': None,
                'lon': None,
                'normalization_success': False
            }

    # JUSO API ëŒ€ì‹  ì¢Œí‘œ APIë§Œ ì‚¬ìš©í•˜ë¯€ë¡œ íŒŒì‹± í•¨ìˆ˜ë“¤ì€ ì œê±°

    def _normalize_childcare_centers(self, file_path: Path, start_row: int = 0):
        """ì–´ë¦°ì´ì§‘ ë°ì´í„° ì •ê·œí™”"""
        if not file_path.exists():
            logger.warning(f"ì–´ë¦°ì´ì§‘ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {file_path}")
            return

        df = read_csv_with_auto_encoding(file_path)
        
        # ì‹œì‘ í–‰ ì„¤ì •
        if start_row > 0:
            logger.info(f"ì–´ë¦°ì´ì§‘ ë°ì´í„° ì •ê·œí™” ì¬ê°œ: {start_row}í–‰ë¶€í„° ì‹œì‘, ì´ {len(df)}ê°œ ì¤‘ {len(df) - start_row}ê°œ ë‚¨ìŒ")
            df = df.iloc[start_row:]
        else:
            logger.info(f"ì–´ë¦°ì´ì§‘ ë°ì´í„° ì •ê·œí™” ì‹œì‘: {len(df)}ê°œ")

        for idx, row in df.iterrows():
            address_raw = str(row.get('CRADDR', ''))
            facility_name = str(row.get('CRNAME', ''))
            address_info = self._normalize_address(address_raw, facility_name, 'childcare', str(file_path), idx)
            
            # ì£¼ì†Œ ì •ê·œí™” ì‹¤íŒ¨ ì‹œ ê±´ë„ˆë›°ê¸° (ì´ë¯¸ failed_addresses.jsonlì— ì €ì¥ë¨)
            if not address_info.get('normalization_success', False):
                continue
            
            facility_data = {
                'facility_id': self._generate_facility_id('childCare'),
                'cd': self._get_facility_cd('childCare'),
                'name': facility_name,
                'address_raw': address_info['address_raw'],
                'address_nm': address_info['address_nm'],
                'address_id': address_info['address_id'],
                'lat': address_info['lat'] or self._safe_float(row.get('LA')),
                'lon': address_info['lon'] or self._safe_float(row.get('LO')),
                'phone': str(row.get('CRTELNO', '')),
                'website': str(row.get('CRHOME', '')),
                'operating_hours': None,
                'is_24h': False,
                'is_emergency': False,
                'capacity': self._safe_int(row.get('CRCAPAT')),
                'grade_level': None,
                'facility_extra': {
                    'childcare_type': str(row.get('CRTYPENAME', ''))
                },
                'data_source': 'openseoul'
            }
            
            # ì‹¤ì‹œê°„ ëª¨ë“œì¼ ë•Œ ì¦‰ì‹œ ì €ì¥, ì•„ë‹ˆë©´ ë©”ëª¨ë¦¬ì— ëˆ„ì 
            if self.realtime_mode and self.output_dir:
                self._save_success_data_immediately(facility_data, self.output_dir, 'childCare')
                # ì§„í–‰ ìƒí™© ì €ì¥
                progress_data = {
                    "file": str(file_path),
                    "row_index": idx,
                    "facility_type": "childCare",
                    "facility_name": facility_name,
                    "processed_count": idx + 1,
                    "timestamp": pd.Timestamp.now().isoformat()
                }
                self._save_progress_immediately(progress_data, self.output_dir)
            else:
                self.normalized_facilities.append(facility_data)
        logger.info(f"ì–´ë¦°ì´ì§‘ ë°ì´í„° ì •ê·œí™” ì™„ë£Œ: {len(self.normalized_facilities)}ê°œ")

    def _normalize_schools(self, file_path: Path, start_row: int = 0):
        """í•™êµ ë°ì´í„° ì •ê·œí™”"""
        if not file_path.exists():
            logger.warning(f"í•™êµ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {file_path}")
            return

        df = read_csv_with_auto_encoding(file_path)
        
        # ì‹œì‘ í–‰ ì„¤ì •
        if start_row > 0:
            logger.info(f"í•™êµ ë°ì´í„° ì •ê·œí™” ì¬ê°œ: {start_row}í–‰ë¶€í„° ì‹œì‘, ì´ {len(df)}ê°œ ì¤‘ {len(df) - start_row}ê°œ ë‚¨ìŒ")
            df = df.iloc[start_row:]
        else:
            logger.info(f"í•™êµ ë°ì´í„° ì •ê·œí™” ì‹œì‘: {len(df)}ê°œ")

        for idx, row in df.iterrows():
            address_raw = str(row.get('ORG_RDNMA', ''))
            facility_name = str(row.get('SCHUL_NM', ''))
            address_info = self._normalize_address(address_raw, facility_name, 'school', str(file_path), idx)
            
            # ì£¼ì†Œ ì •ê·œí™” ì‹¤íŒ¨ ì‹œ ê±´ë„ˆë›°ê¸° (ì´ë¯¸ failed_addresses.jsonlì— ì €ì¥ë¨)
            if not address_info.get('normalization_success', False):
                continue
            
            facility_data = {
                'facility_id': self._generate_facility_id('school'),
                'cd': self._get_facility_cd('school'),
                'name': facility_name,
                'address_raw': address_info['address_raw'],
                'address_nm': address_info['address_nm'],
                'address_id': address_info['address_id'],
                'lat': address_info['lat'] or self._safe_float(row.get('LAT')),
                'lon': address_info['lon'] or self._safe_float(row.get('LON')),
                'phone': str(row.get('ORG_TELNO', '')),
                'website': str(row.get('HMPG_ADRES', '')),
                'operating_hours': None,
                'is_24h': False,
                'is_emergency': False,
                'capacity': None,
                'grade_level': str(row.get('SCHUL_CRSE_SC_NM', '')),
                'facility_extra': {
                    'foundation_type': str(row.get('FOND_SC_NM', '')),
                    'special_class_exist': str(row.get('INDST_SPECL_CCCCL_EXST_YN', '')),
                    'high_school_type': str(row.get('HS_GNRL_BUSNS_SC_NM', '')),
                    'special_purpose': str(row.get('SPCLY_PURPS_HS_ORD_NM', ''))
                },
                'data_source': 'openseoul'
            }
            
            # ì‹¤ì‹œê°„ ëª¨ë“œì¼ ë•Œ ì¦‰ì‹œ ì €ì¥, ì•„ë‹ˆë©´ ë©”ëª¨ë¦¬ì— ëˆ„ì 
            if self.realtime_mode and self.output_dir:
                self._save_success_data_immediately(facility_data, self.output_dir, 'school')
                # ì§„í–‰ ìƒí™© ì €ì¥
                progress_data = {
                    "file": str(file_path),
                    "row_index": idx,
                    "facility_type": "school",
                    "facility_name": facility_name,
                    "processed_count": idx + 1,
                    "timestamp": pd.Timestamp.now().isoformat()
                }
                self._save_progress_immediately(progress_data, self.output_dir)
            else:
                self.normalized_facilities.append(facility_data)
        logger.info(f"í•™êµ ë°ì´í„° ì •ê·œí™” ì™„ë£Œ: {len(self.normalized_facilities)}ê°œ")

    def _normalize_parks(self, file_path: Path, start_row: int = 0):
        """ê³µì› ë°ì´í„° ì •ê·œí™”"""
        if not file_path.exists():
            logger.warning(f"ê³µì› íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {file_path}")
            return

        df = read_csv_with_auto_encoding(file_path)
        
        # ì‹œì‘ í–‰ ì„¤ì •
        if start_row > 0:
            logger.info(f"ê³µì› ë°ì´í„° ì •ê·œí™” ì¬ê°œ: {start_row}í–‰ë¶€í„° ì‹œì‘, ì´ {len(df)}ê°œ ì¤‘ {len(df) - start_row}ê°œ ë‚¨ìŒ")
            df = df.iloc[start_row:]
        else:
            logger.info(f"ê³µì› ë°ì´í„° ì •ê·œí™” ì‹œì‘: {len(df)}ê°œ")

        for idx, row in df.iterrows():
            address_raw = str(row.get('P_ADDR', ''))
            facility_name = str(row.get('P_PARK', ''))
            original_file = str(file_path)
            address_info = self._normalize_address(address_raw, facility_name, 'park', original_file, idx)
            
            # ì£¼ì†Œ ì •ê·œí™” ì‹¤íŒ¨ ì‹œ ê±´ë„ˆë›°ê¸° (ì´ë¯¸ failed_addresses.jsonlì— ì €ì¥ë¨)
            if not address_info.get('normalization_success', False):
                continue
            
            facility_data = {
                'facility_id': self._generate_facility_id('park'),
                'cd': self._get_facility_cd('park'),
                'name': facility_name,
                'address_raw': address_info.get('address_raw', address_raw),
                'address_nm': address_info.get('address_nm'),
                'address_id': address_info.get('address_id'),
                'lat': address_info.get('lat') or self._safe_float(row.get('LATITUDE')),
                'lon': address_info.get('lon') or self._safe_float(row.get('LONGITUDE')),
                'phone': str(row.get('P_ADMINTEL', '')),
                'website': str(row.get('TEMPLATE_URL', '')),
                'operating_hours': None,
                'is_24h': False,
                'is_emergency': False,
                'capacity': None,
                'grade_level': None,
                'facility_extra': {
                    'description': str(row.get('P_LIST_CONTENT', '')),
                    'area': self._safe_float(row.get('AREA')),
                    'open_date': str(row.get('OPEN_DT', '')),
                    'main_equipment': str(row.get('MAIN_EQUIP', '')),
                    'main_plants': str(row.get('MAIN_PLANTS', '')),
                    'guidance': str(row.get('GUIDANCE', '')),
                    'visit_road': str(row.get('VISIT_ROAD', '')),
                    'use_refer': str(row.get('USE_REFER', ''))
                },
                'data_source': 'openseoul'
            }
            
            # ì‹¤ì‹œê°„ ëª¨ë“œì¼ ë•Œ ì¦‰ì‹œ ì €ì¥, ì•„ë‹ˆë©´ ë©”ëª¨ë¦¬ì— ëˆ„ì 
            if self.realtime_mode and self.output_dir:
                self._save_success_data_immediately(facility_data, self.output_dir, 'park')
                # ì§„í–‰ ìƒí™© ì €ì¥
                progress_data = {
                    "file": str(file_path),
                    "row_index": idx,
                    "facility_type": "park",
                    "facility_name": facility_name,
                    "processed_count": idx + 1,
                    "timestamp": pd.Timestamp.now().isoformat()
                }
                self._save_progress_immediately(progress_data, self.output_dir)
            else:
                self.normalized_facilities.append(facility_data)
        logger.info(f"ê³µì› ë°ì´í„° ì •ê·œí™” ì™„ë£Œ: {len(self.normalized_facilities)}ê°œ")

    def _normalize_subway_stations(self, file_path: Path):
        """ì§€í•˜ì² ì—­ ë°ì´í„° ì •ê·œí™” - subwayStationMaster CSVì—ì„œ ì§ì ‘ ì¶”ì¶œ"""
        if not file_path.exists():
            logger.warning(f"ì§€í•˜ì² ì—­ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {file_path}")
            return

        # ì§€í•˜ì² ì—­ ì •ë³´ íŒŒì¼ ë¡œë“œ
        df_stn = read_csv_with_auto_encoding(file_path, dtype=str)
        logger.info(f"ì§€í•˜ì² ì—­ ë°ì´í„° ë¡œë“œ: {len(df_stn)}ê°œ")

        # ì—­ëª…ë³„ë¡œ ë…¸ì„  ì •ë³´ë¥¼ ìˆ˜ì§‘í•˜ì—¬ í™˜ìŠ¹ì—­ ì²˜ë¦¬
        station_lines_map = {}
        
        # 1ë‹¨ê³„: ì—­ëª…ë³„ë¡œ ë…¸ì„  ì •ë³´ ìˆ˜ì§‘
        for _, row in df_stn.iterrows():
            station_name = str(row.get("BLDN_NM", "")).strip()
            line_name = str(row.get("ROUTE", "")).strip()
            
            if station_name and line_name:
                if station_name not in station_lines_map:
                    station_lines_map[station_name] = []
                if line_name not in station_lines_map[station_name]:
                    station_lines_map[station_name].append(line_name)

        # 2ë‹¨ê³„: ì—­ ë°ì´í„° ìƒì„± (í™˜ìŠ¹ì—­ ì •ë³´ í¬í•¨)
        for _, row in df_stn.iterrows():
            station_name = str(row.get("BLDN_NM", "")).strip()
            line_name = str(row.get("ROUTE", "")).strip()
            
            logger.info(f"ì²˜ë¦¬ ì¤‘: {station_name} - {line_name}")
            
            if not station_name or not line_name:
                logger.warning(f"ë¹ˆ ê°’ ê±´ë„ˆë›°ê¸°: station_name='{station_name}', line_name='{line_name}'")
                continue

            # í•´ë‹¹ ì—­ì˜ ëª¨ë“  ë…¸ì„  ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            all_lines = station_lines_map.get(station_name, [line_name])
            is_transfer = len(all_lines) > 1

            # ìµœì¢… ë°ì´í„° êµ¬ì„± (DB ìŠ¤í‚¤ë§ˆì— ë§ê²Œ)
            station_data = {
                "id": self._generate_transport_id('subway'),  # sub001, sub002, ...
                "transport_type": "subway",
                "name": station_name,
                "official_code": str(row.get("BLDN_ID", "")),  # ê±´ë¬¼ IDë¥¼ ê³µì‹ ì½”ë“œë¡œ ì‚¬ìš©
                "line_name": line_name,
                "stop_type": None,  # ì§€í•˜ì² ì€ stop_type ì—†ìŒ
                "is_transfer": is_transfer,  # í™˜ìŠ¹ì—­ ì—¬ë¶€
                "lat": self._safe_float(row.get("LAT")),  # ìœ„ë„
                "lon": self._safe_float(row.get("LOT")),  # ê²½ë„ (LOT ì»¬ëŸ¼ëª…)
                "extra_info": {
                    "all_lines": all_lines  # í•´ë‹¹ ì—­ì˜ ëª¨ë“  ë…¸ì„  ì •ë³´
                }
            }
            
            # ì‹¤ì‹œê°„ ëª¨ë“œì¼ ë•Œ ì¦‰ì‹œ ì €ì¥, ì•„ë‹ˆë©´ ë©”ëª¨ë¦¬ì— ëˆ„ì 
            if self.realtime_mode and self.output_dir:
                self._save_success_data_immediately(station_data, self.output_dir, 'subway')
                # ì§„í–‰ ìƒí™© ì €ì¥
                progress_data = {
                    "file": str(file_path),
                    "row_index": 0,
                    "facility_type": "subway",
                    "facility_name": station_name,
                    "processed_count": len(self.normalized_subway_stations) + 1,
                    "timestamp": pd.Timestamp.now().isoformat()
                }
                self._save_progress_immediately(progress_data, self.output_dir)
            else:
                self.normalized_subway_stations.append(station_data)

        logger.info(f"ì§€í•˜ì² ì—­ ë°ì´í„° ì •ê·œí™” ì™„ë£Œ: {len(self.normalized_subway_stations)}ê°œ")

    def _normalize_pharmacies(self, file_path: Path, start_row: int = 0):
        """ì•½êµ­ ë°ì´í„° ì •ê·œí™”"""
        if not file_path.exists():
            logger.warning(f"ì•½êµ­ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {file_path}")
            return

        df = read_csv_with_auto_encoding(file_path)
        
        # ì‹œì‘ í–‰ ì„¤ì •
        if start_row > 0:
            logger.info(f"ì•½êµ­ ë°ì´í„° ì •ê·œí™” ì¬ê°œ: {start_row}í–‰ë¶€í„° ì‹œì‘, ì´ {len(df)}ê°œ ì¤‘ {len(df) - start_row}ê°œ ë‚¨ìŒ")
            df = df.iloc[start_row:]
        else:
            logger.info(f"ì•½êµ­ ë°ì´í„° ì •ê·œí™” ì‹œì‘: {len(df)}ê°œ")

        for idx, row in df.iterrows():
            address_raw = str(row.get('DUTYADDR', ''))
            facility_name = str(row.get('DUTYNAME', ''))
            address_info = self._normalize_address(address_raw, facility_name, 'pharmacy', str(file_path), idx)
            
            # ì£¼ì†Œ ì •ê·œí™” ì‹¤íŒ¨ ì‹œ ê±´ë„ˆë›°ê¸° (ì´ë¯¸ failed_addresses.jsonlì— ì €ì¥ë¨)
            if not address_info.get('normalization_success', False):
                continue
            
            facility_data = {
                'facility_id': self._generate_facility_id('pharmacy'),
                'cd': self._get_facility_cd('pharmacy'),
                'name': facility_name,
                'address_raw': address_info['address_raw'],
                'address_nm': address_info['address_nm'],
                'address_id': address_info['address_id'],
                'lat': address_info['lat'] or self._safe_float(row.get('WGS84LAT')),
                'lon': address_info['lon'] or self._safe_float(row.get('WGS84LON')),
                'phone': str(row.get('DUTYTEL1', '')),
                'website': None,
                'operating_hours': None,
                'is_24h': False,
                'is_emergency': False,
                'capacity': None,
                'grade_level': None,
                'facility_extra': {
                    'pharmacy_id': str(row.get('HOSID', '')),
                    'district': str(row.get('SIGUN_NM', ''))
                },
                'data_source': 'openseoul'
            }
            
            # ì‹¤ì‹œê°„ ëª¨ë“œì¼ ë•Œ ì¦‰ì‹œ ì €ì¥, ì•„ë‹ˆë©´ ë©”ëª¨ë¦¬ì— ëˆ„ì 
            if self.realtime_mode and self.output_dir:
                self._save_success_data_immediately(facility_data, self.output_dir, 'pharmacy')
                # ì§„í–‰ ìƒí™© ì €ì¥
                progress_data = {
                    "file": str(file_path),
                    "row_index": idx,
                    "facility_type": "pharmacy",
                    "facility_name": facility_name,
                    "processed_count": idx + 1,
                    "timestamp": pd.Timestamp.now().isoformat()
                }
                self._save_progress_immediately(progress_data, self.output_dir)
            else:
                self.normalized_facilities.append(facility_data)
        logger.info(f"ì•½êµ­ ë°ì´í„° ì •ê·œí™” ì™„ë£Œ: {len(self.normalized_facilities)}ê°œ")

    def _normalize_kindergartens(self, file_path: Path, start_row: int = 0):
        """ìœ ì¹˜ì› ë°ì´í„° ì •ê·œí™”"""
        if not file_path.exists():
            logger.warning(f"ìœ ì¹˜ì› íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {file_path}")
            return

        df = read_csv_with_auto_encoding(file_path)
        
        # ì‹œì‘ í–‰ ì„¤ì •
        if start_row > 0:
            logger.info(f"ìœ ì¹˜ì› ë°ì´í„° ì •ê·œí™” ì¬ê°œ: {start_row}í–‰ë¶€í„° ì‹œì‘, ì´ {len(df)}ê°œ ì¤‘ {len(df) - start_row}ê°œ ë‚¨ìŒ")
            df = df.iloc[start_row:]
        else:
            logger.info(f"ìœ ì¹˜ì› ë°ì´í„° ì •ê·œí™” ì‹œì‘: {len(df)}ê°œ")

        for idx, row in df.iterrows():
            address_raw = str(row.get('ADDR', ''))
            facility_name = str(row.get('KDGT_NM', ''))
            address_info = self._normalize_address(address_raw, facility_name, 'childSchool', str(file_path), idx)
            
            # ì£¼ì†Œ ì •ê·œí™” ì‹¤íŒ¨ ì‹œ ê±´ë„ˆë›°ê¸° (ì´ë¯¸ failed_addresses.jsonlì— ì €ì¥ë¨)
            if not address_info.get('normalization_success', False):
                continue
            
            facility_data = {
                'facility_id': self._generate_facility_id('childSchool'),
                'cd': self._get_facility_cd('childSchool'),
                'name': facility_name,
                'address_raw': address_info['address_raw'],
                'address_nm': address_info['address_nm'],
                'address_id': address_info['address_id'],
                'lat': address_info['lat'],
                'lon': address_info['lon'],
                'phone': str(row.get('TELNO', '')),
                'website': str(row.get('HMPG', '')),
                'operating_hours': None,
                'is_24h': False,
                'is_emergency': False,
                'capacity': self._safe_int(row.get('MIX_TDL_CNT')),
                'grade_level': None,
                'facility_extra': {
                    'foundation_type': str(row.get('FNDN_TYPE', ''))
                },
                'data_source': 'openseoul'
            }
            
            # ì‹¤ì‹œê°„ ëª¨ë“œì¼ ë•Œ ì¦‰ì‹œ ì €ì¥, ì•„ë‹ˆë©´ ë©”ëª¨ë¦¬ì— ëˆ„ì 
            if self.realtime_mode and self.output_dir:
                self._save_success_data_immediately(facility_data, self.output_dir, 'childSchool')
                # ì§„í–‰ ìƒí™© ì €ì¥
                progress_data = {
                    "file": str(file_path),
                    "row_index": idx,
                    "facility_type": "childSchool",
                    "facility_name": facility_name,
                    "processed_count": idx + 1,
                    "timestamp": pd.Timestamp.now().isoformat()
                }
                self._save_progress_immediately(progress_data, self.output_dir)
            else:
                self.normalized_facilities.append(facility_data)
        logger.info(f"ìœ ì¹˜ì› ë°ì´í„° ì •ê·œí™” ì™„ë£Œ: {len(self.normalized_facilities)}ê°œ")

    def _normalize_colleges(self, file_path: Path, start_row: int = 0):
        """ëŒ€í•™ ë°ì´í„° ì •ê·œí™”"""
        if not file_path.exists():
            logger.warning(f"ëŒ€í•™ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {file_path}")
            return

        df = read_csv_with_auto_encoding(file_path)
        
        # ì‹œì‘ í–‰ ì„¤ì •
        if start_row > 0:
            logger.info(f"ëŒ€í•™ ë°ì´í„° ì •ê·œí™” ì¬ê°œ: {start_row}í–‰ë¶€í„° ì‹œì‘, ì´ {len(df)}ê°œ ì¤‘ {len(df) - start_row}ê°œ ë‚¨ìŒ")
            df = df.iloc[start_row:]
        else:
            logger.info(f"ëŒ€í•™ ë°ì´í„° ì •ê·œí™” ì‹œì‘: {len(df)}ê°œ")

        for idx, row in df.iterrows():
            address_raw = str(row.get('ADD_KOR', ''))
            facility_name = str(row.get('NAME_KOR', ''))
            address_info = self._normalize_address(address_raw, facility_name, 'college', str(file_path), idx)
            
            # ì£¼ì†Œ ì •ê·œí™” ì‹¤íŒ¨ ì‹œ ê±´ë„ˆë›°ê¸° (ì´ë¯¸ failed_addresses.jsonlì— ì €ì¥ë¨)
            if not address_info.get('normalization_success', False):
                continue
            
            facility_data = {
                'facility_id': self._generate_facility_id('college'),
                'cd': self._get_facility_cd('college'),
                'name': facility_name,
                'address_raw': address_info['address_raw'],
                'address_nm': address_info['address_nm'],
                'address_id': address_info['address_id'],
                'lat': address_info['lat'],
                'lon': address_info['lon'],
                'phone': str(row.get('TEL', '')),
                'website': str(row.get('HP', '')),
                'operating_hours': None,
                'is_24h': False,
                'is_emergency': False,
                'capacity': None,
                'grade_level': None,
                'facility_extra': {
                    'category': str(row.get('CATE1_NAME', '')),
                    'branch': str(row.get('BRANCH', '')),
                    'type': str(row.get('TYPE', ''))
                },
                'data_source': 'openseoul'
            }
            
            # ì‹¤ì‹œê°„ ëª¨ë“œì¼ ë•Œ ì¦‰ì‹œ ì €ì¥, ì•„ë‹ˆë©´ ë©”ëª¨ë¦¬ì— ëˆ„ì 
            if self.realtime_mode and self.output_dir:
                self._save_success_data_immediately(facility_data, self.output_dir, 'college')
                # ì§„í–‰ ìƒí™© ì €ì¥
                progress_data = {
                    "file": str(file_path),
                    "row_index": idx,
                    "facility_type": "college",
                    "facility_name": facility_name,
                    "processed_count": idx + 1,
                    "timestamp": pd.Timestamp.now().isoformat()
                }
                self._save_progress_immediately(progress_data, self.output_dir)
            else:
                self.normalized_facilities.append(facility_data)
        logger.info(f"ëŒ€í•™ ë°ì´í„° ì •ê·œí™” ì™„ë£Œ: {len(self.normalized_facilities)}ê°œ")


    def _normalize_bus_stops(self, file_path: Path):
        """ë²„ìŠ¤ì •ë¥˜ì†Œ ë°ì´í„° ì •ê·œí™” - ê²½ë„,ìœ„ë„ í˜•ì‹ìœ¼ë¡œ address_raw ì €ì¥"""
        if not file_path.exists():
            logger.warning(f"ë²„ìŠ¤ì •ë¥˜ì†Œ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {file_path}")
            return

        df = read_csv_with_auto_encoding(file_path)
        logger.info(f"ë²„ìŠ¤ì •ë¥˜ì†Œ ë°ì´í„° ì •ê·œí™” ì‹œì‘: {len(df)}ê°œ")

        for _, row in df.iterrows():
            # ê²½ë„(XCRD), ìœ„ë„(YCRD)ë¥¼ 'ê²½ë„,ìœ„ë„' í˜•ì‹ìœ¼ë¡œ address_rawì— ì €ì¥
            x_coord = self._safe_float(row.get('XCRD'))
            y_coord = self._safe_float(row.get('YCRD'))
            
            if x_coord is not None and y_coord is not None:
                address_raw = f"{x_coord},{y_coord}"
            else:
                address_raw = ""
            
            # ë²„ìŠ¤ì •ë¥˜ì†Œ ë°ì´í„°ë¥¼ DB ìŠ¤í‚¤ë§ˆì— ë§ê²Œ êµ¬ì„±
            facility_data = {
                'id': self._generate_transport_id('bus'),  # bus001, bus002, ...
                'transport_type': 'bus',
                'name': str(row.get('STOPS_NM', '')),  # ì •ë¥˜ì†Œëª…
                'official_code': str(row.get('STOPS_NO', '')),  # ì •ë¥˜ì†Œ ë²ˆí˜¸ë¥¼ ê³µì‹ ì½”ë“œë¡œ ì‚¬ìš©
                'line_name': None,  # ë²„ìŠ¤ëŠ” ë…¸ì„  ì •ë³´ ì—†ìŒ
                'stop_type': str(row.get('STOPS_TYPE', '')),  # ì •ë¥˜ì†Œ ìœ í˜•
                'lat': y_coord,  # ìœ„ë„
                'lon': x_coord,  # ê²½ë„
                'extra_info': {
                    'node_id': str(row.get('NODE_ID', '')),
                    'x_coord': x_coord,
                    'y_coord': y_coord
                }
            }
            
            # ì‹¤ì‹œê°„ ëª¨ë“œì¼ ë•Œ ì¦‰ì‹œ ì €ì¥, ì•„ë‹ˆë©´ ë©”ëª¨ë¦¬ì— ëˆ„ì 
            if self.realtime_mode and self.output_dir:
                self._save_success_data_immediately(facility_data, self.output_dir, 'bus')
                # ì§„í–‰ ìƒí™© ì €ì¥
                progress_data = {
                    "file": str(file_path),
                    "row_index": 0,  # ë²„ìŠ¤ì •ë¥˜ì†ŒëŠ” ì¸ë±ìŠ¤ê°€ ë³µì¡í•˜ë¯€ë¡œ 0ìœ¼ë¡œ ì„¤ì •
                    "facility_type": "bus",
                    "facility_name": str(row.get('STOPS_NM', '')),
                    "processed_count": len(self.normalized_bus_stops) + 1,
                    "timestamp": pd.Timestamp.now().isoformat()
                }
                self._save_progress_immediately(progress_data, self.output_dir)
            else:
                self.normalized_bus_stops.append(facility_data)
        logger.info(f"ë²„ìŠ¤ì •ë¥˜ì†Œ ë°ì´í„° ì •ê·œí™” ì™„ë£Œ: {len(self.normalized_bus_stops)}ê°œ")

    def normalize_openseoul_data(self, output_dir: Path = None, realtime_mode: bool = False) -> Dict[str, List[Dict]]:
        """OpenSeoul CSV íŒŒì¼ë“¤ì„ ì •ê·œí™”"""
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ë° ì‹¤ì‹œê°„ ëª¨ë“œ ì„¤ì •
        if output_dir:
            self.output_dir = output_dir
            self.failed_output_dir = output_dir
            self.realtime_mode = realtime_mode
            output_dir.mkdir(parents=True, exist_ok=True)
            
            # ì‹¤ì‹œê°„ ëª¨ë“œì¼ ë•Œ ê¸°ì¡´ JSONL íŒŒì¼ë“¤ ì´ˆê¸°í™”
            if realtime_mode:
                self._initialize_realtime_files(output_dir)
            
            # ê¸°ì¡´ facility_id ë¡œë“œí•˜ì—¬ ì¹´ìš´í„° ì´ˆê¸°í™”
            self.facility_counters = self._load_existing_facility_ids(output_dir)
        else:
            # ì¶œë ¥ ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ì¹´ìš´í„°ë¥¼ 0ë¶€í„° ì‹œì‘
            self.facility_counters = {prefix: 0 for prefix in self.facility_id_prefix_map.values()}
        
        openseoul_dir = self.data_dir  # backend/data/public-api/openseoul

        # ì–´ë¦°ì´ì§‘ ë°ì´í„° - ì§„í–‰ ìƒí™© í™•ì¸ í›„ ì¬ê°œ
        childcare_file = openseoul_dir / "seoul_ChildCareInfo_20250928.csv"
        childcare_start_row = self.get_resume_point(output_dir, 'childCare') if output_dir else 0
        if childcare_start_row > 0:
            logger.info(f"ì–´ë¦°ì´ì§‘ ë°ì´í„° ì¬ê°œ: {childcare_start_row}í–‰ë¶€í„° ì‹œì‘")
        self._normalize_childcare_centers(childcare_file, childcare_start_row)
        
        # ìœ ì¹˜ì› ë°ì´í„° - ì§„í–‰ ìƒí™© í™•ì¸ í›„ ì¬ê°œ
        kindergarten_file = openseoul_dir / "seoul_childSchoolInfo_20250919.csv"
        kindergarten_start_row = self.get_resume_point(output_dir, 'childSchool') if output_dir else 0
        if kindergarten_start_row > 0:
            logger.info(f"ìœ ì¹˜ì› ë°ì´í„° ì¬ê°œ: {kindergarten_start_row}í–‰ë¶€í„° ì‹œì‘")
        self._normalize_kindergartens(kindergarten_file, kindergarten_start_row)
        
        # í•™êµ ë°ì´í„° (ì´ˆì¤‘ê³ ) - ì§„í–‰ ìƒí™© í™•ì¸ í›„ ì¬ê°œ
        school_file = openseoul_dir / "seoul_neisSchoolInfo_20250928.csv"
        school_start_row = self.get_resume_point(output_dir, 'school') if output_dir else 0
        if school_start_row > 0:
            logger.info(f"í•™êµ ë°ì´í„° ì¬ê°œ: {school_start_row}í–‰ë¶€í„° ì‹œì‘")
        self._normalize_schools(school_file, school_start_row)
        
        # # ëŒ€í•™ ë°ì´í„° - ì§„í–‰ ìƒí™© í™•ì¸ í›„ ì¬ê°œ
        college_file = openseoul_dir / "seoul_SebcCollegeInfoKor_20250919.csv"
        college_start_row = self.get_resume_point(output_dir, 'college') if output_dir else 0
        if college_start_row > 0:
            logger.info(f"ëŒ€í•™ ë°ì´í„° ì¬ê°œ: {college_start_row}í–‰ë¶€í„° ì‹œì‘")
        self._normalize_colleges(college_file, college_start_row)
        
        # ê³µì› ë°ì´í„° - ì§„í–‰ ìƒí™© í™•ì¸ í›„ ì¬ê°œ
        park_file = openseoul_dir / "seoul_SearchParkInfoService_20250919.csv"
        park_start_row = self.get_resume_point(output_dir, 'park') if output_dir else 0
        if park_start_row > 0:
            logger.info(f"ê³µì› ë°ì´í„° ì¬ê°œ: {park_start_row}í–‰ë¶€í„° ì‹œì‘")
        self._normalize_parks(park_file, park_start_row)
        
        # ì•½êµ­ ë°ì´í„° - ì§„í–‰ ìƒí™© í™•ì¸ í›„ ì¬ê°œ
        pharmacy_file = self.find_latest_csv_file(openseoul_dir, "seoul_TbPharmacyOperateInfo")
        if pharmacy_file:
            pharmacy_start_row = self.get_resume_point(output_dir, 'pharmacy') if output_dir else 0
            if pharmacy_start_row > 0:
                logger.info(f"ì•½êµ­ ë°ì´í„° ì¬ê°œ: {pharmacy_start_row}í–‰ë¶€í„° ì‹œì‘")
            self._normalize_pharmacies(pharmacy_file, pharmacy_start_row)
        else:
            logger.warning("ì•½êµ­ CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        # ì§€í•˜ì² ì—­ê³¼ ë²„ìŠ¤ì •ë¥˜ì†ŒëŠ” ì¢Œí‘œ ê¸°ë°˜ì´ë¯€ë¡œ ì œì™¸
        subway_file = openseoul_dir / "seoul_subwayStationMaster_20250928.csv"
        self._normalize_subway_stations(subway_file)
        bus_stop_file = openseoul_dir / "seoul_busStopLocationXyInfo_20250928.csv"
        self._normalize_bus_stops(bus_stop_file)

        # localdata í´ë” ë°ì´í„° ì²˜ë¦¬
        localdata_dir = self.data_dir.parent / "localdata"
        logger.info(f"localdata ë””ë ‰í† ë¦¬: {localdata_dir}")
        logger.info(f"localdata ë””ë ‰í† ë¦¬ ì¡´ì¬: {localdata_dir.exists()}")
        
        # ê³µê³µì²´ìœ¡ì‹œì„¤ ë°ì´í„° - ì§„í–‰ ìƒí™© í™•ì¸ í›„ ì¬ê°œ
        sports_file = localdata_dir / "utf8_ì„œìš¸ì‹œ ê³µê³µì²´ìœ¡ì‹œì„¤ ì •ë³´.csv"
        logger.info(f"ì²´ìœ¡ì‹œì„¤ íŒŒì¼: {sports_file}")
        logger.info(f"ì²´ìœ¡ì‹œì„¤ íŒŒì¼ ì¡´ì¬: {sports_file.exists()}")
        if sports_file.exists():
            sports_start_row = self.get_resume_point(output_dir, 'gym') if output_dir else 0
            if sports_start_row > 0:
                logger.info(f"ê³µê³µì²´ìœ¡ì‹œì„¤ ë°ì´í„° ì¬ê°œ: {sports_start_row}í–‰ë¶€í„° ì‹œì‘")
            self._normalize_sports_facilities(sports_file, sports_start_row)
        
        # ë§ˆíŠ¸ ë°ì´í„° - ì§„í–‰ ìƒí™© í™•ì¸ í›„ ì¬ê°œ
        mart_file = localdata_dir / "utf8_ì„œìš¸ì‹œ ë§ˆíŠ¸.csv"
        if mart_file.exists():
            mart_start_row = self.get_resume_point(output_dir, 'mt') if output_dir else 0
            if mart_start_row > 0:
                logger.info(f"ë§ˆíŠ¸ ë°ì´í„° ì¬ê°œ: {mart_start_row}í–‰ë¶€í„° ì‹œì‘")
            self._normalize_marts(mart_file, mart_start_row)
        
        # ë³‘ì› ë°ì´í„° - ì§„í–‰ ìƒí™© í™•ì¸ í›„ ì¬ê°œ
        hospital_file = localdata_dir / "utf8_ì„œìš¸ì‹œë³‘ì›_ë‚´ê³¼ì†Œì•„ê³¼ì‘ê¸‰ì˜í•™ê³¼.csv"
        if hospital_file.exists():
            hospital_start_row = self.get_resume_point(output_dir, 'hos') if output_dir else 0
            if hospital_start_row > 0:
                logger.info(f"ë³‘ì› ë°ì´í„° ì¬ê°œ: {hospital_start_row}í–‰ë¶€í„° ì‹œì‘")
            self._normalize_hospitals(hospital_file, hospital_start_row)
        
        # í¸ì˜ì  ë°ì´í„° - ì§„í–‰ ìƒí™© í™•ì¸ í›„ ì¬ê°œ
        convenience_file = self.find_latest_csv_file(localdata_dir, "utf8_ì„œìš¸ì‹œ í¸ì˜ì ")
        if convenience_file:
            convenience_start_row = self.get_resume_point(output_dir, 'convenience') if output_dir else 0
            if convenience_start_row > 0:
                logger.info(f"í¸ì˜ì  ë°ì´í„° ì¬ê°œ: {convenience_start_row}í–‰ë¶€í„° ì‹œì‘")
            self._normalize_convenience_stores(convenience_file, convenience_start_row)
        else:
            logger.warning("í¸ì˜ì  CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        logger.info(f"ì´ {len(self.normalized_facilities)}ê°œì˜ ì‹œì„¤ ë°ì´í„°ì™€ {len(self.normalized_subway_stations)}ê°œì˜ ì§€í•˜ì² ì—­ ë°ì´í„° ì •ê·œí™” ì™„ë£Œ.")
        
        # ì¹´ìš´í„° ìƒíƒœ ë¡œê¹…
        logger.info("ìƒì„±ëœ facility_id ì¹´ìš´í„° ìƒíƒœ:")
        for prefix, count in self.facility_counters.items():
            if count > 0:
                logger.info(f"  {prefix}: {count}ê°œ")
        
        return {
            "public_facilities": self.normalized_facilities,
            "subway_stations": self.normalized_subway_stations,
            "failed_addresses": self.failed_addresses
        }
    


    def _normalize_sports_facilities(self, file_path: Path, start_row: int = 0):
        """ê³µê³µì²´ìœ¡ì‹œì„¤ ì •ë³´ ì •ê·œí™”"""
        logger.info(f"ê³µê³µì²´ìœ¡ì‹œì„¤ ì •ë³´ ì •ê·œí™” ì‹œì‘: {file_path}")
        
        df = read_csv_with_auto_encoding(file_path, dtype=str)
        
        # ì‹œì‘ í–‰ ì„¤ì •
        if start_row > 0:
            logger.info(f"ê³µê³µì²´ìœ¡ì‹œì„¤ ë°ì´í„° ì •ê·œí™” ì¬ê°œ: {start_row}í–‰ë¶€í„° ì‹œì‘, ì´ {len(df)}ê°œ ì¤‘ {len(df) - start_row}ê°œ ë‚¨ìŒ")
            df = df.iloc[start_row:]
        else:
            logger.info(f"ê³µê³µì²´ìœ¡ì‹œì„¤ ë°ì´í„° ë¡œë“œ: {len(df)}ê°œ")
        
        for idx, row in df.iterrows():
            try:
                # ì£¼ì†Œ ì •ê·œí™”
                original_file = str(file_path)
                address_info = self._normalize_address(
                    address_raw=row.get('ì‹œì„¤ì£¼ì†Œ', ''),
                    facility_name=row.get('ì‹œì„¤ëª…', ''),
                    facility_type='gym',
                    original_file=original_file,
                    original_row_index=idx
                )
                
                # ì£¼ì†Œ ì •ê·œí™” ì‹¤íŒ¨ ì‹œ ê±´ë„ˆë›°ê¸° (ì´ë¯¸ failed_addresses.jsonlì— ì €ì¥ë¨)
                if not address_info.get('normalization_success', False):
                    continue
                
                # ì‹œì„¤ ì •ë³´ êµ¬ì„±
                facility = {
                    'facility_id': self._generate_facility_id('gym'),
                    'cd': self._get_facility_cd('gym'),
                    'name': row.get('ì‹œì„¤ëª…', ''),
                    'address_raw': address_info['address_raw'],
                    'address_nm': address_info['address_nm'],
                    'address_id': address_info['address_id'],
                    'lat': address_info['lat'],
                    'lon': address_info['lon'],
                    'phone': row.get('ì—°ë½ì²˜', ''),
                    'website': row.get('í™ˆí˜ì´ì§€', ''),
                    'operating_hours': f"í‰ì¼: {row.get('ìš´ì˜ì‹œê°„_í‰ì¼', '')}, ì£¼ë§: {row.get('ìš´ì˜ì‹œê°„_ì£¼ë§', '')}, ê³µíœ´ì¼: {row.get('ìš´ì˜ì‹œê°„_ê³µíœ´ì¼', '')}",
                    'capacity': self._safe_int(row.get('ì‹œì„¤ê·œëª¨', '')),
                    'facility_extra': {
                        'ì‹œì„¤ìœ í˜•': row.get('ì‹œì„¤ìœ í˜•', ''),
                        'ìš´ì˜ê¸°ê´€': row.get('ìš´ì˜ê¸°ê´€', ''),
                        'ì‹œì„¤ëŒ€ê´€ì—¬ë¶€': row.get('ì‹œì„¤ëŒ€ê´€ì—¬ë¶€', ''),
                        'ì‹œì„¤ì‚¬ìš©ë£Œ': row.get('ì‹œì„¤ì‚¬ìš©ë£Œ', ''),
                        'ì£¼ì°¨ì •ë³´': row.get('ì£¼ì°¨ì •ë³´', ''),
                        'ì‹œì„¤ì¢…ë¥˜': row.get('ì‹œì„¤ì¢…ë¥˜', ''),
                        'ì‹œì„¤ìš´ì˜ìƒíƒœ': row.get('ì‹œì„¤ìš´ì˜ìƒíƒœ', ''),
                        'ì‹œì„¤í¸ì˜ì‹œì„¤': row.get('ì‹œì„¤í¸ì˜ì‹œì„¤', ''),
                        'ë¹„ê³ ': row.get('ë¹„ê³ ', '')
                    },
                    'data_source': 'localdata'
                }
                
                # ì‹¤ì‹œê°„ ëª¨ë“œì¼ ë•Œ ì¦‰ì‹œ ì €ì¥, ì•„ë‹ˆë©´ ë©”ëª¨ë¦¬ì— ëˆ„ì 
                if self.realtime_mode and self.output_dir:
                    self._save_success_data_immediately(facility, self.output_dir, 'gym')
                    # ì§„í–‰ ìƒí™© ì €ì¥
                    progress_data = {
                        "file": str(file_path),
                        "row_index": idx,
                        "facility_type": "gym",
                        "facility_name": row.get('ì‹œì„¤ëª…', ''),
                        "processed_count": idx + 1,
                        "timestamp": pd.Timestamp.now().isoformat()
                    }
                    self._save_progress_immediately(progress_data, self.output_dir)
                else:
                    self.normalized_facilities.append(facility)
                
            except Exception as e:
                logger.error(f"ê³µê³µì²´ìœ¡ì‹œì„¤ ì •ê·œí™” ì˜¤ë¥˜: {row.get('ì‹œì„¤ëª…', '')} - {e}")
        
        logger.info(f"ê³µê³µì²´ìœ¡ì‹œì„¤ ì •ê·œí™” ì™„ë£Œ: {len(df)}ê°œ")

    def _normalize_marts(self, file_path: Path, start_row: int = 0):
        """ë§ˆíŠ¸ ì •ë³´ ì •ê·œí™”"""
        logger.info(f"ë§ˆíŠ¸ ì •ë³´ ì •ê·œí™” ì‹œì‘: {file_path}")
        
        df = read_csv_with_auto_encoding(file_path, dtype=str)
        
        # ì‹œì‘ í–‰ ì„¤ì •
        if start_row > 0:
            logger.info(f"ë§ˆíŠ¸ ë°ì´í„° ì •ê·œí™” ì¬ê°œ: {start_row}í–‰ë¶€í„° ì‹œì‘, ì´ {len(df)}ê°œ ì¤‘ {len(df) - start_row}ê°œ ë‚¨ìŒ")
            df = df.iloc[start_row:]
        else:
            logger.info(f"ë§ˆíŠ¸ ë°ì´í„° ë¡œë“œ: {len(df)}ê°œ")
        
        for idx, row in df.iterrows():
            try:
                # ì£¼ì†Œ ì •ê·œí™” (ë„ë¡œëª…ì „ì²´ì£¼ì†Œ ìš°ì„ , ì—†ìœ¼ë©´ ì†Œì¬ì§€ì „ì²´ì£¼ì†Œ ì‚¬ìš©)
                original_file = str(file_path)
                road_addr = str(row.get('ë„ë¡œëª…ì „ì²´ì£¼ì†Œ', '') or '').strip()
                jibun_addr = str(row.get('ì†Œì¬ì§€ì „ì²´ì£¼ì†Œ', '') or '').strip()
                addr_for_norm = road_addr if road_addr else jibun_addr
                
                address_info = self._normalize_address(
                    address_raw=addr_for_norm,
                    facility_name=row.get('ì‚¬ì—…ì¥ëª…', ''),
                    facility_type='mart',
                    original_file=original_file,
                    original_row_index=idx
                )
                
                # ì£¼ì†Œ ì •ê·œí™” ì‹¤íŒ¨ ì‹œ ê±´ë„ˆë›°ê¸° (ì´ë¯¸ failed_addresses.jsonlì— ì €ì¥ë¨)
                if not address_info.get('normalization_success', False):
                    continue
                
                # ì‹œì„¤ ì •ë³´ êµ¬ì„±
                facility = {
                    'facility_id': self._generate_facility_id('mart'),
                    'cd': self._get_facility_cd('mart'),
                    'name': row.get('ì‚¬ì—…ì¥ëª…', ''),
                    'address_raw': address_info['address_raw'],
                    'address_nm': address_info['address_nm'],
                    'address_id': address_info['address_id'],
                    'lat': address_info['lat'],
                    'lon': address_info['lon'],
                    'phone': row.get('ì†Œì¬ì§€ì „í™”', ''),
                    'website': None,  # í™ˆí˜ì´ì§€ ì •ë³´ ì—†ìŒ
                    'operating_hours': None,  # ìš´ì˜ì‹œê°„ ì •ë³´ ì—†ìŒ
                    'is_24h': False,  # 24ì‹œê°„ ì •ë³´ ì—†ìŒ
                    'is_emergency': False,
                    'capacity': None,
                    'grade_level': None,
                    'facility_extra': {
                        'ì—…íƒœêµ¬ë¶„ëª…': row.get('ì—…íƒœêµ¬ë¶„ëª…', ''),
                        'ì í¬êµ¬ë¶„ëª…': row.get('ì í¬êµ¬ë¶„ëª…', '')
                        
                    },
                    'data_source': 'localdata'
                }
                
                # ì‹¤ì‹œê°„ ëª¨ë“œì¼ ë•Œ ì¦‰ì‹œ ì €ì¥, ì•„ë‹ˆë©´ ë©”ëª¨ë¦¬ì— ëˆ„ì 
                if self.realtime_mode and self.output_dir:
                    self._save_success_data_immediately(facility, self.output_dir, 'mart')
                    # ì§„í–‰ ìƒí™© ì €ì¥
                    progress_data = {
                        "file": str(file_path),
                        "row_index": idx,
                        "facility_type": "mart",
                        "facility_name": row.get('ì‚¬ì—…ì¥ëª…', ''),
                        "processed_count": idx + 1,
                        "timestamp": pd.Timestamp.now().isoformat()
                    }
                    self._save_progress_immediately(progress_data, self.output_dir)
                else:
                    self.normalized_facilities.append(facility)
                
            except Exception as e:
                logger.error(f"ë§ˆíŠ¸ ì •ê·œí™” ì˜¤ë¥˜: {row.get('ì‚¬ì—…ì¥ëª…', '')} - {e}")
        
        logger.info(f"ë§ˆíŠ¸ ì •ê·œí™” ì™„ë£Œ: {len(df)}ê°œ")

    def _normalize_convenience_stores(self, file_path: Path, start_row: int = 0):
        """í¸ì˜ì  ì •ë³´ ì •ê·œí™”"""
        logger.info(f"í¸ì˜ì  ì •ë³´ ì •ê·œí™” ì‹œì‘: {file_path}")
        
        df = read_csv_with_auto_encoding(file_path, dtype=str)
        
        # ì‹œì‘ í–‰ ì„¤ì •
        if start_row > 0:
            logger.info(f"í¸ì˜ì  ë°ì´í„° ì •ê·œí™” ì¬ê°œ: {start_row}í–‰ë¶€í„° ì‹œì‘, ì´ {len(df)}ê°œ ì¤‘ {len(df) - start_row}ê°œ ë‚¨ìŒ")
            df = df.iloc[start_row:]
        else:
            logger.info(f"í¸ì˜ì  ë°ì´í„° ë¡œë“œ: {len(df)}ê°œ")
        
        for idx, row in df.iterrows():
            try:
                # ì£¼ì†Œ ì •ê·œí™” (ë„ë¡œëª…ì „ì²´ì£¼ì†Œ ìš°ì„ , ì—†ìœ¼ë©´ ì†Œì¬ì§€ì „ì²´ì£¼ì†Œ ì‚¬ìš©)
                original_file = str(file_path)
                road_addr = str(row.get('ë„ë¡œëª…ì „ì²´ì£¼ì†Œ', '') or '').strip()
                jibun_addr = str(row.get('ì†Œì¬ì§€ì „ì²´ì£¼ì†Œ', '') or '').strip()
                addr_for_norm = road_addr if road_addr else jibun_addr
                
                address_info = self._normalize_address(
                    address_raw=addr_for_norm,
                    facility_name=row.get('ì‚¬ì—…ì¥ëª…', ''),
                    facility_type='convenience',
                    original_file=original_file,
                    original_row_index=idx
                )
                
                # ì£¼ì†Œ ì •ê·œí™” ì‹¤íŒ¨ ì‹œ ê±´ë„ˆë›°ê¸° (ì´ë¯¸ failed_addresses.jsonlì— ì €ì¥ë¨)
                if not address_info.get('normalization_success', False):
                    continue
                
                # ì¢Œí‘œëŠ” APIì—ì„œ ì¶”ì¶œëœ ê°’ ì‚¬ìš©
                lat = address_info['lat']
                lon = address_info['lon']
                
                # ì‹œì„¤ ì •ë³´ êµ¬ì„±
                facility = {
                    'facility_id': self._generate_facility_id('convenience'),
                    'cd': self._get_facility_cd('convenience'),
                    'name': row.get('ì‚¬ì—…ì¥ëª…', ''),
                    'address_raw': address_info['address_raw'],
                    'address_nm': address_info['address_nm'],
                    'address_id': address_info['address_id'],
                    'lat': lat,
                    'lon': lon,
                    'phone': row.get('ì†Œì¬ì§€ì „í™”', ''),
                    'website': row.get('í™ˆí˜ì´ì§€', ''),
                    'operating_hours': None,  # ìš´ì˜ì‹œê°„ ì •ë³´ ì—†ìŒ
                    'is_24h': False,  # 24ì‹œê°„ ì •ë³´ ì—†ìŒ
                    'is_emergency': False,
                    'capacity': None,
                    'grade_level': None,
                    'facility_extra': {
                        'ì†Œì¬ì§€ë©´ì ': row.get('ì†Œì¬ì§€ë©´ì ', ''),
                        'ë„ë¡œëª…ì „ì²´ì£¼ì†Œ': row.get('ë„ë¡œëª…ì „ì²´ì£¼ì†Œ', '')
                    },
                    'data_source': 'localdata'
                }
                
                # ì‹¤ì‹œê°„ ëª¨ë“œì¼ ë•Œ ì¦‰ì‹œ ì €ì¥, ì•„ë‹ˆë©´ ë©”ëª¨ë¦¬ì— ëˆ„ì 
                if self.realtime_mode and self.output_dir:
                    self._save_success_data_immediately(facility, self.output_dir, 'convenience')
                    # ì§„í–‰ ìƒí™© ì €ì¥
                    progress_data = {
                        "file": str(file_path),
                        "row_index": idx,
                        "facility_type": "convenience",
                        "facility_name": row.get('ì‚¬ì—…ì¥ëª…', ''),
                        "processed_count": idx + 1,
                        "timestamp": pd.Timestamp.now().isoformat()
                    }
                    self._save_progress_immediately(progress_data, self.output_dir)
                else:
                    self.normalized_facilities.append(facility)
                
            except Exception as e:
                logger.error(f"í¸ì˜ì  ì •ê·œí™” ì˜¤ë¥˜: {row.get('ì‚¬ì—…ì¥ëª…', '')} - {e}")
        
        logger.info(f"í¸ì˜ì  ì •ê·œí™” ì™„ë£Œ: {len(df)}ê°œ")

    def _normalize_hospitals(self, file_path: Path, start_row: int = 0):
        """ë³‘ì› ì •ë³´ ì •ê·œí™”"""
        logger.info(f"ë³‘ì› ì •ë³´ ì •ê·œí™” ì‹œì‘: {file_path}")
        
        df = read_csv_with_auto_encoding(file_path, dtype=str)
        
        # ì‹œì‘ í–‰ ì„¤ì •
        if start_row > 0:
            logger.info(f"ë³‘ì› ë°ì´í„° ì •ê·œí™” ì¬ê°œ: {start_row}í–‰ë¶€í„° ì‹œì‘, ì´ {len(df)}ê°œ ì¤‘ {len(df) - start_row}ê°œ ë‚¨ìŒ")
            df = df.iloc[start_row:]
        else:
            logger.info(f"ë³‘ì› ë°ì´í„° ë¡œë“œ: {len(df)}ê°œ")
        
        for idx, row in df.iterrows():
            try:
                # ì£¼ì†Œ ì •ê·œí™”
                original_file = str(file_path)
                # ë„ë¡œëª…ì£¼ì†Œ ìš°ì„ , ì—†ìœ¼ë©´ ì†Œì¬ì§€ì „ì²´ì£¼ì†Œ ì‚¬ìš©
                road_addr = str(row.get('ë„ë¡œëª…ì „ì²´ì£¼ì†Œ', '') or '').strip()
                jibun_addr = str(row.get('ì†Œì¬ì§€ì „ì²´ì£¼ì†Œ', '') or '').strip()
                addr_for_norm = road_addr if road_addr else jibun_addr
                address_info = self._normalize_address(
                    address_raw=addr_for_norm,
                    facility_name=row.get('ì‚¬ì—…ì¥ëª…', ''),
                    facility_type='hospital',
                    original_file=original_file,
                    original_row_index=idx
                )
                
                # ì£¼ì†Œ ì •ê·œí™” ì‹¤íŒ¨ ì‹œ ê±´ë„ˆë›°ê¸° (ì´ë¯¸ failed_addresses.jsonlì— ì €ì¥ë¨)
                if not address_info.get('normalization_success', False):
                    continue
                
                # ì‹œì„¤ ì •ë³´ êµ¬ì„±
                facility = {
                    'facility_id': self._generate_facility_id('hospital'),
                    'cd': self._get_facility_cd('hospital'),
                    'name': row.get('ì‚¬ì—…ì¥ëª…', ''),
                    'address_raw': address_info['address_raw'],
                    'address_nm': address_info['address_nm'],
                    'address_id': address_info['address_id'],
                    'lat': address_info['lat'],
                    'lon': address_info['lon'],
                    'phone': row.get('ì†Œì¬ì§€ì „í™”', ''),
                    'website': row.get('í™ˆí˜ì´ì§€', ''),
                    'operating_hours': None,  # ìš´ì˜ì‹œê°„ ì •ë³´ ì—†ìŒ
                    'is_24h': False,  # 24ì‹œê°„ ì •ë³´ ì—†ìŒ
                    'is_emergency': 'ì‘ê¸‰' in str(row.get('ì§„ë£Œê³¼ëª©ë‚´ìš©ëª…', '')),
                    'facility_extra': {
                        'ì§„ë£Œê³¼ëª©': row.get('ì§„ë£Œê³¼ëª©ë‚´ìš©ëª…', ''),
                        'ì˜ë£Œê¸°ê´€ì¢…ë³„': row.get('ì˜ë£Œê¸°ê´€ì¢…ë³„ëª…', ''),
                        'ë³‘ìƒìˆ˜': row.get('ë³‘ìƒìˆ˜', ''),
                        'ì˜ë£Œì¸ìˆ˜': row.get('ì˜ë£Œì¸ìˆ˜', '')
                    },
                    'data_source': 'localdata'
                }
                
                # ì‹¤ì‹œê°„ ëª¨ë“œì¼ ë•Œ ì¦‰ì‹œ ì €ì¥, ì•„ë‹ˆë©´ ë©”ëª¨ë¦¬ì— ëˆ„ì 
                if self.realtime_mode and self.output_dir:
                    self._save_success_data_immediately(facility, self.output_dir, 'hospital')
                    # ì§„í–‰ ìƒí™© ì €ì¥
                    progress_data = {
                        "file": str(file_path),
                        "row_index": idx,
                        "facility_type": "hospital",
                        "facility_name": row.get('ì‚¬ì—…ì¥ëª…', ''),
                        "processed_count": idx + 1,
                        "timestamp": pd.Timestamp.now().isoformat()
                    }
                    self._save_progress_immediately(progress_data, self.output_dir)
                else:
                    self.normalized_facilities.append(facility)
                
            except Exception as e:
                logger.error(f"ë³‘ì› ì •ê·œí™” ì˜¤ë¥˜: {row.get('ì‚¬ì—…ì¥ëª…', '')} - {e}")
        
        logger.info(f"ë³‘ì› ì •ê·œí™” ì™„ë£Œ: {len(df)}ê°œ")

# ì˜ˆì‹œ ì‚¬ìš©ë²• (CLIì—ì„œ í˜¸ì¶œë  ë•Œ)
if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    # í”„ë¡œì íŠ¸ ë£¨íŠ¸ì—ì„œ ì‹¤í–‰í•œë‹¤ê³  ê°€ì •
    project_root = Path(__file__).resolve().parents[5]
    openseoul_data_path = project_root / "backend" / "data" / "raw" / "api"
    
    normalizer = InfraNormalizer(data_dir=openseoul_data_path)
    output_dir = project_root / "backend" / "data" / "normalized" / "infra"
    
    # ì‹¤ì‹œê°„ ëª¨ë“œë¡œ ì‹¤í–‰ (ê¸°ì¡´ ë°ì´í„° ìœ ì§€í•˜ë©´ì„œ ëˆ„ì )
    normalized_data = normalizer.normalize_openseoul_data(output_dir, realtime_mode=True)
    
    # JSONL íŒŒì¼ë“¤ì„ JSON íŒŒì¼ë¡œ ë³€í™˜
    normalizer.convert_jsonl_to_json(output_dir)
    
    print("\n--- ì •ê·œí™”ëœ ê³µê³µì‹œì„¤ ë°ì´í„° (ì¼ë¶€) ---")
    for i, facility in enumerate(normalized_data["public_facilities"][:5]):
        print(f"{i+1}. {facility['name']} (Category ID: {facility.get('category_id', 'N/A')})")

    print("\n--- ì •ê·œí™”ëœ ì§€í•˜ì² ì—­ ë°ì´í„° (ì¼ë¶€) ---")
    for i, station in enumerate(normalized_data["subway_stations"][:5]):
        print(f"{i+1}. {station['station_name']} (Line: {station['line_name']})")

#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
RAW í¬ë¡¤ë§ ë°ì´í„° ë¶„ì„ê¸°
"""

import json
import pandas as pd
from pathlib import Path
from typing import Dict, Any, List
from datetime import datetime

class RawDataAnalyzer:
    """RAW í¬ë¡¤ë§ ë°ì´í„° ë¶„ì„ê¸°"""
    
    def __init__(self, data_root: str = None):
        if data_root is None:
            # backend ë£¨íŠ¸ ê¸°ì¤€ìœ¼ë¡œ data/raw ê²½ë¡œ ì„¤ì • (ë¶„ì„í•  ë°ì´í„° ìœ„ì¹˜)
            # data_analyzer.py -> parsers -> services -> ingestion -> backend
            self.data_root = Path(__file__).parent.parent.parent.parent.parent / "data" / "raw"
        else:
            self.data_root = Path(data_root)
        self.platforms = []
        self.analysis_results = {}
    
    def analyze_all_platforms(self) -> Dict[str, Any]:
        """ëª¨ë“  í”Œë«í¼ì˜ RAW ë°ì´í„° ë¶„ì„"""
        print(f"ğŸ“ ë¶„ì„ ëŒ€ìƒ ë””ë ‰í† ë¦¬: {self.data_root}")
        
        if not self.data_root.exists():
            print("âŒ ë¶„ì„í•  ë°ì´í„° ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            return {}
        
        # í”Œë«í¼ë³„ ë””ë ‰í† ë¦¬ ì°¾ê¸°
        platform_dirs = [d for d in self.data_root.iterdir() if d.is_dir()]
        
        if not platform_dirs:
            print("âŒ ë¶„ì„í•  í”Œë«í¼ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return {}
        
        print(f"ğŸ” ë°œê²¬ëœ í”Œë«í¼: {[d.name for d in platform_dirs]}")
        
        for platform_dir in platform_dirs:
            platform_name = platform_dir.name
            print(f"\nğŸ“Š {platform_name} í”Œë«í¼ ë¶„ì„ ì¤‘...")
            
            # ë‚ ì§œë³„ ë””ë ‰í† ë¦¬ ì°¾ê¸°
            date_dirs = [d for d in platform_dir.iterdir() if d.is_dir()]
            
            if not date_dirs:
                print(f"  âš ï¸ {platform_name}ì— ë‚ ì§œë³„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                continue
            
            # ê°€ì¥ ìµœê·¼ ë‚ ì§œ ì„ íƒ
            latest_date_dir = max(date_dirs, key=lambda x: x.name)
            print(f"  ğŸ“… ìµœì‹  ë°ì´í„°: {latest_date_dir.name}")
            
            # CSV íŒŒì¼ ë¶„ì„
            csv_path = latest_date_dir / "raw.csv"
            if csv_path.exists():
                self.analyze_platform_csv(platform_name, csv_path, latest_date_dir)
            else:
                print(f"  âŒ {platform_name}ì— raw.csv íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        
        return self.analysis_results
    
    def analyze_platform_csv(self, platform_name: str, csv_path: Path, data_dir: Path):
        """í”Œë«í¼ë³„ CSV ë°ì´í„° ë¶„ì„"""
        try:
            df = pd.read_csv(csv_path)
            print(f"  ğŸ“ˆ ì´ {len(df)}ê°œ ì£¼íƒ ë°ì´í„°")
            
            # ê¸°ë³¸ í†µê³„
            stats = {
                'platform': platform_name,
                'total_houses': len(df),
                'crawl_date': data_dir.name,
                'data_quality': {}
            }
            
            # ë°ì´í„° í’ˆì§ˆ ë¶„ì„
            stats['data_quality'] = self.analyze_data_quality(df, data_dir)
            
            # ì£¼ì†Œ ë¶„ì„
            stats['address_analysis'] = self.analyze_addresses(df)
            
            # ì´ë¯¸ì§€ ë¶„ì„
            stats['image_analysis'] = self.analyze_images(df, data_dir)
            
            # í…ìŠ¤íŠ¸ ë¶„ì„
            stats['text_analysis'] = self.analyze_texts(df, data_dir)
            
            self.analysis_results[platform_name] = stats
            
            print(f"  âœ… {platform_name} ë¶„ì„ ì™„ë£Œ")
            
        except Exception as e:
            print(f"  âŒ {platform_name} ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
    
    def analyze_data_quality(self, df: pd.DataFrame, data_dir: Path) -> Dict[str, Any]:
        """ë°ì´í„° í’ˆì§ˆ ë¶„ì„"""
        quality = {
            'missing_data': {},
            'completeness_score': 0
        }
        
        # í•„ìˆ˜ í•„ë“œ ëˆ„ë½ ë¶„ì„
        essential_fields = ['house_name', 'address', 'detail_url']
        for field in essential_fields:
            if field in df.columns:
                missing_count = df[field].isna().sum()
                quality['missing_data'][field] = {
                    'count': int(missing_count),
                    'percentage': round(missing_count / len(df) * 100, 2)
                }
        
        # ì™„ì„±ë„ ì ìˆ˜ ê³„ì‚°
        total_fields = len(essential_fields)
        complete_fields = sum(1 for field in essential_fields 
                            if field in df.columns and df[field].isna().sum() == 0)
        quality['completeness_score'] = round(complete_fields / total_fields * 100, 2)
        
        return quality
    
    def analyze_addresses(self, df: pd.DataFrame) -> Dict[str, Any]:
        """ì£¼ì†Œ ì •ë³´ ë¶„ì„"""
        if 'address' not in df.columns:
            return {'error': 'ì£¼ì†Œ í•„ë“œê°€ ì—†ìŠµë‹ˆë‹¤.'}
        
        addresses = df['address'].dropna()
        
        analysis = {
            'total_addresses': len(addresses),
            'seoul_addresses': 0,
            'gu_distribution': {}
        }
        
        # ì„œìš¸ì‹œ ì£¼ì†Œ ë¶„ì„
        seoul_count = addresses.str.contains('ì„œìš¸', na=False).sum()
        analysis['seoul_addresses'] = int(seoul_count)
        
        # êµ¬ë³„ ë¶„í¬ ë¶„ì„
        for address in addresses:
            if 'ì„œìš¸' in str(address):
                # êµ¬ ì¶”ì¶œ (ê°„ë‹¨í•œ íŒ¨í„´ ë§¤ì¹­)
                for gu in ['ê°•ë‚¨êµ¬', 'ê°•ë™êµ¬', 'ê°•ë¶êµ¬', 'ê°•ì„œêµ¬', 'ê´€ì•…êµ¬', 'ê´‘ì§„êµ¬', 'êµ¬ë¡œêµ¬', 
                          'ê¸ˆì²œêµ¬', 'ë…¸ì›êµ¬', 'ë„ë´‰êµ¬', 'ë™ëŒ€ë¬¸êµ¬', 'ë™ì‘êµ¬', 'ë§ˆí¬êµ¬', 
                          'ì„œëŒ€ë¬¸êµ¬', 'ì„œì´ˆêµ¬', 'ì„±ë™êµ¬', 'ì„±ë¶êµ¬', 'ì†¡íŒŒêµ¬', 'ì–‘ì²œêµ¬', 
                          'ì˜ë“±í¬êµ¬', 'ìš©ì‚°êµ¬', 'ì€í‰êµ¬', 'ì¢…ë¡œêµ¬', 'ì¤‘êµ¬', 'ì¤‘ë‘êµ¬']:
                    if gu in str(address):
                        analysis['gu_distribution'][gu] = analysis['gu_distribution'].get(gu, 0) + 1
                        break
        
        return analysis
    
    def analyze_images(self, df: pd.DataFrame, data_dir: Path) -> Dict[str, Any]:
        """ì´ë¯¸ì§€ íŒŒì¼ ë¶„ì„"""
        if 'image_paths' not in df.columns:
            return {'error': 'ì´ë¯¸ì§€ ê²½ë¡œ í•„ë“œê°€ ì—†ìŠµë‹ˆë‹¤.'}
        
        image_dir = data_dir / 'images'
        total_images = 0
        image_sizes = []
        
        if image_dir.exists():
            # ì‹¤ì œ ì´ë¯¸ì§€ íŒŒì¼ ê°œìˆ˜
            image_files = list(image_dir.glob('*.jpg')) + list(image_dir.glob('*.png'))
            total_images = len(image_files)
            
            # ì´ë¯¸ì§€ í¬ê¸° ë¶„ì„
            for img_file in image_files[:10]:  # ìƒ˜í”Œ 10ê°œë§Œ
                try:
                    size = img_file.stat().st_size
                    image_sizes.append(size)
                except:
                    pass
        
        # CSVì—ì„œ ì´ë¯¸ì§€ ê²½ë¡œ ê°œìˆ˜
        image_path_counts = []
        for paths in df['image_paths'].dropna():
            if isinstance(paths, str) and paths.strip():
                count = len([p for p in paths.split(';') if p.strip()])
                image_path_counts.append(count)
        
        return {
            'total_image_files': total_images,
            'avg_images_per_house': round(sum(image_path_counts) / len(image_path_counts), 2) if image_path_counts else 0,
            'max_images_per_house': max(image_path_counts) if image_path_counts else 0,
            'avg_image_size_kb': round(sum(image_sizes) / len(image_sizes) / 1024, 2) if image_sizes else 0
        }
    
    def analyze_texts(self, df: pd.DataFrame, data_dir: Path) -> Dict[str, Any]:
        """í…ìŠ¤íŠ¸ íŒŒì¼ ë¶„ì„"""
        text_dir = data_dir / 'texts'
        
        if not text_dir.exists():
            return {'error': 'í…ìŠ¤íŠ¸ ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤.'}
        
        text_files = list(text_dir.glob('*.txt'))
        text_lengths = []
        
        for txt_file in text_files:
            try:
                content = txt_file.read_text(encoding='utf-8')
                text_lengths.append(len(content))
            except:
                pass
        
        return {
            'total_text_files': len(text_files),
            'avg_text_length': round(sum(text_lengths) / len(text_lengths), 2) if text_lengths else 0,
            'max_text_length': max(text_lengths) if text_lengths else 0
        }
    
    def print_summary(self):
        """ë¶„ì„ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        print("\n" + "="*60)
        print("ğŸ“Š RAW ë°ì´í„° ë¶„ì„ ê²°ê³¼ ìš”ì•½")
        print("="*60)
        
        total_houses = 0
        for platform, stats in self.analysis_results.items():
            print(f"\nğŸ  {platform.upper()}")
            print(f"  ğŸ“ˆ ì´ ì£¼íƒ ìˆ˜: {stats['total_houses']}ê°œ")
            print(f"  ğŸ“… í¬ë¡¤ë§ ë‚ ì§œ: {stats['crawl_date']}")
            
            # ë°ì´í„° í’ˆì§ˆ
            quality = stats.get('data_quality', {})
            print(f"  ğŸ¯ ì™„ì„±ë„ ì ìˆ˜: {quality.get('completeness_score', 0)}%")
            
            # ì£¼ì†Œ ë¶„ì„
            addr = stats.get('address_analysis', {})
            if 'seoul_addresses' in addr:
                print(f"  ğŸ™ï¸ ì„œìš¸ì‹œ ì£¼ì†Œ: {addr['seoul_addresses']}ê°œ")
            
            # ì´ë¯¸ì§€ ë¶„ì„
            img = stats.get('image_analysis', {})
            if 'total_image_files' in img:
                print(f"  ğŸ–¼ï¸ ì´ë¯¸ì§€ íŒŒì¼: {img['total_image_files']}ê°œ")
                print(f"  ğŸ“Š ì£¼íƒë‹¹ í‰ê·  ì´ë¯¸ì§€: {img.get('avg_images_per_house', 0)}ê°œ")
            
            # í…ìŠ¤íŠ¸ ë¶„ì„
            txt = stats.get('text_analysis', {})
            if 'total_text_files' in txt:
                print(f"  ğŸ“ í…ìŠ¤íŠ¸ íŒŒì¼: {txt['total_text_files']}ê°œ")
                print(f"  ğŸ“ í‰ê·  í…ìŠ¤íŠ¸ ê¸¸ì´: {txt.get('avg_text_length', 0)}ì")
            
            total_houses += stats['total_houses']
        
        print(f"\nğŸ‰ ì „ì²´ ìš”ì•½: {len(self.analysis_results)}ê°œ í”Œë«í¼, ì´ {total_houses}ê°œ ì£¼íƒ")
        print("="*60)
    
    def save_analysis_report(self):
        """ë¶„ì„ ë³´ê³ ì„œë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        report_path = Path("data_analysis_report.json")
        
        report = {
            'analysis_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'data_root': str(self.data_root),
            'platforms_analyzed': list(self.analysis_results.keys()),
            'total_houses': sum(stats['total_houses'] for stats in self.analysis_results.values()),
            'detailed_results': self.analysis_results
        }
        
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“„ ìƒì„¸ ë³´ê³ ì„œ ì €ì¥: {report_path.absolute()}")

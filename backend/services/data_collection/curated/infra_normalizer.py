#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
ì¸í”„ë¼ ë°ì´í„° ì •ê·œí™” ëª¨ë“ˆ
ì„œìš¸ ì—´ë¦°ë°ì´í„°ê´‘ì¥ API ë°ì´í„°ë¥¼ infra ìŠ¤í‚¤ë§ˆì— ë§ê²Œ ì •ê·œí™”
"""

import pandas as pd
from pathlib import Path
import os
import re
import requests
import time
from typing import List, Dict, Optional, Any, Tuple
import logging
from dotenv import load_dotenv
import chardet

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

from backend.services.data_collection.curated.address_api import normalize_address, AddressNormalizerError

logger = logging.getLogger(__name__)

# ----- ìƒˆë¡œìš´ API í´ë¼ì´ì–¸íŠ¸ í´ë˜ìŠ¤ë“¤ -----
class EmdCodeAPI:
    """ë²•ì •ë™ ì½”ë“œ API í´ë¼ì´ì–¸íŠ¸"""
    
    def __init__(self, api_key: str):
        self.api_key = api_key
        self.base_url = "http://apis.data.go.kr/1741000/StanReginCd/getStanReginCdList"
    
    def get_emd_code(self, address_nm: str) -> Optional[str]:
        """ì •ê·œí™”ëœ ì£¼ì†Œë¡œ ë²•ì •ë™ ì½”ë“œ ì¡°íšŒ (ì• 3ë‹¨ì–´ë§Œ ì‚¬ìš©)"""
        if not address_nm:
            return None
        
        # address_nmì—ì„œ ì• 3ë‹¨ì–´ë§Œ íŒŒì‹± (ì˜ˆ: "ì„œìš¸íŠ¹ë³„ì‹œ ì¢…ë¡œêµ¬ ìˆ­ì¸ë™" -> "ì„œìš¸íŠ¹ë³„ì‹œ ì¢…ë¡œêµ¬ ìˆ­ì¸ë™")
        address_parts = address_nm.strip().split()
        if len(address_parts) >= 3:
            search_address = ' '.join(address_parts[:3])  # ì• 3ë‹¨ì–´ë§Œ ì‚¬ìš©
        else:
            search_address = address_nm  # 3ë‹¨ì–´ ë¯¸ë§Œì´ë©´ ì „ì²´ ì‚¬ìš©
            
        logger.info(f"ë²•ì •ë™ ì½”ë“œ ì¡°íšŒ: '{address_nm}' -> '{search_address}'")
            
        try:
            params = {
                'ServiceKey': self.api_key,
                'type': 'xml',
                'pageNo': 1,
                'numOfRows': 3,
                'flag': 'Y',
                'locatadd_nm': search_address
            }
            
            response = requests.get(self.base_url, params=params, timeout=10)
            logger.info(f"ğŸ” ë²•ì •ë™ API ì‘ë‹µ ìƒíƒœ: {response.status_code}")
            logger.info(f"ğŸ“„ ë²•ì •ë™ API ì „ì²´ ì‘ë‹µ ë‚´ìš©:")
            logger.info(f"{response.text}")
            
            response.raise_for_status()
            
            # XML íŒŒì‹±í•˜ì—¬ region_cd ì¶”ì¶œ
            import xml.etree.ElementTree as ET
            root = ET.fromstring(response.content)
            
            # region_cd ì°¾ê¸°
            found_codes = []
            for item in root.findall('row'):
                region_cd = item.find('region_cd')
                if region_cd is not None and region_cd.text:
                    found_codes.append(region_cd.text)
                    logger.info(f"âœ… ë²•ì •ë™ ì½”ë“œ ë°œê²¬: {region_cd.text}")
            
            if found_codes:
                logger.info(f"ğŸ¯ ë²•ì •ë™ ì½”ë“œ ì¡°íšŒ ì„±ê³µ: {found_codes[0]} (ì´ {len(found_codes)}ê°œ ë°œê²¬)")
                return found_codes[0]
            
            logger.warning(f"âŒ ë²•ì •ë™ ì½”ë“œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            logger.warning(f"ğŸ“„ ì „ì²´ ì‘ë‹µ ë‚´ìš©: {response.text}")
            return None
            
        except Exception as e:
            logger.error(f"ë²•ì •ë™ ì½”ë“œ ì¡°íšŒ ì‹¤íŒ¨: {address_nm} - {e}")
            logger.error(f"ìš”ì²­ URL: {self.base_url}")
            logger.error(f"ìš”ì²­ íŒŒë¼ë¯¸í„°: {params}")
            return None

class ToLolaAPI:
    """ì¢Œí‘œ ë³€í™˜ API í´ë¼ì´ì–¸íŠ¸"""
    
    def __init__(self, api_key: str):
        self.api_key = api_key
        self.base_url = "https://api.vworld.kr/req/address"
    
    def get_coordinates(self, address: str, address_type: str = 'ROAD') -> Tuple[Optional[float], Optional[float]]:
        """ì£¼ì†Œë¥¼ ì¢Œí‘œë¡œ ë³€í™˜"""
        if not address:
            return None, None
            
        try:
            params = {
                'service': 'address',
                'request': 'getcoord',
                'version': '2.0',
                'crs': 'epsg:4326',
                'address': address,
                'refine': 'true',
                'simple': 'false',
                'format': 'xml',
                'type': address_type,
                'key': self.api_key
            }
            
            logger.info(f"ğŸŒ ì¢Œí‘œ ë³€í™˜ API ìš”ì²­: {address}")
            logger.info(f"ğŸ“‹ ì¢Œí‘œ ë³€í™˜ API ìš”ì²­ íŒŒë¼ë¯¸í„°: {params}")
            
            response = requests.get(self.base_url, params=params, timeout=10)
            logger.info(f"ğŸ” ì¢Œí‘œ ë³€í™˜ API ì‘ë‹µ ìƒíƒœ: {response.status_code}")
            logger.info(f"ğŸ“„ ì¢Œí‘œ ë³€í™˜ API ì „ì²´ ì‘ë‹µ ë‚´ìš©:")
            logger.info(f"{response.text}")
            
            response.raise_for_status()
            
            # XML íŒŒì‹±í•˜ì—¬ ì¢Œí‘œ ì¶”ì¶œ
            import xml.etree.ElementTree as ET
            root = ET.fromstring(response.content)
            
            # ì¢Œí‘œ ì°¾ê¸° (VWorld API ì‘ë‹µ êµ¬ì¡°ì— ë§ê²Œ ìˆ˜ì •)
            point = root.find('.//point')
            if point is not None:
                x = point.find('x')
                y = point.find('y')
                if x is not None and y is not None and x.text and y.text:
                    try:
                        lon = float(x.text)
                        lat = float(y.text)
                        logger.info(f"âœ… ì¢Œí‘œ ë³€í™˜ ì„±ê³µ: lat={lat}, lon={lon}")
                        return lat, lon
                    except ValueError as e:
                        logger.error(f"âŒ ì¢Œí‘œ ë³€í™˜ ì‹¤íŒ¨ (ê°’ ë³€í™˜ ì˜¤ë¥˜): {e}")
                        logger.error(f"ğŸ“„ ì›ë³¸ xê°’: {x.text}, yê°’: {y.text}")
                        return None, None
                else:
                    logger.warning(f"âš ï¸ ì¢Œí‘œ ê°’ì´ ë¹„ì–´ìˆìŒ: x={x.text if x is not None else None}, y={y.text if y is not None else None}")
            else:
                logger.warning(f"âŒ XMLì—ì„œ point íƒœê·¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            
            logger.warning(f"âŒ ì¢Œí‘œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            logger.warning(f"ğŸ“„ ì „ì²´ ì‘ë‹µ ë‚´ìš©: {response.text}")
            return None, None
            
        except Exception as e:
            logger.error(f"ì¢Œí‘œ ë³€í™˜ ì‹¤íŒ¨: {address} - {e}")
            logger.error(f"ìš”ì²­ URL: {self.base_url}")
            logger.error(f"ìš”ì²­ íŒŒë¼ë¯¸í„°: {params}")
            return None, None

def detect_address_type(address: str) -> str:
    """ì£¼ì†Œê°€ ë„ë¡œëª…ì£¼ì†Œì¸ì§€ ì§€ë²ˆì£¼ì†Œì¸ì§€ ê°ì§€"""
    if not address:
        return "unknown"
    
    # '~ê°€'ë¡œ ëë‚˜ëŠ” ë™ëª…ì€ ì§€ë²ˆì£¼ì†Œ (ì˜ˆ: ì¢…ë¡œ6ê°€, ì¢…ë¡œ5ê°€)
    if re.search(r'[ê°€-í£]+ê°€\s+\d+', address):
        return "jibun"
    
    # 'ë¡œ' ë˜ëŠ” 'ê¸¸'ë¡œ ëë‚˜ëŠ” ë‹¨ì–´ê°€ í¬í•¨ë˜ì–´ ìˆìœ¼ë©´ ë„ë¡œëª…ì£¼ì†Œ (ë‹¨, '~ê°€'ëŠ” ì œì™¸)
    if re.search(r'[ê°€-í£]+ë¡œ(?!\d+ê°€)\d*[ê°€-í£]*\s+\d+', address) or re.search(r'[ê°€-í£]+ê¸¸\d*[ê°€-í£]*\s+\d+', address):
        return "road"
    
    # 'ì§€í•˜'ê°€ í¬í•¨ëœ ë„ë¡œëª…ì£¼ì†Œ íŒ¨í„´
    if re.search(r'[ê°€-í£]+ë¡œ\s+ì§€í•˜\d+', address) or re.search(r'[ê°€-í£]+ê¸¸\s+ì§€í•˜\d+', address):
        return "road"
    else:
        return "jibun"

def detect_file_encoding(file_path: Path) -> str:
    """íŒŒì¼ì˜ ì¸ì½”ë”©ì„ ìë™ìœ¼ë¡œ ê°ì§€"""
    try:
        with open(file_path, 'rb') as f:
            raw_data = f.read()
            encoding_result = chardet.detect(raw_data)
            detected_encoding = encoding_result['encoding']
            confidence = encoding_result['confidence']
            
        logger.info(f"ğŸ” íŒŒì¼ ì¸ì½”ë”© ê°ì§€: {file_path.name} -> {detected_encoding} (ì‹ ë¢°ë„: {confidence:.2f})")
        
        # ì‹ ë¢°ë„ê°€ ë‚®ê±°ë‚˜ Noneì¸ ê²½ìš° UTF-8ë¡œ í´ë°±
        if not detected_encoding or confidence < 0.7:
            logger.warning(f"âš ï¸ ì¸ì½”ë”© ê°ì§€ ì‹ ë¢°ë„ ë‚®ìŒ, UTF-8ë¡œ í´ë°±: {file_path.name}")
            return 'utf-8'
            
        return detected_encoding
    except Exception as e:
        logger.error(f"âŒ ì¸ì½”ë”© ê°ì§€ ì‹¤íŒ¨: {file_path.name} - {e}")
        return 'utf-8'

def read_csv_with_auto_encoding(file_path: Path, **kwargs) -> pd.DataFrame:
    """ì¸ì½”ë”©ì„ ìë™ìœ¼ë¡œ ê°ì§€í•˜ì—¬ CSV íŒŒì¼ì„ ì½ê¸°"""
    detected_encoding = detect_file_encoding(file_path)
    
    try:
        # ê°ì§€ëœ ì¸ì½”ë”©ìœ¼ë¡œ íŒŒì¼ ì½ê¸°
        df = pd.read_csv(file_path, encoding=detected_encoding, **kwargs)
        logger.info(f"âœ… CSV íŒŒì¼ ì½ê¸° ì„±ê³µ: {file_path.name} (ì¸ì½”ë”©: {detected_encoding})")
        return df
    except UnicodeDecodeError:
        # ê°ì§€ëœ ì¸ì½”ë”©ìœ¼ë¡œ ì‹¤íŒ¨í•˜ë©´ UTF-8ë¡œ ì¬ì‹œë„
        logger.warning(f"âš ï¸ {detected_encoding} ì¸ì½”ë”© ì‹¤íŒ¨, UTF-8ë¡œ ì¬ì‹œë„: {file_path.name}")
        df = pd.read_csv(file_path, encoding='utf-8', **kwargs)
        return df
    except Exception as e:
        logger.error(f"âŒ CSV íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {file_path.name} - {e}")
        raise

def preprocess_subway_address(addr_raw: str) -> str:
    """ì§€í•˜ì² ì—­ ì£¼ì†Œ ì „ì²˜ë¦¬ - ì§€í•˜ì² ì—­ íŠ¹í™” ì „ì²˜ë¦¬"""
    if not addr_raw:
        return addr_raw
    
    import re
    
    # ì§€í•˜ì² ì—­ ê´€ë ¨ ì „ì²˜ë¦¬
    # ì—­ëª…ê³¼ ê´„í˜¸ ì œê±° (ì˜ˆ: "ì‹ ì„¤ë™ì—­(2í˜¸ì„ )" â†’ "")
    addr_raw = re.sub(r'\s+[ê°€-í£]+ì—­\([^)]*\)', '', addr_raw).strip()
    
    # ë‚¨ì€ ê´„í˜¸ ì œê±° (ì˜ˆ: "(2í˜¸ì„ )" â†’ "")
    addr_raw = re.sub(r'\s*\([^)]*\)', '', addr_raw).strip()
    
    return addr_raw

def preprocess_park_address(addr_raw: str) -> str:
    """ê³µì› ì£¼ì†Œ ì „ì²˜ë¦¬ - ê³µì› íŠ¹í™” ì „ì²˜ë¦¬"""
    if not addr_raw:
        return addr_raw
    
    import re
    
    # ê³µì› íŠ¹í™” ì „ì²˜ë¦¬
    # ê´„í˜¸ ì•ˆì˜ ë™ ì •ë³´ ì œê±° (ì˜ˆ: "(ì˜ˆì¥ë™)" â†’ "")
    addr_raw = re.sub(r'\s*\([^)]*ë™[^)]*\)', '', addr_raw).strip()
    
    # ê³µì›ëª… ì œê±° (ì˜ˆ: "ê¸¸ë™ìƒíƒœê³µì›" â†’ "")
    addr_raw = re.sub(r'\s+[ê°€-í£]*ê³µì›[ê°€-í£]*', '', addr_raw).strip()
    
    # ë¶ˆí•„ìš”í•œ ê³µë°± ì •ë¦¬
    addr_raw = re.sub(r'\s+', ' ', addr_raw).strip()
    
    return addr_raw

def preprocess_address(addr_raw: str) -> str:
    """ì£¼ì†Œ ì „ì²˜ë¦¬ - JUSO API ë§¤ì¹­ë¥  í–¥ìƒì„ ìœ„í•œ ì •ë¦¬"""
    if not addr_raw:
        return addr_raw
    
    import re
    
    # 0. ì¤‘ë³µ ì£¼ì†Œ ì œê±° (ê°€ì¥ ë¨¼ì € ì ìš©)
    # ì˜ˆ: "ì„œìš¸íŠ¹ë³„ì‹œ ì–‘ì²œêµ¬ ëª©ë™ì¤‘ì•™ë³¸ë¡œ20ê¸¸33 (ëª©ë™, ëª©ë™ê³¨ë“ ë¹Œ) ì„œìš¸íŠ¹ë³„ì‹œ ì–‘ì²œêµ¬ ëª©ë™ì¤‘ì•™ë³¸ë¡œ20ê¸¸33 (ëª©ë™, ëª©ë™ê³¨ë“ ë¹Œ)" 
    # -> "ì„œìš¸íŠ¹ë³„ì‹œ ì–‘ì²œêµ¬ ëª©ë™ì¤‘ì•™ë³¸ë¡œ20ê¸¸33 (ëª©ë™, ëª©ë™ê³¨ë“ ë¹Œ)"
    # ë” ì •í™•í•œ ì¤‘ë³µ ì œê±°: ìµœì†Œ 5ê¸€ì ì´ìƒì˜ ì˜ë¯¸ìˆëŠ” ì¤‘ë³µë§Œ ì œê±°
    addr_raw = re.sub(r'(.{5,}?)\s+\1\s*.*$', r'\1', addr_raw)  # ê³µë°± ìˆëŠ” ì¤‘ë³µ (5ê¸€ì ì´ìƒ)
    # ê³µë°± ì—†ëŠ” ì¤‘ë³µì€ ë” ì—„ê²©í•˜ê²Œ: ìµœì†Œ 10ê¸€ì ì´ìƒ
    addr_raw = re.sub(r'(.{10,}?)\1.*$', r'\1', addr_raw)  # ê³µë°± ì—†ëŠ” ì¤‘ë³µ (10ê¸€ì ì´ìƒ)
    
    # 0-1. ì¤‘ë³µ ì£¼ì†Œ ì œê±° (ë” ê°„ë‹¨í•˜ê³  íš¨ê³¼ì ì¸ ë°©ë²•)
    # ì˜ˆ: "ì„œìš¸íŠ¹ë³„ì‹œ ì–‘ì²œêµ¬ ëª©ë™ì¤‘ì•™ë³¸ë¡œ20ê¸¸33 (ëª©ë™, ëª©ë™ê³¨ë“ ë¹Œ) ì„œìš¸íŠ¹ë³„ì‹œ ì–‘ì²œêµ¬ ëª©ë™ì¤‘ì•™ë³¸ë¡œ20ê¸¸33 (ëª©ë™, ëª©ë™ê³¨ë“ ë¹Œ)"
    # -> "ì„œìš¸íŠ¹ë³„ì‹œ ì–‘ì²œêµ¬ ëª©ë™ì¤‘ì•™ë³¸ë¡œ20ê¸¸33 (ëª©ë™, ëª©ë™ê³¨ë“ ë¹Œ)"
    
    # ì„œìš¸íŠ¹ë³„ì‹œë¡œ ì‹œì‘í•˜ëŠ” ì£¼ì†Œì˜ ì¤‘ë³µ ì œê±°
    addr_raw = re.sub(r'(ì„œìš¸íŠ¹ë³„ì‹œ\s+[^ì„œìš¸íŠ¹ë³„ì‹œ]*?)\s+ì„œìš¸íŠ¹ë³„ì‹œ\s+.*$', r'\1', addr_raw)
    
    # ì„œìš¸ë¡œ ì‹œì‘í•˜ëŠ” ì£¼ì†Œì˜ ì¤‘ë³µ ì œê±°
    addr_raw = re.sub(r'(ì„œìš¸\s+[^ì„œìš¸]*?)\s+ì„œìš¸\s+.*$', r'\1', addr_raw)
    
    # 1. ê±´ë¬¼ë²ˆí˜¸ê°€ ë¹„ì •ìƒì ìœ¼ë¡œ í° ê²½ìš° ìˆ˜ì • (ì˜ˆ: 217 912 -> 217)
    # ë„ë¡œëª… + ê±´ë¬¼ë²ˆí˜¸ íŒ¨í„´ ì°¾ê¸°
    road_pattern = r'([ê°€-í£]+ë¡œ\d*[ê°€-í£]*)\s+(\d+)\s+(\d{3,})'
    match = re.search(road_pattern, addr_raw)
    if match:
        road_name = match.group(1)
        building_num = match.group(2)
        large_num = match.group(3)
        # í° ë²ˆí˜¸ë¥¼ ì œê±°í•˜ê³  ë„ë¡œëª… + ê±´ë¬¼ë²ˆí˜¸ë§Œ ì‚¬ìš©
        addr_raw = re.sub(road_pattern, f'{road_name} {building_num}', addr_raw)
        logger.info(f"ì£¼ì†Œ ì „ì²˜ë¦¬: ê±´ë¬¼ë²ˆí˜¸ ì •ë¦¬ -> {addr_raw}")
    
    # 2. ì§€ë²ˆ ì£¼ì†ŒëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€ (JUSO APIê°€ ì§€ë²ˆ ì£¼ì†Œë„ ì²˜ë¦¬ ê°€ëŠ¥)
    # ë‹¤ë§Œ ë¶ˆí•„ìš”í•œ ì •ë³´ë§Œ ì œê±°
    
    # 3. ë¶ˆí•„ìš”í•œ ì¸µìˆ˜ ì •ë³´ ì œê±°
    addr_raw = re.sub(r'\s+\d+\s*ì¸µ.*$', '', addr_raw)
    addr_raw = re.sub(r'\s+ì „ì¸µ.*$', '', addr_raw)
    addr_raw = re.sub(r'\s+\d+\s*~\s*\d+\s*ì¸µ.*$', '', addr_raw)
    addr_raw = re.sub(r'\s+\d+,\d+.*ì¸µ.*$', '', addr_raw)  # 2,3,4,5ì¸µ íŒ¨í„´
    addr_raw = re.sub(r'\s+\d+[Ff].*$', '', addr_raw)  # 1F, 2f íŒ¨í„´
    # ì¶”ê°€ ì¸µìˆ˜ íŒ¨í„´ë“¤
    addr_raw = re.sub(r'\s+ì§€ìƒ\d+\s*~\s*\d+\s*ì¸µ.*$', '', addr_raw)  # ì§€ìƒ1ì¸µ~3ì¸µ
    addr_raw = re.sub(r'\s+ì§€í•˜\d+\s*~\s*\d+\s*ì¸µ.*$', '', addr_raw)  # ì§€í•˜1ì¸µ~3ì¸µ
    addr_raw = re.sub(r'\s+ì§€ìƒ\d+\s*ì¸µ.*$', '', addr_raw)  # ì§€ìƒ1ì¸µ
    addr_raw = re.sub(r'\s+ì§€í•˜\d+\s*ì¸µ.*$', '', addr_raw)  # ì§€í•˜1ì¸µ
    
    # 4. ê±´ë¬¼ëª… ì œê±° (ê´„í˜¸ ì•ˆì˜ ë‚´ìš©)
    addr_raw = re.sub(r'\s*\([^)]*\)\s*', ' ', addr_raw)
    addr_raw = re.sub(r'\s+[ê°€-í£]+ìƒí™œ.*$', '', addr_raw)
    addr_raw = re.sub(r'\s+[ê°€-í£]+ë¹Œ.*$', '', addr_raw)
    # ì¶”ê°€ ê±´ë¬¼ëª… íŒ¨í„´ë“¤
    addr_raw = re.sub(r'\s+ì• ìŠ¤íŠ¸ë¦¬\d+.*$', '', addr_raw)  # ì• ìŠ¤íŠ¸ë¦¬23
    addr_raw = re.sub(r'\s+ì‚¬ëŠ”ìë¦¬.*$', '', addr_raw)  # ì‚¬ëŠ”ìë¦¬
    addr_raw = re.sub(r'\s+ì¨ë“œí”Œë ˆì´ìŠ¤.*$', '', addr_raw)  # ì¨ë“œí”Œë ˆì´ìŠ¤ í™ì€7
    addr_raw = re.sub(r'\s+ì½”ì´ë…¸ë‹ˆì•„.*$', '', addr_raw)  # ì½”ì´ë…¸ë‹ˆì•„ìŠ¤í…Œì´
    addr_raw = re.sub(r'\s+ë§‘ì€êµ¬ë¦„ì§‘.*$', '', addr_raw)  # ë§‘ì€êµ¬ë¦„ì§‘
    addr_raw = re.sub(r'\s+ë„ˆë‚˜ë“¤ì´.*$', '', addr_raw)  # ë„ˆë‚˜ë“¤ì´
    addr_raw = re.sub(r'\s+í™”ê³¡ë™.*$', '', addr_raw)  # í™”ê³¡ë™ ê³µë™ì²´ì£¼íƒ
    # ì¶”ê°€ ê±´ë¬¼ëª… íŒ¨í„´ë“¤ (sohouse ì‹¤íŒ¨ ì¼€ì´ìŠ¤)
    addr_raw = re.sub(r'\s+ë…¹ìƒ‰ì¹œêµ¬ë“¤.*$', '', addr_raw)  # ë…¹ìƒ‰ì¹œêµ¬ë“¤ ëŒ€ì¡°, ë…¹ìƒ‰ì¹œêµ¬ë“¤ ì°½ì²œ
    addr_raw = re.sub(r'\s+í•¨ê»˜ì£¼íƒ.*$', '', addr_raw)  # í•¨ê»˜ì£¼íƒ6í˜¸, í•¨ê»˜ì£¼íƒ5í˜¸
    addr_raw = re.sub(r'\s+ì£¼ìƒë³µí•©ê±´ë¬¼.*$', '', addr_raw)  # ì£¼ìƒë³µí•©ê±´ë¬¼
    
    # 5. ì§€ë²ˆì£¼ì†Œ ì •ë¦¬ (~ë™ ìˆ«ì ë˜ëŠ” ~ë™ ìˆ«ì-ìˆ«ì ë’¤ì˜ ëª¨ë“  ë¬¸ì ì œê±°)
    # ì˜ˆ: "ì„œìš¸ ì¢…ë¡œêµ¬ ë™ìˆ­ë™ 192-6 1" -> "ì„œìš¸ ì¢…ë¡œêµ¬ ë™ìˆ­ë™ 192-6"
    # ì˜ˆ: "ì„œìš¸ ê´‘ì§„êµ¬ ëŠ¥ë™ 25 ì„ í™”ì˜ˆìˆ ì¤‘ê³ ë“±í•™êµ" -> "ì„œìš¸ ê´‘ì§„êµ¬ ëŠ¥ë™ 25"
    addr_raw = re.sub(r'([ê°€-í£]+ë™\s+\d+[-\d]*)\s+.*$', r'\1', addr_raw)
    
    # 6. ë„ë¡œëª…+ê±´ë¬¼ë²ˆí˜¸ ë¶„ë¦¬ (ì¼ë°˜í™”ëœ íŒ¨í„´)
    # 6-1. *ë¡œ*ê¸¸+ìˆ«ì -> *ë¡œ*ê¸¸ ìˆ«ì (ì˜ˆ: ë¶ì•…ì‚°ë¡œ3ê¸¸44 -> ë¶ì•…ì‚°ë¡œ3ê¸¸ 44)
    # ë” ì •í™•í•œ íŒ¨í„´ìœ¼ë¡œ ìˆ˜ì •: ~ë¡œë¡œ ëë‚˜ê³  ~ê¸¸ë¡œ ëë‚˜ëŠ” íŒ¨í„´
    addr_raw = re.sub(r'([ê°€-í£]+ë¡œ\d*[ê°€-í£]*ê¸¸)(\d+[-\d]*)', r'\1 \2', addr_raw)
    
    # 6-2. *ë¡œ+ìˆ«ì -> *ë¡œ ìˆ«ì (ê¸¸ì´ ì—†ëŠ” ê²½ìš°ë§Œ)
    # ë‹¨, ~ê¸¸ì´ í¬í•¨ëœ ê²½ìš°ëŠ” ì œì™¸ (ì—°í¬ë¡œ18ê¸¸ -> ì—°í¬ë¡œ18ê¸¸ ìœ ì§€)
    if 'ê¸¸' not in addr_raw:
        addr_raw = re.sub(r'([ê°€-í£]+ë¡œ)(\d+[-\d]*)', r'\1 \2', addr_raw)
    
    # 7. ë„ë¡œëª…ì£¼ì†Œ ì •ë¦¬ (~ê¸¸ìˆ«ì(-ìˆ«ì) ë˜ëŠ” ~ê¸¸ ìˆ«ì(-ìˆ«ì) ë’¤ì˜ ëª¨ë“  ë¬¸ì ì œê±°)
    # ì˜ˆ: "ì„œìš¸ ì„œëŒ€ë¬¸êµ¬ ì—°í¬ë¡œ18ê¸¸ 36 ì• ìŠ¤íŠ¸ë¦¬23" -> "ì„œìš¸ ì„œëŒ€ë¬¸êµ¬ ì—°í¬ë¡œ18ê¸¸ 36"
    # ì£¼ì˜: "ë‹¤ê¸¸", "ë‚˜ê¸¸" ë“±ì€ ì‹¤ì œ ë„ë¡œëª…ì´ë¯€ë¡œ ê±´ë¬¼ë²ˆí˜¸ë¥¼ ì œê±°í•˜ë©´ ì•ˆë¨
    # ~ê¸¸ìˆ«ì(-ìˆ«ì) íŒ¨í„´ ë’¤ì˜ ëª¨ë“  ë¬¸ì ì œê±° (ë‹¨, "ë‹¤ê¸¸", "ë‚˜ê¸¸" ë“±ì€ ì œì™¸)
    # "ë‹¤ê¸¸", "ë‚˜ê¸¸" ë“±ì´ í¬í•¨ëœ ê²½ìš°ëŠ” ê±´ë¬¼ë²ˆí˜¸ë¥¼ ì œê±°í•˜ì§€ ì•ŠìŒ
    if not re.search(r'[ë‹¤ë‚˜]ê¸¸', addr_raw):
        addr_raw = re.sub(r'([ê°€-í£]+ê¸¸\d+[-\d]*)\s+.*$', r'\1', addr_raw)
    # ~ê¸¸ ìˆ«ì(-ìˆ«ì) íŒ¨í„´ ë’¤ì˜ ëª¨ë“  ë¬¸ì ì œê±° (ë‹¨, "ë‹¤ê¸¸", "ë‚˜ê¸¸" ë“±ì€ ì œì™¸)
    if not re.search(r'[ë‹¤ë‚˜]ê¸¸', addr_raw):
        addr_raw = re.sub(r'([ê°€-í£]+ê¸¸\s+\d+[-\d]*)\s+.*$', r'\1', addr_raw)
    # ~ë¡œ ìˆ«ì(-ìˆ«ì) íŒ¨í„´ ë’¤ì˜ ëª¨ë“  ë¬¸ì ì œê±° (ê¸¸ì´ ì—†ëŠ” ê²½ìš°)
    addr_raw = re.sub(r'([ê°€-í£]+ë¡œ\s+\d+[-\d]*)\s+.*$', r'\1', addr_raw)
    
    # 8. ~ë¡œ ì£¼ì†Œì—ì„œ ~ë¡œ ì œê±° (ê¸¸ì´ ì—†ëŠ” ê²½ìš°)
    # ì˜ˆ: "ì„œìš¸íŠ¹ë³„ì‹œ ë§ˆí¬êµ¬ ì™€ìš°ì‚°ë¡œ ìƒìˆ˜ë™ 321-6" -> "ì„œìš¸íŠ¹ë³„ì‹œ ë§ˆí¬êµ¬ ìƒìˆ˜ë™ 321-6"
    # ~ê¸¸ì´ ì—†ëŠ” ì£¼ì†Œì—ì„œë§Œ ~ë¡œ ì œê±°
    if 'ê¸¸' not in addr_raw and 'ë¡œ' in addr_raw:
        addr_raw = re.sub(r'([ê°€-í£]+êµ¬)\s+([ê°€-í£]+ë¡œ)\s+([ê°€-í£]+ë™)', r'\1 \3', addr_raw)
    
    # 9. ì§€ë²ˆ ì£¼ì†Œ ë’¤ ì¶”ê°€ ì •ë³´ ì œê±° (ì¦ì‚°ë™ 202-25 ë“±)
    # ì˜ˆ: "ì¦ì‚°ë¡œ9ê¸¸ 26-21 ì¦ì‚°ë™ 202-25" -> "ì¦ì‚°ë¡œ9ê¸¸ 26-21"
    addr_raw = re.sub(r'([ê°€-í£]+ë¡œ\d*[ê°€-í£]*ê¸¸\s+\d+[-\d]*)\s+[ê°€-í£]+ë™\s+\d+[-\d]*.*$', r'\1', addr_raw)
    # ì¶”ê°€ ì§€ë²ˆ ì£¼ì†Œ ì œê±° (ì„œìš¸ì‹œ ê¸ˆì²œêµ¬ ë…ì‚°ë™ 964-42 ë“±)
    addr_raw = re.sub(r'([ê°€-í£]+ë¡œ\d*[ê°€-í£]*ê¸¸\s+\d+[-\d]*)\s+ì„œìš¸[ì‹œíŠ¹ë³„ì‹œ]*\s+[ê°€-í£]+êµ¬\s+[ê°€-í£]+ë™\s+\d+[-\d]*.*$', r'\1', addr_raw)
    
    # 10. ë§ˆì¹¨í‘œ ë° ë¶ˆí•„ìš”í•œ ë¬¸ì ì œê±°
    addr_raw = re.sub(r'\.+$', '', addr_raw)  # ëì˜ ë§ˆì¹¨í‘œ ì œê±°
    addr_raw = re.sub(r'\s+$', '', addr_raw)  # ëì˜ ê³µë°± ì œê±°
    
    # 12. ì•„íŒŒíŠ¸/ê±´ë¬¼ ë‚´ë¶€ ì •ë³´ ì œê±° (infra ì‹¤íŒ¨ ì¼€ì´ìŠ¤ ëŒ€ì‘)
    # ë™/í˜¸ìˆ˜ ì •ë³´ ì œê±° (ì˜ˆ: 101ë™ 102í˜¸, 13ë™ 204í˜¸)
    addr_raw = re.sub(r'\s+\d+ë™\s+\d+í˜¸.*$', '', addr_raw)
    addr_raw = re.sub(r'\s+\d+ë™\s+\d+,\s*\d+í˜¸.*$', '', addr_raw)
    # ì¸µìˆ˜ ì •ë³´ ì œê±° (ì˜ˆ: 4ì¸µ, 3ì¸µ, 1ì¸µ)
    addr_raw = re.sub(r'\s+\d+ì¸µ.*$', '', addr_raw)
    # ë‹¨ì§€/ì•„íŒŒíŠ¸ ë‚´ë¶€ ì •ë³´ ì œê±°
    addr_raw = re.sub(r'\s+ë‹¨ì§€\s*ë‚´.*$', '', addr_raw)
    addr_raw = re.sub(r'\s+ê´€ë¦¬ë™.*$', '', addr_raw)
    addr_raw = re.sub(r'\s+ê´€ë¦¬ì‹¤.*$', '', addr_raw)
    addr_raw = re.sub(r'\s+í‚¤ì¦ˆì„¼í„°\d+ì¸µ.*$', '', addr_raw)
    addr_raw = re.sub(r'\s+í‚¤ì¦ˆí´ëŸ½.*$', '', addr_raw)
    # ìƒì„¸ ìœ„ì¹˜ ì •ë³´ ì œê±° (ì˜ˆ: B113í˜¸, 101-2í˜¸, 206-2í˜¸)
    addr_raw = re.sub(r'\s+[A-Z]\d+í˜¸.*$', '', addr_raw)  # B113í˜¸, A101í˜¸ ë“±
    addr_raw = re.sub(r'\s+\d+-\d+í˜¸.*$', '', addr_raw)  # 101-2í˜¸, 206-2í˜¸ ë“±
    addr_raw = re.sub(r'\s+\d+,\d+í˜¸.*$', '', addr_raw)  # 105,106í˜¸ ë“±
    # ì§€í•˜/ì§€ìƒ ì •ë³´ ì œê±°
    addr_raw = re.sub(r'\s+ì§€í•˜\d+.*$', '', addr_raw)
    addr_raw = re.sub(r'\s+ì§€ìƒ\s*\d+.*$', '', addr_raw)
    # B1ì¸µ, B2ì¸µ ë“± ì œê±°
    addr_raw = re.sub(r'\s+B\d+ì¸µ.*$', '', addr_raw)
    # ìƒê°€/ë¹Œë”© ë‚´ë¶€ ì •ë³´ ì œê±°
    addr_raw = re.sub(r'\s+ìƒê°€ë™.*$', '', addr_raw)
    addr_raw = re.sub(r'\s+ë¹Œë”©.*$', '', addr_raw)
    addr_raw = re.sub(r'\s+íƒ€ì›Œ.*$', '', addr_raw)
    # ì£¼ë¯¼ê³µë™ì‹œì„¤, ì¢…í•©ìƒê°€ ë“± ì œê±°
    addr_raw = re.sub(r'\s+ì£¼ë¯¼ê³µë™ì‹œì„¤.*$', '', addr_raw)
    addr_raw = re.sub(r'\s+ì¢…í•©ìƒê°€.*$', '', addr_raw)
    # ì„¼í„°/ê´€ë ¨ ì •ë³´ ì œê±°
    addr_raw = re.sub(r'\s+ê³ ê°ì„¼í„°\d+ì¸µ.*$', '', addr_raw)
    addr_raw = re.sub(r'\s+ê·¼ë¡œë³µì§€ê´€\d+ì¸µ.*$', '', addr_raw)
    addr_raw = re.sub(r'\s+êµìœ¡ê´€.*$', '', addr_raw)
    addr_raw = re.sub(r'\s+ë¬¸í™”ê±´ê°•ì„¼í„°.*$', '', addr_raw)
    addr_raw = re.sub(r'\s+ì—´ë¦°ë¬¸í™”ì„¼í„°.*$', '', addr_raw)
    
    # 13. ë³µì¡í•œ ê±´ë¬¼ëª… íŒ¨í„´ ì œê±° (infra íŠ¹í™”)
    # ì•„íŒŒíŠ¸ ë‹¨ì§€ëª… ì œê±°
    addr_raw = re.sub(r'\s+[ê°€-í£]+ì•„íŒŒíŠ¸.*$', '', addr_raw)
    addr_raw = re.sub(r'\s+[ê°€-í£]+íƒ€ìš´.*$', '', addr_raw)
    addr_raw = re.sub(r'\s+[ê°€-í£]+ì‹œí‹°.*$', '', addr_raw)
    addr_raw = re.sub(r'\s+[ê°€-í£]+íŒŒí¬.*$', '', addr_raw)
    addr_raw = re.sub(r'\s+[ê°€-í£]+íìŠ¤.*$', '', addr_raw)
    addr_raw = re.sub(r'\s+[ê°€-í£]+ìœ„ë¸Œ.*$', '', addr_raw)
    addr_raw = re.sub(r'\s+[ê°€-í£]+ìì´.*$', '', addr_raw)
    addr_raw = re.sub(r'\s+[ê°€-í£]+í‘¸ë¥´ì§€ì˜¤.*$', '', addr_raw)
    addr_raw = re.sub(r'\s+[ê°€-í£]+í”„ë ˆìŠ¤í‹°ì§€.*$', '', addr_raw)
    addr_raw = re.sub(r'\s+[ê°€-í£]+ë‰´íƒ€ìš´.*$', '', addr_raw)
    addr_raw = re.sub(r'\s+[ê°€-í£]+ë‰´íƒ€ì›Œ.*$', '', addr_raw)
    addr_raw = re.sub(r'\s+[ê°€-í£]+ë¸Œì´ì›.*$', '', addr_raw)
    addr_raw = re.sub(r'\s+[ê°€-í£]+ì—ì½”.*$', '', addr_raw)
    addr_raw = re.sub(r'\s+[ê°€-í£]+ë¦¬ë²„.*$', '', addr_raw)
    addr_raw = re.sub(r'\s+[ê°€-í£]+ë”í”„ë ˆí‹°ì›€.*$', '', addr_raw)
    addr_raw = re.sub(r'\s+[ê°€-í£]+ìŠ¤ìœ„ì²¸.*$', '', addr_raw)
    addr_raw = re.sub(r'\s+[ê°€-í£]+ë¦¬ì²¸ì‹œì•„.*$', '', addr_raw)
    addr_raw = re.sub(r'\s+[ê°€-í£]+í”Œë ˆì´ìŠ¤.*$', '', addr_raw)
    addr_raw = re.sub(r'\s+[ê°€-í£]+ì„¼íŠ¸ëŸ´.*$', '', addr_raw)
    addr_raw = re.sub(r'\s+[ê°€-í£]+ê·¸ë¼ì‹œì›€.*$', '', addr_raw)
    addr_raw = re.sub(r'\s+[ê°€-í£]+ì•„ì´íŒŒí¬.*$', '', addr_raw)
    addr_raw = re.sub(r'\s+[ê°€-í£]+ì•„ì¹´ë°ë¯¸.*$', '', addr_raw)
    addr_raw = re.sub(r'\s+[ê°€-í£]+ë¸Œì´ì›.*$', '', addr_raw)
    addr_raw = re.sub(r'\s+[ê°€-í£]+ì—”ì§€ë‹ˆì–´ë§.*$', '', addr_raw)
    addr_raw = re.sub(r'\s+[ê°€-í£]+ë©”ë””ì»¬.*$', '', addr_raw)
    addr_raw = re.sub(r'\s+[ê°€-í£]+í™ˆìºìŠ¤íŠ¸.*$', '', addr_raw)
    addr_raw = re.sub(r'\s+[ê°€-í£]+ìŠ¤ìœ„íŠ¸.*$', '', addr_raw)
    
    # 14. ê´„í˜¸ ì•ˆì˜ ìƒì„¸ ì •ë³´ ì œê±° (ì˜ˆ: (ë§ìš°ë³¸ë™), (ì •ë¦‰ë™))
    addr_raw = re.sub(r'\s*\([^)]*ë™[^)]*\)\s*', ' ', addr_raw)
    addr_raw = re.sub(r'\s*\([^)]*ê°€[^)]*\)\s*', ' ', addr_raw)
    
    # 15. ê³µë°± ì •ë¦¬
    addr_raw = re.sub(r'\s+', ' ', addr_raw).strip()



    # --- ğŸ”¥ [ì¶”ê°€: ì£¼íƒ ì •ê·œí™” ì „ìš© ê·œì¹™] ---
    # 1. ì‰¼í‘œ(,) ë’¤ ìƒì„¸ì •ë³´ ì œê±° â†’ "ì„œìš¸ ì„±ë¶êµ¬ ë™ì†Œë¬¸ë¡œ 305, ì§€ì¸µ" â†’ "ì„œìš¸ ì„±ë¶êµ¬ ë™ì†Œë¬¸ë¡œ 305"
    addr_raw = re.sub(r',\s*[^,]+$', '', addr_raw)

    # 2. 'ì§€ìƒ', 'ì§€í•˜', 'ì¸µ', 'í˜¸' ë“± ì„¸ë¶€ìœ„ì¹˜ ì œê±°
    addr_raw = re.sub(r'\s*ì§€[ìƒí•˜]\d*ì¸µ?', '', addr_raw)
    addr_raw = re.sub(r'\s*\d+ì¸µ', '', addr_raw)
    addr_raw = re.sub(r'\s*\d+í˜¸', '', addr_raw)
    addr_raw = re.sub(r'\s*\d+ë™', '', addr_raw)

    # 3. ê´„í˜¸ ì•ˆì— ìˆëŠ” ë™/ê°€/ê±´ë¬¼ëª… ì œê±°
    addr_raw = re.sub(r'\([^)]*(ë™|ê°€|ì•„íŒŒíŠ¸|ë¹Œë¼|ë‹¨ì§€|ìƒê°€|íƒ€ì›Œ)[^)]*\)', '', addr_raw)

    # 4. "ì•", "ë‚´", "ê´€ë¦¬ë™", "ê´€ë¦¬ì‹¤" ë“± ë¶ˆí•„ìš”í•œ ë‹¨ì–´ ì œê±°
    addr_raw = re.sub(r'\s*(ì•|ë‚´|ê´€ë¦¬ë™|ê´€ë¦¬ì‹¤|ì£¼ë¯¼ì„¼í„°|í‚¤ì¦ˆì„¼í„°\d*ì¸µ?|ë¬¸í™”ì„¼í„°).*$', '', addr_raw)

    # 5. ì£¼ì†Œ ë ë¶ˆí•„ìš”í•œ ë¬¸ì ì œê±°
    addr_raw = re.sub(r'[-,\s]+$', '', addr_raw)

    # 6. ë„ë¡œëª… + ë„ë¡œëª…ì£¼ì†Œë¶€ë²ˆê¹Œì§€ë§Œ ì¸ì‹í•˜ê³  ê·¸ ë’¤ëŠ” ëª¨ë‘ ì œê±°
    # ~ê¸¸ + ìˆ«ì íŒ¨í„´ (ë„ë¡œëª…ì£¼ì†Œë¶€ë²ˆê¹Œì§€) - ë” ì •í™•í•œ íŒ¨í„´
    addr_raw = re.sub(r'([ê°€-í£]+ê¸¸\d*[ê°€-í£]*\s+\d+[-\d]*)\s+[ê°€-í£].*$', r'\1', addr_raw)
    # ~ë¡œ + ìˆ«ì íŒ¨í„´ (ë„ë¡œëª…ì£¼ì†Œë¶€ë²ˆê¹Œì§€) - ê¸¸ì´ ì—†ëŠ” ê²½ìš°ë§Œ
    if 'ê¸¸' not in addr_raw:
        addr_raw = re.sub(r'([ê°€-í£]+ë¡œ\d*[ê°€-í£]*\s+\d+[-\d]*)\s+[ê°€-í£].*$', r'\1', addr_raw)
    
    # 6-1. íŠ¹ìˆ˜ ì¼€ì´ìŠ¤: ë„ë¡œëª…ì´ ë„ˆë¬´ êµ¬ì²´ì ì¸ ê²½ìš° ë” ê°„ë‹¨í•˜ê²Œ
    # ì£¼ì˜: "8ê¸¸", "9ê¸¸" ë“±ì€ ì‹¤ì œ ë„ë¡œëª…ì´ë¯€ë¡œ ì œê±°í•˜ë©´ ì•ˆë¨
    # ê´‘í‰ë¡œ34ê¸¸ -> ê´‘í‰ë¡œë¡œ ë‹¨ìˆœí™” (ë‹¨, ì¼ë°˜ì ì¸ ìˆ«ìê¸¸ì€ ì œì™¸)
    # addr_raw = re.sub(r'([ê°€-í£]+ë¡œ)\d+ê¸¸', r'\1', addr_raw)  # ì£¼ì„ ì²˜ë¦¬
    # ë„ë¡œëª… + ìˆ«ì ë’¤ì˜ ëª¨ë“  ê²ƒ ì œê±° (ê¸¸ì´ ì—†ëŠ” ê²½ìš°ë§Œ)
    if 'ê¸¸' not in addr_raw:
        addr_raw = re.sub(r'([ê°€-í£]+ë¡œ\s+\d+[-\d]*)\s+[ê°€-í£].*$', r'\1', addr_raw)
    
    # 7. ì–´ë¦°ì´ì§‘/ìœ ì¹˜ì› ë“± ì‹œì„¤ëª… ì œê±° (ë” êµ¬ì²´ì ì¸ íŒ¨í„´)
    # ~ê¸¸/ë¡œ + ìˆ«ì + ì‹œì„¤ëª… íŒ¨í„´
    addr_raw = re.sub(r'([ê°€-í£]+ê¸¸\d*[ê°€-í£]*\s+\d+)\s+[ê°€-í£]*(ì–´ë¦°ì´ì§‘|ìœ ì¹˜ì›|í•™êµ|ë³‘ì›|ì•½êµ­|ë§ˆíŠ¸|í¸ì˜ì |ê³µì›|ì²´ìœ¡ê´€|ë¬¸í™”ì„¼í„°|ì£¼ë¯¼ì„¼í„°|ì„¼í„°|ê´€ë¦¬ë™|ê´€ë¦¬ì‹¤|í‚¤ì¦ˆì„¼í„°|í‚¤ì¦ˆí´ëŸ½).*$', r'\1', addr_raw)
    addr_raw = re.sub(r'([ê°€-í£]+ë¡œ\d*[ê°€-í£]*\s+\d+)\s+[ê°€-í£]*(ì–´ë¦°ì´ì§‘|ìœ ì¹˜ì›|í•™êµ|ë³‘ì›|ì•½êµ­|ë§ˆíŠ¸|í¸ì˜ì |ê³µì›|ì²´ìœ¡ê´€|ë¬¸í™”ì„¼í„°|ì£¼ë¯¼ì„¼í„°|ì„¼í„°|ê´€ë¦¬ë™|ê´€ë¦¬ì‹¤|í‚¤ì¦ˆì„¼í„°|í‚¤ì¦ˆí´ëŸ½).*$', r'\1', addr_raw)
    
    # ì¼ë°˜ì ì¸ ì‹œì„¤ëª… ì œê±° (ê¸¸/ë¡œ íŒ¨í„´ì´ ì—†ëŠ” ê²½ìš°)
    addr_raw = re.sub(r'\s+[ê°€-í£]*(ì–´ë¦°ì´ì§‘|ìœ ì¹˜ì›|í•™êµ|ë³‘ì›|ì•½êµ­|ë§ˆíŠ¸|í¸ì˜ì |ê³µì›|ì²´ìœ¡ê´€|ë¬¸í™”ì„¼í„°|ì£¼ë¯¼ì„¼í„°|ì„¼í„°|ê´€ë¦¬ë™|ê´€ë¦¬ì‹¤|í‚¤ì¦ˆì„¼í„°|í‚¤ì¦ˆí´ëŸ½).*$', '', addr_raw)
    
    # 7. ê³µë°± ì •ë¦¬
    addr_raw = re.sub(r'\s+', ' ', addr_raw).strip()

    # 8. ì¶”ê°€ ì£¼ì†Œì •ê·œí™” ê·œì¹™ (ì‚° í‘œê¸°, ë™/í˜¸ìˆ˜, ë‹¨ì§€ëª… ì œê±° ë“±)
    # 8-1. 'ì‚°' ë’¤ ìˆ«ì ê³µë°± ì‚½ì… (ì˜ˆ: ì‚°19 -> ì‚° 19)
    addr_raw = re.sub(r"(ì‚°)(\d+)", r"\1 \2", addr_raw)

    # 8-2. ì„¸ë¶€ ë™Â·í˜¸ìˆ˜ ì œê±° (ì˜ˆ: 106-101, 101-102)
    # ì£¼ì˜: ê±´ë¬¼ë²ˆí˜¸ëŠ” ì œê±°í•˜ë©´ ì•ˆë¨ (ì˜ˆ: 22-5ëŠ” ê±´ë¬¼ë²ˆí˜¸)
    # í° ë²ˆí˜¸ì˜ ì„¸ë¶€ ë™Â·í˜¸ìˆ˜ë§Œ ì œê±° (ì˜ˆ: 106-101, 101-102)
    addr_raw = re.sub(r"\d{3,}-\d+$", "", addr_raw)

    # 8-3. ì¸µ/í˜¸/ìƒì„¸ ìœ„ì¹˜ ì œê±°
    addr_raw = re.sub(r"\d+ì¸µ", "", addr_raw)      # 3ì¸µ, 2ì¸µ
    addr_raw = re.sub(r"\d+í˜¸", "", addr_raw)      # 101í˜¸, 201í˜¸
    addr_raw = re.sub(r"B\d+", "", addr_raw)       # B113í˜¸ ê°™ì€ íŒ¨í„´
    addr_raw = re.sub(r"(ìƒê°€|ë³¸ê´€|ê´€ë¦¬ë™).*", "", addr_raw)  # ê´€ë¦¬ë™, ìƒê°€ 2ì¸µ ë“±

    # 8-4. ë‹¨ì§€ëª…/ì•„íŒŒíŠ¸ëª… ì œê±° (ê³µí†µ ì•„íŒŒíŠ¸ ë¸Œëœë“œëª…)
    apt_keywords = [
        "ìì´", "í‘¸ë¥´ì§€ì˜¤", "íìŠ¤í…Œì´íŠ¸", "ë˜ë¯¸ì•ˆ",
        "ì•„ì´íŒŒí¬", "ë¦¬ì²¸ì‹œì•„", "ë¡¯ë°ìºìŠ¬", "eí¸í•œì„¸ìƒ",
        "ì„¼íŠ¸ë ˆë¹Œ", "ìœ„ë¸Œ", "ë”ìƒµ", "SKë·°", "ì— ë²¨ë¦¬"
    ]
    for kw in apt_keywords:
        addr_raw = re.sub(rf"\s*{kw}\S*", "", addr_raw)

    # 8-5. ê´„í˜¸/ì‰¼í‘œ ì•ˆì˜ ë¶€ê°€ì„¤ëª… ì œê±°
    addr_raw = re.sub(r"\(.*?\)", "", addr_raw)
    addr_raw = re.sub(r",.*", "", addr_raw)

    # 8-6. ê±´ë¬¼ëª…/ì‹œì„¤ëª… ì œê±° (E3, Aë™, Bë™ ë“±)
    addr_raw = re.sub(r"\s+[A-Z]\d*$", "", addr_raw)  # E3, A1, B2 ë“±
    addr_raw = re.sub(r"\s+[ê°€-í£]*ë™$", "", addr_raw)  # Aë™, Bë™ ë“±
    
    # 8-7. ë¶ˆí•„ìš”í•œ ê³µë°± ì •ë¦¬
    addr_raw = re.sub(r"\s+", " ", addr_raw).strip()

    # âœ… ì£¼íƒ ì£¼ì†Œ ì „ìš© ê°•ì œ ì •ê·œí™” (ë§ˆì§€ë§‰ ë‹¨ê³„)
    # -------------------
    # 1. ê´„í˜¸ ì œê±°
    addr = re.sub(r'\([^)]*\)', '', addr_raw)

    # 2. ì‰¼í‘œ ë’¤ ìƒì„¸ì •ë³´ ì œê±°
    addr = re.sub(r',.*$', '', addr)

    # 3. "ì¸µ", "í˜¸", "ë™" ê°™ì€ ì„¸ë¶€ ìœ„ì¹˜ ì œê±°
    addr = re.sub(r'\s*\d+ì¸µ.*$', '', addr)
    addr = re.sub(r'\s*\d+í˜¸.*$', '', addr)
    addr = re.sub(r'\s*\d+ë™.*$', '', addr)
    addr = re.sub(r'\s*ì§€í•˜\d*ì¸µ?', '', addr)
    addr = re.sub(r'\s*ì§€ìƒ\d*ì¸µ?', '', addr)

    # 4. ë„ë¡œëª…+ë²ˆì§€ê¹Œì§€ë§Œ ë‚¨ê¸°ê¸°
    # ì˜ˆ: "ì„œìš¸íŠ¹ë³„ì‹œ ì„±ë¶êµ¬ ë™ì†Œë¬¸ë¡œ 305 ì§€ì¸µ" -> "ì„œìš¸íŠ¹ë³„ì‹œ ì„±ë¶êµ¬ ë™ì†Œë¬¸ë¡œ 305"
    # ì£¼ì˜: "ë‹¤ê¸¸", "ë‚˜ê¸¸" ë“±ì€ ì‹¤ì œ ë„ë¡œëª…ì´ë¯€ë¡œ ê±´ë¬¼ë²ˆí˜¸ë¥¼ ì œê±°í•˜ë©´ ì•ˆë¨
    # "ë¡œ"ë¡œ ëë‚˜ëŠ” ë„ë¡œëª…ì—ë§Œ ì ìš© (ê¸¸ì´ ì—†ëŠ” ê²½ìš°ë§Œ)
    if not re.search(r'[ë‹¤ë‚˜]ê¸¸', addr) and 'ê¸¸' not in addr:
        addr = re.sub(r'([ê°€-í£]+ë¡œ\s*\d+[-\d]*).*$', r'\1', addr)
    # ì£¼ì˜: "8ê¸¸", "9ê¸¸" ë“±ë„ ì‹¤ì œ ë„ë¡œëª…ì´ë¯€ë¡œ ê±´ë¬¼ë²ˆí˜¸ë¥¼ ì œê±°í•˜ë©´ ì•ˆë¨
    # "ë‹¤ê¸¸", "ë‚˜ê¸¸" ë“± íŠ¹ìˆ˜í•œ ê²½ìš°ë§Œ ì œì™¸
    # ì¼ë°˜ì ì¸ ìˆ«ìê¸¸ì€ ê±´ë¬¼ë²ˆí˜¸ë¥¼ ì œê±°í•˜ì§€ ì•ŠìŒ
    if not re.search(r'[ë‹¤ë‚˜]ê¸¸', addr) and not re.search(r'\d+ê¸¸', addr):
        addr = re.sub(r'([ê°€-í£]+ê¸¸\s*\d+[-\d]*).*$', r'\1', addr)

    # 5. ê³µë°± ì •ë¦¬
    addr = re.sub(r'\s+', ' ', addr).strip()

    return addr

class InfraNormalizer:
    """ì¸í”„ë¼ ë°ì´í„° ì •ê·œí™” í´ë˜ìŠ¤"""
    
    def __init__(self, data_dir: Path):
        self.data_dir = data_dir
        self.normalized_facilities: List[Dict] = []
        self.normalized_subway_stations: List[Dict] = []
        self.failed_addresses: List[Dict] = []  # ì‹¤íŒ¨í•œ ì£¼ì†Œ ì •ê·œí™” ë°ì´í„°
        
        # ìƒˆë¡œìš´ API í´ë¼ì´ì–¸íŠ¸ë“¤ ì´ˆê¸°í™”
        emd_api_key = os.getenv("TOEMDCD_API_KEY")
        tolola_api_key = os.getenv("TOLOLA_API_KEY")
        
        if not emd_api_key:
            raise ValueError("TOEMDCD_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        if not tolola_api_key:
            raise ValueError("TOLOLA_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            
        self.emd_api = EmdCodeAPI(emd_api_key)
        self.tolola_api = ToLolaAPI(tolola_api_key)
        
        # facility_categories ë§¤í•‘ (DBì—ì„œ ê°€ì ¸ì˜¤ê±°ë‚˜ í•˜ë“œì½”ë”©)
        # ì‹¤ì œ ìš´ì˜ í™˜ê²½ì—ì„œëŠ” DBì—ì„œ ë™ì ìœ¼ë¡œ ê°€ì ¸ì˜¤ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.
        self.category_map = {
            'hospital': 1,
            'school': 2,           # ì´ˆì¤‘ê³  + ëŒ€í•™
            'kindergarten': 3,     # ì–´ë¦°ì´ì§‘ â†’ childcare(ID 3)
            'childSchool': 11,     # ìœ ì¹˜ì› â†’ childSchool (ID 11)
            'park': 4,
            'mart': 5,
            'convenience': 6,
            'pharmacy': 7,
            'subway': 8,
            'bus': 9,              # ë²„ìŠ¤ì •ë¥˜ì†Œ
            'gym': 10,             # ê³µê³µì²´ìœ¡ì‹œì„¤
            'college': 2,          # schoolë¡œ í†µí•©
            'bus_stop': 9          # busë¡œ í†µí•©
        }
        
        # facility_id ìƒì„±ìš© ì¹´í…Œê³ ë¦¬ë³„ ì ‘ë‘ì‚¬ ë§¤í•‘
        self.facility_id_prefix_map = {
            'kindergarten': 'child',    # ì–´ë¦°ì´ì§‘
            'childSchool': 'chsch',     # ìœ ì¹˜ì›
            'school': 'sch',            # ì´ˆì¤‘ê³ 
            'college': 'col',           # ëŒ€í•™
            'pharmacy': 'pha',          # ì•½êµ­
            'hospital': 'hos',          # ë³‘ì›
            'mart': 'mart',             # ë§ˆíŠ¸
            'convenience': 'con',       # í¸ì˜ì 
            'gym': 'gym',               # ê³µê³µì²´ìœ¡ì‹œì„¤
            'park': 'pk',               # ê³µì›
            'subway': 'sub',            # ì§€í•˜ì² ì—­
            'bus': 'bus',               # ë²„ìŠ¤ì •ë¥˜ì†Œ
            'bus_stop': 'bus'           # ë²„ìŠ¤ì •ë¥˜ì†Œ (ë³„ì¹­)
        }
        
        # ì¹´í…Œê³ ë¦¬ë³„ ì¹´ìš´í„° (facility_id ìƒì„±ìš©)
        self.facility_counters = {prefix: 0 for prefix in self.facility_id_prefix_map.values()}
        
        # ì£¼ì†Œ ì •ê·œí™” API í‚¤ ë¡œë“œ
        self.juso_api_key = os.getenv("JUSO_API_KEY")
        if not self.juso_api_key:
            logger.warning("JUSO_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì£¼ì†Œ ì •ê·œí™” ê¸°ëŠ¥ì´ ì œí•œë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        
        # í•„ìš”í•œ ì¹´í…Œê³ ë¦¬ í™•ì¸ ë° ì¶”ê°€
        self._ensure_categories_exist()

    def _ensure_categories_exist(self):
        """í•„ìš”í•œ ì¹´í…Œê³ ë¦¬ê°€ DBì— ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•˜ê³  ì—†ìœ¼ë©´ ì¶”ê°€"""
        try:
            from backend.db.db_utils_pg import get_engine
            from sqlalchemy import text
            
            engine = get_engine()
            with engine.connect() as conn:
                # ID 11 (childSchool - ìœ ì¹˜ì›) í™•ì¸
                result = conn.execute(text("""
                    SELECT id FROM infra.facility_categories 
                    WHERE id = 11 AND code = 'childSchool'
                """)).fetchone()
                
                if not result:
                    # ID 11 ìœ ì¹˜ì› ì¹´í…Œê³ ë¦¬ ì¶”ê°€
                    conn.execute(text("""
                        INSERT INTO infra.facility_categories (id, code, name, description, created_at) 
                        VALUES (11, 'childSchool', 'ìœ ì¹˜ì›', 'êµìœ¡ì‹œì„¤', NOW())
                    """))
                    conn.commit()
                    logger.info("âœ… ìœ ì¹˜ì› ì¹´í…Œê³ ë¦¬(ID: 11) ì¶”ê°€ ì™„ë£Œ")
                else:
                    logger.info("âœ… ìœ ì¹˜ì› ì¹´í…Œê³ ë¦¬(ID: 11) ì´ë¯¸ ì¡´ì¬")
                    
        except Exception as e:
            logger.warning(f"âš ï¸ ì¹´í…Œê³ ë¦¬ í™•ì¸/ì¶”ê°€ ì¤‘ ì˜¤ë¥˜: {e}")

    def _get_category_id(self, code: str) -> Optional[int]:
        """ì‹œì„¤ ì¹´í…Œê³ ë¦¬ ì½”ë“œë¥¼ ê¸°ë°˜ìœ¼ë¡œ IDë¥¼ ì¡°íšŒ"""
        return self.category_map.get(code)
    
    def _generate_facility_id(self, facility_type: str) -> str:
        """ì‹œì„¤ íƒ€ì…ì— ë”°ë¼ ê³ ìœ í•œ facility_id ìƒì„±"""
        prefix = self.facility_id_prefix_map.get(facility_type, 'unk')
        
        # ì¹´ìš´í„° ì¦ê°€
        self.facility_counters[prefix] += 1
        
        # 4ìë¦¬ ìˆ«ìë¡œ í¬ë§·íŒ… (ì˜ˆ: child0001, sch0001)
        return f"{prefix}{self.facility_counters[prefix]:04d}"
    
    def _get_facility_cd(self, facility_type: str) -> str:
        """ì‹œì„¤ íƒ€ì…ì— ë”°ë¼ cd (ì ‘ë‘ì‚¬) ë°˜í™˜"""
        return self.facility_id_prefix_map.get(facility_type, 'unk')

    def _safe_int(self, value) -> Optional[int]:
        """ì•ˆì „í•˜ê²Œ ì •ìˆ˜ë¡œ ë³€í™˜"""
        if pd.isna(value) or value == '' or value is None:
            return None
        try:
            return int(float(str(value)))
        except (ValueError, TypeError):
            return None

    def _safe_float(self, value) -> Optional[float]:
        """ì•ˆì „í•˜ê²Œ ì‹¤ìˆ˜ë¡œ ë³€í™˜"""
        if pd.isna(value) or value == '' or value is None:
            return None
        try:
            return float(str(value))
        except (ValueError, TypeError):
            return None

    def _normalize_address(self, address_raw: str, facility_name: str = "", facility_type: str = "") -> Dict[str, Any]:
        """ì£¼ì†Œ ì •ê·œí™” - ìƒˆë¡œìš´ í•„ë“œë“¤ ì¶”ê°€"""
        if not address_raw or address_raw.strip() == '':
            return {
                'address_raw': address_raw,
                'address_nm': None,
                'address_id': None,
                'lat': None,
                'lon': None
            }
        
        logger.info(f"ğŸ  ì£¼ì†Œ ì •ê·œí™” ì‹œì‘")
        logger.info(f"ğŸ“ ì›ë³¸ ì£¼ì†Œ: {address_raw}")
        logger.info(f"ğŸ¢ ì‹œì„¤ëª…: {facility_name}")
        logger.info(f"ğŸ·ï¸ ì‹œì„¤íƒ€ì…: {facility_type}")
        
        # ì„œìš¸íŠ¹ë³„ì‹œ ì¶”ì¶œ (ëª¨ë“  ì£¼ì†Œì— ê³µí†µ ì ìš©)
        si_do = None
        if 'ì„œìš¸íŠ¹ë³„ì‹œ' in address_raw:
            si_do = 'ì„œìš¸íŠ¹ë³„ì‹œ'
        elif 'ì„œìš¸' in address_raw:
            si_do = 'ì„œìš¸íŠ¹ë³„ì‹œ'
        
        # ê³µì› ì£¼ì†Œì˜ ê²½ìš° ë¯¸ë¦¬ ì²´í¬
        is_park = facility_name and 'ê³µì›' in facility_name
        
        # ì£¼ì†Œ ì „ì²˜ë¦¬ (ì‹œì„¤ íƒ€ì…ë³„ ì „ìš© ì „ì²˜ë¦¬ í•¨ìˆ˜ ì‚¬ìš©)
        if facility_type == 'subway':
            addr_processed = preprocess_subway_address(address_raw)
            logger.info(f"ğŸš‡ ì§€í•˜ì²  ì „ìš© ì „ì²˜ë¦¬ ì ìš©")
        elif is_park:
            addr_processed = preprocess_park_address(address_raw)
            logger.info(f"ğŸŒ³ ê³µì› ì „ìš© ì „ì²˜ë¦¬ ì ìš©")
        else:
            addr_processed = preprocess_address(address_raw)
            logger.info(f"ğŸ”§ ì¼ë°˜ ì „ì²˜ë¦¬ ì ìš©")
        logger.info(f"âœ¨ ì „ì²˜ë¦¬ëœ ì£¼ì†Œ: {addr_processed}")
        
        # ì—¬ëŸ¬ íŒ¨í„´ìœ¼ë¡œ ì£¼ì†Œ ì •ê·œí™” ì‹œë„
        success = False
        result = None  # result ë³€ìˆ˜ ì´ˆê¸°í™”
        dong = None    # dong ë³€ìˆ˜ ì´ˆê¸°í™”
        
        for attempt, addr_to_try in enumerate([addr_processed, address_raw]):
            if attempt > 0:
                logger.info(f"ğŸ”„ ì£¼ì†Œ ì •ê·œí™” ì¬ì‹œë„ ({attempt+1}): {addr_to_try}")
            else:
                logger.info(f"ğŸ¯ ì£¼ì†Œ ì •ê·œí™” ì‹œë„: {addr_to_try}")
            
            try:
                result = normalize_address(addr_to_try)
                
                # ë²•ì •ë™ ì¶”ì¶œ (JUSO APIì—ì„œ ì§ì ‘ ê°€ì ¸ì˜¤ê¸°)
                dong = result.get('eupmyeon_dong', '')  # JUSO APIì—ì„œ ì§ì ‘ ê°€ì ¸ì˜¤ê¸°
                
                logger.info(f"âœ… ì£¼ì†Œ ì •ê·œí™” ì„±ê³µ!")
                logger.info(f"ğŸ“‹ ì •ê·œí™” ê²°ê³¼:")
                logger.info(f"   - ë„ë¡œëª…ì£¼ì†Œ: {result.get('road_full', 'N/A')}")
                logger.info(f"   - ì§€ë²ˆì£¼ì†Œ: {result.get('jibun_full', 'N/A')}")
                logger.info(f"   - ì‹œë„: {result.get('sido', 'N/A')}")
                logger.info(f"   - ì‹œêµ°êµ¬: {result.get('sigungu', 'N/A')}")
                logger.info(f"   - ìë©´ë™: {result.get('eupmyeon_dong', 'N/A')}")
                logger.info(f"   - ë²•ì •ë™ì½”ë“œ: {result.get('bcode', 'N/A')}")
                logger.info(f"   - ì¢Œí‘œ: ({result.get('y', 'N/A')}, {result.get('x', 'N/A')})")
                success = True
                break
            except AddressNormalizerError as e:
                logger.warning(f"âš ï¸ ì£¼ì†Œ ì •ê·œí™” ì‹¤íŒ¨ (ì‹œë„ {attempt+1}): {addr_to_try} - {e}")
                if attempt == 1:  # ë§ˆì§€ë§‰ ì‹œë„
                    logger.error(f"âŒ ì£¼ì†Œ ì •ê·œí™” ìµœì¢… ì‹¤íŒ¨: {address_raw} - {e}")
                    
                    # ê³µì›ì¸ ê²½ìš° ì§ì ‘ íŒŒì‹± ì‹œë„
                    if is_park:
                        logger.info(f"ğŸŒ³ ê³µì› ì£¼ì†Œ ì§ì ‘ íŒŒì‹± ì‹œë„: {address_raw}")
                        result = self._parse_park_address(address_raw)
                        result['si_do'] = si_do  # ì„œìš¸íŠ¹ë³„ì‹œ ì •ë³´ ë®ì–´ì“°ê¸°
                        return result
                    
                    # ì§„ì§œ ì‚° ì£¼ì†Œì¸ ê²½ìš° ì§ì ‘ íŒŒì‹± ì‹œë„ (ë„ë¡œëª…ì´ ì•„ë‹Œ ê²½ìš°ë§Œ)
                    if 'ì‚°' in address_raw and not ('ë¡œ' in address_raw or 'ê¸¸' in address_raw):
                        logger.info(f"â›°ï¸ ì‚° ì£¼ì†Œ ì§ì ‘ íŒŒì‹± ì‹œë„: {address_raw}")
                        result = self._parse_mountain_address(address_raw)
                        result['si_do'] = si_do  # ì„œìš¸íŠ¹ë³„ì‹œ ì •ë³´ ë®ì–´ì“°ê¸°
                        return result
                    
                    # ì‹¤íŒ¨í•œ ì£¼ì†Œ ì •ë³´ ìˆ˜ì§‘
                    failed_data = {
                        "facility_type": facility_type,
                        "facility_name": facility_name,
                        "address_raw": address_raw,
                        "address_processed": addr_processed,
                        "error_reason": str(e),
                        "timestamp": pd.Timestamp.now().isoformat()
                    }
                    self.failed_addresses.append(failed_data)
        
        if success and result:  # resultê°€ ì¡´ì¬í•  ë•Œë§Œ
            # address_nm ì„¤ì • (ì •ê·œí™”ëœ ì£¼ì†Œ)
            address_nm = result.get("jibun_full")
            # address_nmì—ì„œ ê±´ë¬¼ëª… ì œê±° (ë™+ì§€ë²ˆ ë’¤ì˜ ë¬¸ì ì œê±°)
            if address_nm:
                import re
                address_nm = re.sub(r'([ê°€-í£]+ë™\s+\d+[-\d]*)\s+.*$', r'\1', address_nm)
            logger.info(f"ğŸ“ address_nm ì„¤ì •: {address_nm}")
            
            # ë²•ì •ë™ ì½”ë“œ ì¡°íšŒ
            emd_code = None
            if address_nm:
                logger.info(f"ğŸ” ë²•ì •ë™ ì½”ë“œ ì¡°íšŒ ì‹œì‘...")
                emd_code = self.emd_api.get_emd_code(address_nm)
                if emd_code:
                    logger.info(f"âœ… ë²•ì •ë™ ì½”ë“œ ì¡°íšŒ ì„±ê³µ: {emd_code}")
                else:
                    logger.warning(f"âš ï¸ ë²•ì •ë™ ì½”ë“œ ì¡°íšŒ ì‹¤íŒ¨")
                time.sleep(0.1)  # API í˜¸ì¶œ ê°„ê²© ì¡°ì ˆ
            else:
                # address_nmì´ ì—†ëŠ” ê²½ìš° ì›ë³¸ ì£¼ì†Œì—ì„œ ì• 3ë‹¨ì–´ ì¶”ì¶œí•´ì„œ ì‹œë„
                logger.info(f"ğŸ”„ address_nmì´ ì—†ì–´ì„œ ì›ë³¸ ì£¼ì†Œì—ì„œ ì• 3ë‹¨ì–´ ì¶”ì¶œ ì‹œë„...")
                fallback_address = self._extract_first_three_words(address_raw)
                if fallback_address:
                    logger.info(f"ğŸ“ í´ë°± ì£¼ì†Œ: {fallback_address}")
                    emd_code = self.emd_api.get_emd_code(fallback_address)
                    if emd_code:
                        logger.info(f"âœ… í´ë°± ë²•ì •ë™ ì½”ë“œ ì¡°íšŒ ì„±ê³µ: {emd_code}")
                        # address_nmë„ í´ë°± ì£¼ì†Œë¡œ ì„¤ì •
                        address_nm = fallback_address
                    else:
                        logger.warning(f"âš ï¸ í´ë°± ë²•ì •ë™ ì½”ë“œ ì¡°íšŒë„ ì‹¤íŒ¨")
                else:
                    logger.warning(f"âš ï¸ í´ë°± ì£¼ì†Œ ì¶”ì¶œ ì‹¤íŒ¨")
            
            # ì¢Œí‘œ ë³€í™˜ (childSchool, neisSchool, subway, SebcCollege, gym, school, collegeë§Œ)
            lat, lon = None, None
            if facility_type in ["childSchool", "neisSchool", "subway", "SebcCollege", "gym", "school", "college"] and addr_processed:
                logger.info(f"ğŸŒ ì¢Œí‘œ ë³€í™˜ ì‹œì‘...")
                # ì „ì²˜ë¦¬ëœ ì£¼ì†Œë¥¼ ì‚¬ìš©í•˜ê³  ì£¼ì†Œ íƒ€ì…ì— ë”°ë¼ type íŒŒë¼ë¯¸í„° ì„¤ì •
                address_type = detect_address_type(addr_processed)
                type_param = "ROAD" if address_type == "road" else "PARCEL"
                logger.info(f"ğŸ“ ì£¼ì†Œ íƒ€ì… ê°ì§€: {address_type} -> API type: {type_param}")
                lat, lon = self.tolola_api.get_coordinates(addr_processed, type_param)
                if lat and lon:
                    logger.info(f"âœ… ì¢Œí‘œ ë³€í™˜ ì„±ê³µ: lat={lat}, lon={lon}")
                else:
                    logger.warning(f"âš ï¸ ì¢Œí‘œ ë³€í™˜ ì‹¤íŒ¨")
                time.sleep(0.1)  # API í˜¸ì¶œ ê°„ê²© ì¡°ì ˆ
            
            final_result = {
                'address_raw': address_raw,
                'address_nm': address_nm,
                'address_id': emd_code,
                'lat': lat,
                'lon': lon
            }
            logger.info(f"ğŸ¯ ìµœì¢… ì£¼ì†Œ ì •ê·œí™” ê²°ê³¼:")
            logger.info(f"   - address_raw: {final_result['address_raw']}")
            logger.info(f"   - address_nm: {final_result['address_nm']}")
            logger.info(f"   - address_id: {final_result['address_id']}")
            logger.info(f"   - lat: {final_result['lat']}")
            logger.info(f"   - lon: {final_result['lon']}")
            
            return final_result
        else:
            # JUSO API ì™„ì „ ì‹¤íŒ¨ ì‹œ
            logger.warning(f"ì£¼ì†Œ ì •ê·œí™” ì‹¤íŒ¨: {address_raw}")
            return {
                'address_raw': address_raw,
                'address_nm': None,
                'address_id': None,
                'lat': None,
                'lon': None
            }

    def _parse_mountain_address(self, address_raw: str) -> Dict[str, Any]:
        """ì‚°ì´ í¬í•¨ëœ ì£¼ì†Œ ì§ì ‘ íŒŒì‹±"""
        result = self._parse_common_address(address_raw, create_norm=False)
        result['address_norm'] = address_raw  # ì‚° ì£¼ì†ŒëŠ” ì›ë³¸ ê·¸ëŒ€ë¡œ ì‚¬ìš©
        logger.info(f"ì‚° ì£¼ì†Œ íŒŒì‹± ê²°ê³¼: si_do={result['si_do']}, si_gun_gu={result['si_gun_gu']}, si_gun_gu_dong={result['si_gun_gu_dong']}")
        return result

    def _parse_park_address(self, address_raw: str) -> Dict[str, Any]:
        """ê³µì› ì£¼ì†Œ ì§ì ‘ íŒŒì‹± (JUSO API ì‹¤íŒ¨ ì‹œ) - ì—¬ëŸ¬ ë™ ëª¨ë‘ ì¶”ì¶œ"""
        result = self._parse_common_address(address_raw, create_norm=False, extract_all_dongs=True)
        result['address_norm'] = address_raw  # ê³µì› ì£¼ì†ŒëŠ” ì›ë³¸ ê·¸ëŒ€ë¡œ ì‚¬ìš©
        logger.info(f"ê³µì› ì£¼ì†Œ íŒŒì‹± ê²°ê³¼: si_do={result['si_do']}, si_gun_gu={result['si_gun_gu']}, si_gun_gu_dong={result['si_gun_gu_dong']}, all_dongs={result.get('si_gun_gu_dongs', [])}")
        return result

    def _parse_minimal_address(self, address_raw: str) -> Dict[str, Any]:
        """ìµœì†Œí•œì˜ ì£¼ì†Œ íŒŒì‹± (JUSO API ì™„ì „ ì‹¤íŒ¨ ì‹œ)"""
        result = self._parse_common_address(address_raw, create_norm=False)
        result['address_norm'] = address_raw  # ì›ë³¸ ê·¸ëŒ€ë¡œ ì‚¬ìš©
        result['si_gun_gu_dong'] = None  # ë™ ì •ë³´ëŠ” ì¶”ì¶œí•˜ì§€ ì•ŠìŒ
        logger.info(f"ìµœì†Œ íŒŒì‹± ê²°ê³¼: si_do={result['si_do']}, si_gun_gu={result['si_gun_gu']}")
        return result

    def _parse_common_address(self, address_raw: str, create_norm: bool = True, extract_all_dongs: bool = False) -> Dict[str, Any]:
        """ê³µí†µ ì£¼ì†Œ íŒŒì‹± í•¨ìˆ˜ (ëª¨ë“  ë°ì´í„°ì…‹ì—ì„œ ì‚¬ìš©) - ì„œìš¸íŠ¹ë³„ì‹œëŠ” _normalize_addressì—ì„œ ì²˜ë¦¬"""
        import re
        
        # êµ¬ ì¶”ì¶œ
        si_gun_gu = None
        gu_pattern = r'([ê°€-í£]+êµ¬)'
        gu_match = re.search(gu_pattern, address_raw)
        if gu_match:
            si_gun_gu = gu_match.group(1)
        
        # ë™/ê°€ ì¶”ì¶œ (ì‚° ë²ˆí˜¸ ë“±ì€ ì œì™¸)
        si_gun_gu_dong = None
        si_gun_gu_dongs = []  # ì—¬ëŸ¬ ë™ ì €ì¥ìš©
        
        # ~ë™ìœ¼ë¡œ ëë‚˜ëŠ” ê²ƒë§Œ ì¶”ì¶œ (ì‚° ë²ˆí˜¸, ê±´ë¬¼ ë²ˆí˜¸ ë“± ì œì™¸)
        dong_pattern = r'([ê°€-í£]+ë™+" ")'
        dong_matches = re.findall(dong_pattern, address_raw)
        
        if dong_matches:
            if extract_all_dongs:
                # ëª¨ë“  ë™ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ì €ì¥
                si_gun_gu_dongs = dong_matches
                si_gun_gu_dong = dong_matches[0] if dong_matches else None  # ì²« ë²ˆì§¸ ë™ì„ ê¸°ë³¸ê°’ìœ¼ë¡œ
            else:
                # ì²« ë²ˆì§¸ ë™ë§Œ ì €ì¥ (ê¸°ì¡´ ë°©ì‹)
                si_gun_gu_dong = dong_matches[0]
        
        # address_norm ìƒì„± (ì •ê·œí™”ëœ ì£¼ì†Œ)
        address_norm = address_raw
        if create_norm and si_gun_gu and si_gun_gu_dong:
            # "ê°•ë‚¨êµ¬ ì—­ì‚¼ë™" í˜•íƒœë¡œ ì •ê·œí™” (ì„œìš¸íŠ¹ë³„ì‹œëŠ” _normalize_addressì—ì„œ ì¶”ê°€)
            address_norm = f"{si_gun_gu} {si_gun_gu_dong}"
        elif create_norm and si_gun_gu:
            # "ê°•ë‚¨êµ¬" í˜•íƒœë¡œ ì •ê·œí™”
            address_norm = si_gun_gu
        
        logger.info(f"ê³µí†µ ì£¼ì†Œ íŒŒì‹± ê²°ê³¼: si_gun_gu={si_gun_gu}, si_gun_gu_dong={si_gun_gu_dong}, all_dongs={si_gun_gu_dongs if extract_all_dongs else None}")
        
        result = {
            'address_raw': address_raw,
            'address_norm': address_norm,
            'si_do': None,  # _normalize_addressì—ì„œ ì²˜ë¦¬
            'si_gun_gu': si_gun_gu,
            'si_gun_gu_dong': si_gun_gu_dong,
            'road_full': None,
            'jibun_full': None,
            'lat': None,
            'lon': None,
            'geo_extra': None
        }
        
        # ì—¬ëŸ¬ ë™ì´ ìˆëŠ” ê²½ìš° ì¶”ê°€ ì •ë³´ë¡œ ì €ì¥
        if extract_all_dongs and si_gun_gu_dongs:
            result['si_gun_gu_dongs'] = si_gun_gu_dongs
        
        return result

    def _normalize_childcare_centers(self, file_path: Path):
        """ì–´ë¦°ì´ì§‘ ë°ì´í„° ì •ê·œí™”"""
        if not file_path.exists():
            logger.warning(f"ì–´ë¦°ì´ì§‘ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {file_path}")
            return

        df = read_csv_with_auto_encoding(file_path)
        logger.info(f"ì–´ë¦°ì´ì§‘ ë°ì´í„° ì •ê·œí™” ì‹œì‘: {len(df)}ê°œ")

        for _, row in df.iterrows():
            address_raw = str(row.get('CRADDR', ''))
            facility_name = str(row.get('CRNAME', ''))
            address_info = self._normalize_address(address_raw, facility_name, 'childcare')
            
            facility_data = {
                'facility_id': self._generate_facility_id('kindergarten'),
                'cd': self._get_facility_cd('kindergarten'),
                'name': facility_name,
                'address_raw': address_info['address_raw'],
                'address_nm': address_info['address_nm'],
                'address_id': address_info['address_id'],
                'lat': address_info['lat'] or self._safe_float(row.get('LA')),
                'lon': address_info['lon'] or self._safe_float(row.get('LO')),
                'phone': str(row.get('CRTELNO', '')),
                'website': str(row.get('CRHOME', '')),
                'operating_hours': None,
                'is_24h': False,
                'is_emergency': False,
                'capacity': self._safe_int(row.get('CRCAPAT')),
                'grade_level': None,
                'facility_extra': {
                    'childcare_type': str(row.get('CRTYPENAME', ''))
                },
                'data_source': 'openseoul'
            }
            self.normalized_facilities.append(facility_data)
        logger.info(f"ì–´ë¦°ì´ì§‘ ë°ì´í„° ì •ê·œí™” ì™„ë£Œ: {len(self.normalized_facilities)}ê°œ")

    def _normalize_schools(self, file_path: Path):
        """í•™êµ ë°ì´í„° ì •ê·œí™”"""
        if not file_path.exists():
            logger.warning(f"í•™êµ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {file_path}")
            return

        df = read_csv_with_auto_encoding(file_path)
        logger.info(f"í•™êµ ë°ì´í„° ì •ê·œí™” ì‹œì‘: {len(df)}ê°œ")

        for _, row in df.iterrows():
            address_raw = str(row.get('ORG_RDNMA', ''))
            facility_name = str(row.get('SCHUL_NM', ''))
            address_info = self._normalize_address(address_raw, facility_name, 'school')
            
            facility_data = {
                'facility_id': self._generate_facility_id('school'),
                'cd': self._get_facility_cd('school'),
                'name': facility_name,
                'address_raw': address_info['address_raw'],
                'address_nm': address_info['address_nm'],
                'address_id': address_info['address_id'],
                'lat': address_info['lat'] or self._safe_float(row.get('LAT')),
                'lon': address_info['lon'] or self._safe_float(row.get('LON')),
                'phone': str(row.get('ORG_TELNO', '')),
                'website': str(row.get('HMPG_ADRES', '')),
                'operating_hours': None,
                'is_24h': False,
                'is_emergency': False,
                'capacity': None,
                'grade_level': str(row.get('SCHUL_CRSE_SC_NM', '')),
                'facility_extra': {
                    'foundation_type': str(row.get('FOND_SC_NM', '')),
                    'special_class_exist': str(row.get('INDST_SPECL_CCCCL_EXST_YN', '')),
                    'high_school_type': str(row.get('HS_GNRL_BUSNS_SC_NM', '')),
                    'special_purpose': str(row.get('SPCLY_PURPS_HS_ORD_NM', ''))
                },
                'data_source': 'openseoul'
            }
            self.normalized_facilities.append(facility_data)
        logger.info(f"í•™êµ ë°ì´í„° ì •ê·œí™” ì™„ë£Œ: {len(self.normalized_facilities)}ê°œ")

    def _normalize_parks(self, file_path: Path):
        """ê³µì› ë°ì´í„° ì •ê·œí™”"""
        if not file_path.exists():
            logger.warning(f"ê³µì› íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {file_path}")
            return

        df = read_csv_with_auto_encoding(file_path)
        logger.info(f"ê³µì› ë°ì´í„° ì •ê·œí™” ì‹œì‘: {len(df)}ê°œ")

        for _, row in df.iterrows():
            address_raw = str(row.get('P_ADDR', ''))
            facility_name = str(row.get('P_PARK', ''))
            address_info = self._normalize_address(address_raw, facility_name, 'park')
            
            facility_data = {
                'facility_id': self._generate_facility_id('park'),
                'cd': self._get_facility_cd('park'),
                'name': facility_name,
                'address_raw': address_info.get('address_raw', address_raw),
                'address_nm': address_info.get('address_nm'),
                'address_id': address_info.get('address_id'),
                'lat': address_info.get('lat') or self._safe_float(row.get('LATITUDE')),
                'lon': address_info.get('lon') or self._safe_float(row.get('LONGITUDE')),
                'phone': str(row.get('P_ADMINTEL', '')),
                'website': str(row.get('TEMPLATE_URL', '')),
                'operating_hours': None,
                'is_24h': False,
                'is_emergency': False,
                'capacity': None,
                'grade_level': None,
                'facility_extra': {
                    'description': str(row.get('P_LIST_CONTENT', '')),
                    'area': self._safe_float(row.get('AREA')),
                    'open_date': str(row.get('OPEN_DT', '')),
                    'main_equipment': str(row.get('MAIN_EQUIP', '')),
                    'main_plants': str(row.get('MAIN_PLANTS', '')),
                    'guidance': str(row.get('GUIDANCE', '')),
                    'visit_road': str(row.get('VISIT_ROAD', '')),
                    'use_refer': str(row.get('USE_REFER', ''))
                },
                'data_source': 'openseoul'
            }
            self.normalized_facilities.append(facility_data)
        logger.info(f"ê³µì› ë°ì´í„° ì •ê·œí™” ì™„ë£Œ: {len(self.normalized_facilities)}ê°œ")

    def _normalize_subway_stations(self, file_path: Path):
        """ì§€í•˜ì² ì—­ ë°ì´í„° ì •ê·œí™” (ì—­ì •ë³´ + ì£¼ì†Œ CSV ë§¤í•‘ + ì£¼ì†Œ ì •ê·œí™”)"""
        if not file_path.exists():
            logger.warning(f"ì§€í•˜ì² ì—­ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {file_path}")
            return

        # 1. ì§€í•˜ì² ì—­ ì •ë³´ íŒŒì¼ ë¡œë“œ
        df_stn = read_csv_with_auto_encoding(file_path, dtype=str)
        logger.info(f"ì§€í•˜ì² ì—­ ë°ì´í„° ë¡œë“œ: {len(df_stn)}ê°œ")

        # 2. ì£¼ì†Œ ì •ë³´ íŒŒì¼ ë¡œë“œ (StationAdresTelno)
        addr_file = self.data_dir / "seoul_StationAdresTelno_20250921.csv"
        addr_map = {}
        if addr_file.exists():
            df_addr = read_csv_with_auto_encoding(addr_file, dtype=str)

            # ì£¼ì†Œ ì»¬ëŸ¼ ìë™ íƒìƒ‰
            addr_col_candidates = ["OLD_ADDR", "OLD_ADDRESS", "ADDR", "ADDRESS"]
            addr_col = next((c for c in addr_col_candidates if c in df_addr.columns), None)

            if not addr_col:
                raise ValueError(f"ì£¼ì†Œ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. df_addr.columns={df_addr.columns.tolist()}")

            if "SBWY_STNS_NM" not in df_addr.columns:
                raise ValueError(f"'SBWY_STNS_NM' ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. df_addr.columns={df_addr.columns.tolist()}")

            # "ì—­" ì œê±° ë° ê´„í˜¸ ë‚´ìš© ì œê±° í›„ ë§¤í•‘
            import re
            station_names_cleaned = df_addr["SBWY_STNS_NM"].astype(str).str.replace("ì—­", "").str.strip()
            # ê´„í˜¸ ì•ˆì˜ ë‚´ìš© ì œê±° (ì˜ˆ: "ì¶©ì •ë¡œ(ê²½ê¸°ëŒ€ì…êµ¬)" â†’ "ì¶©ì •ë¡œ")
            station_names_cleaned = station_names_cleaned.apply(lambda x: re.sub(r'\([^)]*\)', '', x).strip())
            
            addr_map = dict(
                zip(
                    station_names_cleaned,
                    df_addr[addr_col].astype(str).str.strip()
                )
            )
            logger.info(f"ì§€í•˜ì² ì—­ ì£¼ì†Œ ë°ì´í„° ë¡œë“œ: {len(addr_map)}ê°œ")
        else:
            logger.warning(f"ì§€í•˜ì² ì—­ ì£¼ì†Œ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {addr_file}")

        # 3. ì§€í•˜ì² ì—­ ë°ì´í„° ìˆœíšŒ
        for _, row in df_stn.iterrows():
            # (1) ì—­ëª…
            station_name_raw = str(row.get("STATION_NM", "")).strip()

            # (2) ì£¼ì†Œ ë§¤í•‘ (ì„œìš¸ë§Œ ì„±ê³µ â†’ ì„œìš¸ ì™¸ ì§€ì—­ì€ ë¹ˆê°’)
            address_raw = addr_map.get(station_name_raw, "")
            
            # (2-1) ì§€í•˜ì² ì—­ ì£¼ì†Œ ì „ì²˜ë¦¬ (ì œê±° - ë‹¤ë¥¸ ì‹œì„¤ë“¤ê³¼ ë™ì¼í•˜ê²Œ _normalize_addressì—ì„œ ì²˜ë¦¬)
            # address_rawëŠ” ì›ë³¸ ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ê³ , address_nm êµ¬í•  ë•Œë§Œ ì „ì²˜ë¦¬
            
            # (3) ì£¼ì†Œ ì •ê·œí™” (ë‹¤ë¥¸ ì‹œì„¤ë“¤ê³¼ ë™ì¼í•˜ê²Œ _normalize_address ì‚¬ìš©)
            if address_raw:
                address_info = self._normalize_address(address_raw, station_name_raw, 'subway')
            else:
                address_info = {
                    'address_raw': address_raw,
                    'address_nm': None,
                    'address_id': None,
                    'lat': None,
                    'lon': None
                }

            # (4) í˜¸ì„  ì •ë³´ (CSV ì›ë³¸ ê·¸ëŒ€ë¡œ ë°˜ì˜)
            line_name = str(row.get("LINE_NUM", "")).strip()

            # (5) í™˜ìŠ¹ ì²˜ë¦¬
            transfer_lines = []
            is_transfer = False
            if "," in line_name:
                transfer_lines = [ln.strip() for ln in line_name.split(",")]
                is_transfer = True

            # (6) ìµœì¢… ë°ì´í„° êµ¬ì„± (ìƒˆë¡œìš´ í•„ë“œ êµ¬ì¡°)
            station_data = {
                "facility_id": self._generate_facility_id('subway'),
                "cd": self._get_facility_cd('subway'),
                "station_name": station_name_raw,
                "line_name": line_name,  # âœ… CSV ì›ë³¸ ë°˜ì˜
                "station_code": str(row.get("FR_CODE", "")),
                "address_raw": address_info['address_raw'],
                "address_nm": address_info['address_nm'],
                "address_id": address_info['address_id'],
                "lat": address_info['lat'],
                "lon": address_info['lon'],
                "exit_count": None,
                "is_transfer": is_transfer,
                "transfer_lines": transfer_lines,
                "station_extra": {
                    "station_name_eng": str(row.get("STATION_NM_ENG", "")),
                    "station_name_chn": str(row.get("STATION_NM_CHN", "")),
                    "station_name_jpn": str(row.get("STATION_NM_JPN", "")),
                    "subway_code": str(row.get("SBWY_STNS_NM", "")),
                    "route_code": str(row.get("SBWY_ROUT_LN", "")),
                },
                "data_source": "openseoul"
            }
            self.normalized_subway_stations.append(station_data)

        logger.info(f"ì§€í•˜ì² ì—­ ë°ì´í„° ì •ê·œí™” ì™„ë£Œ: {len(self.normalized_subway_stations)}ê°œ")

    def _normalize_pharmacies(self, file_path: Path):
        """ì•½êµ­ ë°ì´í„° ì •ê·œí™”"""
        if not file_path.exists():
            logger.warning(f"ì•½êµ­ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {file_path}")
            return

        df = read_csv_with_auto_encoding(file_path)
        logger.info(f"ì•½êµ­ ë°ì´í„° ì •ê·œí™” ì‹œì‘: {len(df)}ê°œ")

        for _, row in df.iterrows():
            address_raw = str(row.get('DUTYADDR', ''))
            facility_name = str(row.get('DUTYNAME', ''))
            address_info = self._normalize_address(address_raw, facility_name, 'pharmacy')
            
            facility_data = {
                'facility_id': self._generate_facility_id('pharmacy'),
                'cd': self._get_facility_cd('pharmacy'),
                'name': facility_name,
                'address_raw': address_info['address_raw'],
                'address_nm': address_info['address_nm'],
                'address_id': address_info['address_id'],
                'lat': address_info['lat'] or self._safe_float(row.get('WGS84LAT')),
                'lon': address_info['lon'] or self._safe_float(row.get('WGS84LON')),
                'phone': str(row.get('DUTYTEL1', '')),
                'website': None,
                'operating_hours': None,
                'is_24h': False,
                'is_emergency': False,
                'capacity': None,
                'grade_level': None,
                'facility_extra': {
                    'pharmacy_id': str(row.get('HOSID', '')),
                    'district': str(row.get('SIGUN_NM', ''))
                },
                'data_source': 'openseoul'
            }
            self.normalized_facilities.append(facility_data)
        logger.info(f"ì•½êµ­ ë°ì´í„° ì •ê·œí™” ì™„ë£Œ: {len(self.normalized_facilities)}ê°œ")

    def _normalize_kindergartens(self, file_path: Path):
        """ìœ ì¹˜ì› ë°ì´í„° ì •ê·œí™”"""
        if not file_path.exists():
            logger.warning(f"ìœ ì¹˜ì› íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {file_path}")
            return

        df = read_csv_with_auto_encoding(file_path)
        logger.info(f"ìœ ì¹˜ì› ë°ì´í„° ì •ê·œí™” ì‹œì‘: {len(df)}ê°œ")

        for _, row in df.iterrows():
            address_raw = str(row.get('ADDR', ''))
            facility_name = str(row.get('KDGT_NM', ''))
            address_info = self._normalize_address(address_raw, facility_name, 'kindergarten')
            
            facility_data = {
                'facility_id': self._generate_facility_id('childSchool'),
                'cd': self._get_facility_cd('childSchool'),
                'name': facility_name,
                'address_raw': address_info['address_raw'],
                'address_nm': address_info['address_nm'],
                'address_id': address_info['address_id'],
                'lat': address_info['lat'],
                'lon': address_info['lon'],
                'phone': str(row.get('TELNO', '')),
                'website': str(row.get('HMPG', '')),
                'operating_hours': None,
                'is_24h': False,
                'is_emergency': False,
                'capacity': self._safe_int(row.get('MIX_TDL_CNT')),
                'grade_level': None,
                'facility_extra': {
                    'foundation_type': str(row.get('FNDN_TYPE', ''))
                },
                'data_source': 'openseoul'
            }
            self.normalized_facilities.append(facility_data)
        logger.info(f"ìœ ì¹˜ì› ë°ì´í„° ì •ê·œí™” ì™„ë£Œ: {len(self.normalized_facilities)}ê°œ")

    def _normalize_colleges(self, file_path: Path):
        """ëŒ€í•™ ë°ì´í„° ì •ê·œí™”"""
        if not file_path.exists():
            logger.warning(f"ëŒ€í•™ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {file_path}")
            return

        df = read_csv_with_auto_encoding(file_path)
        logger.info(f"ëŒ€í•™ ë°ì´í„° ì •ê·œí™” ì‹œì‘: {len(df)}ê°œ")

        for _, row in df.iterrows():
            address_raw = str(row.get('ADD_KOR', ''))
            facility_name = str(row.get('NAME_KOR', ''))
            address_info = self._normalize_address(address_raw, facility_name, 'college')
            
            facility_data = {
                'facility_id': self._generate_facility_id('college'),
                'cd': self._get_facility_cd('college'),
                'name': facility_name,
                'address_raw': address_info['address_raw'],
                'address_nm': address_info['address_nm'],
                'address_id': address_info['address_id'],
                'lat': address_info['lat'],
                'lon': address_info['lon'],
                'phone': str(row.get('TEL', '')),
                'website': str(row.get('HP', '')),
                'operating_hours': None,
                'is_24h': False,
                'is_emergency': False,
                'capacity': None,
                'grade_level': None,
                'facility_extra': {
                    'category': str(row.get('CATE1_NAME', '')),
                    'branch': str(row.get('BRANCH', '')),
                    'type': str(row.get('TYPE', ''))
                },
                'data_source': 'openseoul'
            }
            self.normalized_facilities.append(facility_data)
        logger.info(f"ëŒ€í•™ ë°ì´í„° ì •ê·œí™” ì™„ë£Œ: {len(self.normalized_facilities)}ê°œ")


    def _normalize_bus_stops(self, file_path: Path):
        """ë²„ìŠ¤ì •ë¥˜ì†Œ ë°ì´í„° ì •ê·œí™” - ê²½ë„,ìœ„ë„ í˜•ì‹ìœ¼ë¡œ address_raw ì €ì¥"""
        if not file_path.exists():
            logger.warning(f"ë²„ìŠ¤ì •ë¥˜ì†Œ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {file_path}")
            return

        df = read_csv_with_auto_encoding(file_path)
        logger.info(f"ë²„ìŠ¤ì •ë¥˜ì†Œ ë°ì´í„° ì •ê·œí™” ì‹œì‘: {len(df)}ê°œ")

        for _, row in df.iterrows():
            # ê²½ë„(XCRD), ìœ„ë„(YCRD)ë¥¼ 'ê²½ë„,ìœ„ë„' í˜•ì‹ìœ¼ë¡œ address_rawì— ì €ì¥
            x_coord = self._safe_float(row.get('XCRD'))
            y_coord = self._safe_float(row.get('YCRD'))
            
            if x_coord is not None and y_coord is not None:
                address_raw = f"{x_coord},{y_coord}"
            else:
                address_raw = ""
            
            facility_data = {
                'facility_id': self._generate_facility_id('bus_stop'),
                'name': str(row.get('STOPS_NM', '')),  # ì •ë¥˜ì†Œëª…
                'address_raw': address_raw,
                'address_nm': None,  # ì¢Œí‘œ ê¸°ë°˜ì´ë¯€ë¡œ ì •ê·œí™” ë¶ˆê°€
                'address_id': None,  # ì¢Œí‘œ ê¸°ë°˜ì´ë¯€ë¡œ ë²•ì •ë™ ì½”ë“œ ì—†ìŒ
                'lat': y_coord,  # ìœ„ë„
                'lon': x_coord,  # ê²½ë„
                'phone': None,
                'website': None,
                'operating_hours': None,
                'is_24h': False,
                'is_emergency': False,
                'capacity': None,
                'grade_level': None,
                'facility_extra': {
                    'stops_no': str(row.get('STOPS_NO', '')),
                    'node_id': str(row.get('NODE_ID', '')),
                    'stops_type': str(row.get('STOPS_TYPE', '')),
                    'x_coord': x_coord,
                    'y_coord': y_coord
                },
                'data_source': 'openseoul'
            }
            self.normalized_facilities.append(facility_data)
        logger.info(f"ë²„ìŠ¤ì •ë¥˜ì†Œ ë°ì´í„° ì •ê·œí™” ì™„ë£Œ: {len(self.normalized_facilities)}ê°œ")

    def normalize_openseoul_data(self) -> Dict[str, List[Dict]]:
        """OpenSeoul CSV íŒŒì¼ë“¤ì„ ì •ê·œí™”"""
        # ì¹´ìš´í„° ì´ˆê¸°í™”
        self.facility_counters = {prefix: 0 for prefix in self.facility_id_prefix_map.values()}
        
        openseoul_dir = self.data_dir  # backend/data/public-api/openseoul

        # ì–´ë¦°ì´ì§‘ ë°ì´í„°
        childcare_file = openseoul_dir / "seoul_ChildCareInfo_20250919.csv"
        self._normalize_childcare_centers(childcare_file)
        
        # ìœ ì¹˜ì› ë°ì´í„°
        kindergarten_file = openseoul_dir / "seoul_childSchoolInfo_20250919.csv"
        self._normalize_kindergartens(kindergarten_file)
        
        # í•™êµ ë°ì´í„° (ì´ˆì¤‘ê³ )
        school_file = openseoul_dir / "seoul_neisSchoolInfo_20250919.csv"
        self._normalize_schools(school_file)
        
        # ëŒ€í•™ ë°ì´í„°
        college_file = openseoul_dir / "seoul_SebcCollegeInfoKor_20250919.csv"
        self._normalize_colleges(college_file)
        
        # ê³µì› ë°ì´í„°
        park_file = openseoul_dir / "seoul_SearchParkInfoService_20250919.csv"
        self._normalize_parks(park_file)
        
        # ì§€í•˜ì² ì—­ ë°ì´í„°
        subway_file = openseoul_dir / "seoul_SearchSTNBySubwayLineInfo_20250919.csv"
        self._normalize_subway_stations(subway_file)
        
        # ì•½êµ­ ë°ì´í„°
        pharmacy_file = openseoul_dir / "seoul_TbPharmacyOperateInfo_20250919.csv"
        self._normalize_pharmacies(pharmacy_file)
        
        # ëŒ€í•™ ë°ì´í„°
        college_file = openseoul_dir / "seoul_SebcCollegeInfoKor_20250919.csv"
        self._normalize_colleges(college_file)


        # ë²„ìŠ¤ì •ë¥˜ì†Œ ë°ì´í„°ëŠ” ë³„ë„ í…Œì´ë¸”ì— ì €ì¥í•˜ë¯€ë¡œ ì œì™¸
        # bus_stop_file = openseoul_dir / "seoul_busStopLocationXyInfo_20250921.csv"
        # self._normalize_bus_stops(bus_stop_file)

        # localdata í´ë” ë°ì´í„° ì²˜ë¦¬
        localdata_dir = self.data_dir.parent / "localdata"
        
        # ê³µê³µì²´ìœ¡ì‹œì„¤ ë°ì´í„°
        sports_file = localdata_dir / "utf8_ì„œìš¸ì‹œ ê³µê³µì²´ìœ¡ì‹œì„¤ ì •ë³´.csv"
        if sports_file.exists():
            self._normalize_sports_facilities(sports_file)
        
        # ë§ˆíŠ¸ ë°ì´í„°
        mart_file = localdata_dir / "utf8_ì„œìš¸ì‹œ ë§ˆíŠ¸.csv"
        if mart_file.exists():
            self._normalize_marts(mart_file)
        
        # ë³‘ì› ë°ì´í„°
        hospital_file = localdata_dir / "utf8_ì„œìš¸ì‹œë³‘ì›_ë‚´ê³¼ì†Œì•„ê³¼ì‘ê¸‰ì˜í•™ê³¼.csv"
        if hospital_file.exists():
            self._normalize_hospitals(hospital_file)
        
        # í¸ì˜ì  ë°ì´í„°
        convenience_file = localdata_dir / "utf8_ì„œìš¸ì‹œ í¸ì˜ì .csv"
        if convenience_file.exists():
            self._normalize_convenience_stores(convenience_file)

        logger.info(f"ì´ {len(self.normalized_facilities)}ê°œì˜ ì‹œì„¤ ë°ì´í„°ì™€ {len(self.normalized_subway_stations)}ê°œì˜ ì§€í•˜ì² ì—­ ë°ì´í„° ì •ê·œí™” ì™„ë£Œ.")
        
        # ì¹´ìš´í„° ìƒíƒœ ë¡œê¹…
        logger.info("ìƒì„±ëœ facility_id ì¹´ìš´í„° ìƒíƒœ:")
        for prefix, count in self.facility_counters.items():
            if count > 0:
                logger.info(f"  {prefix}: {count}ê°œ")
        
        return {
            "public_facilities": self.normalized_facilities,
            "subway_stations": self.normalized_subway_stations,
            "failed_addresses": self.failed_addresses
        }
    
    def save_normalized_data(self, output_dir: Path) -> Path:
        """ì •ê·œí™”ëœ ë°ì´í„°ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        import json
        from datetime import datetime
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # ì •ê·œí™”ëœ ë°ì´í„° êµ¬ì„±
        # ì •ê·œí™”ëœ ë°ì´í„° êµ¬ì¡°í™”
        public_facilities_data = {
            "public_facilities": self.normalized_facilities,
            "metadata": {
                "normalized_at": datetime.now().isoformat(),
                "facilities_count": len(self.normalized_facilities),
                "failed_addresses_count": len(self.failed_addresses)
            }
        }
        
        subway_stations_data = {
            "subway_stations": self.normalized_subway_stations,
            "metadata": {
                "normalized_at": datetime.now().isoformat(),
                "subway_stations_count": len(self.normalized_subway_stations)
            }
        }
        
        # JSON íŒŒì¼ë¡œ ì €ì¥ (ë³„ë„ íŒŒì¼)
        public_facilities_file = output_dir / "public_facilities.json"
        with open(public_facilities_file, 'w', encoding='utf-8') as f:
            json.dump(public_facilities_data, f, ensure_ascii=False, indent=2)
        
        subway_stations_file = output_dir / "subway_stations.json"
        with open(subway_stations_file, 'w', encoding='utf-8') as f:
            json.dump(subway_stations_data, f, ensure_ascii=False, indent=2)
        
        # ì‹¤íŒ¨í•œ ì£¼ì†Œ ì •ê·œí™” ë°ì´í„° CSVë¡œ ì €ì¥
        if self.failed_addresses:
            failed_file = output_dir / "failed_addresses.csv"
            df_failed = pd.DataFrame(self.failed_addresses)
            df_failed.to_csv(failed_file, index=False, encoding='utf-8')
            logger.info(f"ì‹¤íŒ¨í•œ ì£¼ì†Œ ì •ê·œí™” ë°ì´í„° ì €ì¥: {failed_file}")
        
        # ë©”íƒ€ë°ì´í„° íŒŒì¼ ì €ì¥
        metadata_file = output_dir / "metadata.json"
        combined_metadata = {
            "normalized_at": datetime.now().isoformat(),
            "public_facilities_count": len(self.normalized_facilities),
            "subway_stations_count": len(self.normalized_subway_stations),
            "failed_addresses_count": len(self.failed_addresses)
        }
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump(combined_metadata, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ì •ê·œí™”ëœ ë°ì´í„° ì €ì¥ ì™„ë£Œ:")
        logger.info(f"  - public_facilities.json: {len(self.normalized_facilities)}ê°œ")
        logger.info(f"  - subway_stations.json: {len(self.normalized_subway_stations)}ê°œ")
        logger.info(f"  - metadata.json: ë©”íƒ€ë°ì´í„°")
        
        return public_facilities_file, subway_stations_file

    def retry_failed_addresses(self, failed_csv_path: Path) -> Dict[str, List[Dict]]:
        """ì‹¤íŒ¨í•œ ì£¼ì†Œë“¤ë§Œ ë‹¤ì‹œ ì •ê·œí™” ì‹œë„"""
        logger.info(f"ì‹¤íŒ¨í•œ ì£¼ì†Œ ì¬ì •ê·œí™” ì‹œì‘: {failed_csv_path}")
        
        # ì‹¤íŒ¨í•œ ì£¼ì†Œ CSV ì½ê¸°
        df_failed = pd.read_csv(failed_csv_path, encoding='utf-8')
        logger.info(f"ì¬ì •ê·œí™” ëŒ€ìƒ: {len(df_failed)}ê°œ")
        
        # ì¬ì •ê·œí™”ëœ ë°ì´í„° ì €ì¥ìš©
        retry_facilities = []
        retry_failed_addresses = []
        
        for idx, row in df_failed.iterrows():
            facility_type = row['facility_type']
            facility_name = row['facility_name']
            address_raw = row['address_raw']
            
            logger.info(f"ì¬ì •ê·œí™” ì‹œë„ [{idx+1}/{len(df_failed)}]: {facility_name} - {address_raw}")
            
            # ì£¼ì†Œ ì •ê·œí™” ì‹œë„
            address_info = self._normalize_address(address_raw, facility_name, facility_type)
            
            if address_info['address_norm']:
                # ì •ê·œí™” ì„±ê³µ - ì‹œì„¤ ë°ì´í„° ìƒì„±
                facility_data = {
                    'facility_id': self._generate_facility_id(facility_type),
                    'cd': self._get_facility_cd(facility_type),
                    'name': facility_name,
                    'address_raw': address_info['address_raw'],
                    'address_norm': address_info['address_norm'],
                    'si_do': address_info['si_do'],
                    'si_gun_gu': address_info['si_gun_gu'],
                    'si_gun_gu_dong': address_info.get('si_gun_gu_dong'),
                    'road_full': address_info.get('road_full'),
                    'jibun_full': address_info.get('jibun_full'),
                    'lat': address_info['lat'],
                    'lon': address_info['lon'],
                    'phone': None,
                    'website': None,
                    'operating_hours': None,
                    'is_24h': False,
                    'is_emergency': False,
                    'capacity': None,
                    'grade_level': None,
                    'facility_extra': {
                        'retry_success': True,
                        'original_failed_reason': row['error_reason']
                    },
                    'data_source': 'openseoul',
                    'geo_extra': address_info['geo_extra']
                }
                retry_facilities.append(facility_data)
                logger.info(f"âœ… ì¬ì •ê·œí™” ì„±ê³µ: {facility_name}")
            else:
                # ì—¬ì „íˆ ì‹¤íŒ¨
                failed_data = {
                    "facility_type": facility_type,
                    "facility_name": facility_name,
                    "address_raw": address_raw,
                    "address_processed": preprocess_address(address_raw),
                    "error_reason": "Retry failed - No match from Juso API",
                    "timestamp": pd.Timestamp.now().isoformat(),
                    "original_failed_reason": row['error_reason']
                }
                retry_failed_addresses.append(failed_data)
                logger.warning(f"âŒ ì¬ì •ê·œí™” ì‹¤íŒ¨: {facility_name}")
        
        logger.info(f"ì¬ì •ê·œí™” ì™„ë£Œ: ì„±ê³µ {len(retry_facilities)}ê°œ, ì‹¤íŒ¨ {len(retry_failed_addresses)}ê°œ")
        
        return {
            "retry_facilities": retry_facilities,
            "retry_failed_addresses": retry_failed_addresses
        }

    def _normalize_sports_facilities(self, file_path: Path):
        """ê³µê³µì²´ìœ¡ì‹œì„¤ ì •ë³´ ì •ê·œí™”"""
        logger.info(f"ê³µê³µì²´ìœ¡ì‹œì„¤ ì •ë³´ ì •ê·œí™” ì‹œì‘: {file_path}")
        
        df = read_csv_with_auto_encoding(file_path, dtype=str)
        logger.info(f"ê³µê³µì²´ìœ¡ì‹œì„¤ ë°ì´í„° ë¡œë“œ: {len(df)}ê°œ")
        
        for _, row in df.iterrows():
            try:
                # ì£¼ì†Œ ì •ê·œí™”
                address_info = self._normalize_address(
                    address_raw=row.get('ì‹œì„¤ì£¼ì†Œ', ''),
                    facility_name=row.get('ì‹œì„¤ëª…', ''),
                    facility_type='gym'
                )
                
                # ì‹œì„¤ ì •ë³´ êµ¬ì„±
                facility = {
                    'facility_id': self._generate_facility_id('gym'),
                    'cd': self._get_facility_cd('gym'),
                    'name': row.get('ì‹œì„¤ëª…', ''),
                    'address_raw': address_info['address_raw'],
                    'address_nm': address_info['address_nm'],
                    'address_id': address_info['address_id'],
                    'lat': address_info['lat'],
                    'lon': address_info['lon'],
                    'phone': row.get('ì—°ë½ì²˜', ''),
                    'website': row.get('í™ˆí˜ì´ì§€', ''),
                    'operating_hours': f"í‰ì¼: {row.get('ìš´ì˜ì‹œê°„_í‰ì¼', '')}, ì£¼ë§: {row.get('ìš´ì˜ì‹œê°„_ì£¼ë§', '')}, ê³µíœ´ì¼: {row.get('ìš´ì˜ì‹œê°„_ê³µíœ´ì¼', '')}",
                    'capacity': self._safe_int(row.get('ì‹œì„¤ê·œëª¨', '')),
                    'facility_extra': {
                        'ì‹œì„¤ìœ í˜•': row.get('ì‹œì„¤ìœ í˜•', ''),
                        'ìš´ì˜ê¸°ê´€': row.get('ìš´ì˜ê¸°ê´€', ''),
                        'ì‹œì„¤ëŒ€ê´€ì—¬ë¶€': row.get('ì‹œì„¤ëŒ€ê´€ì—¬ë¶€', ''),
                        'ì‹œì„¤ì‚¬ìš©ë£Œ': row.get('ì‹œì„¤ì‚¬ìš©ë£Œ', ''),
                        'ì£¼ì°¨ì •ë³´': row.get('ì£¼ì°¨ì •ë³´', ''),
                        'ì‹œì„¤ì¢…ë¥˜': row.get('ì‹œì„¤ì¢…ë¥˜', ''),
                        'ì‹œì„¤ìš´ì˜ìƒíƒœ': row.get('ì‹œì„¤ìš´ì˜ìƒíƒœ', ''),
                        'ì‹œì„¤í¸ì˜ì‹œì„¤': row.get('ì‹œì„¤í¸ì˜ì‹œì„¤', ''),
                        'ë¹„ê³ ': row.get('ë¹„ê³ ', '')
                    },
                    'data_source': 'localdata'
                }
                
                self.normalized_facilities.append(facility)
                
            except Exception as e:
                logger.error(f"ê³µê³µì²´ìœ¡ì‹œì„¤ ì •ê·œí™” ì˜¤ë¥˜: {row.get('ì‹œì„¤ëª…', '')} - {e}")
        
        logger.info(f"ê³µê³µì²´ìœ¡ì‹œì„¤ ì •ê·œí™” ì™„ë£Œ: {len(df)}ê°œ")

    def _normalize_marts(self, file_path: Path):
        """ë§ˆíŠ¸ ì •ë³´ ì •ê·œí™”"""
        logger.info(f"ë§ˆíŠ¸ ì •ë³´ ì •ê·œí™” ì‹œì‘: {file_path}")
        
        df = read_csv_with_auto_encoding(file_path, dtype=str)
        logger.info(f"ë§ˆíŠ¸ ë°ì´í„° ë¡œë“œ: {len(df)}ê°œ")
        
        for _, row in df.iterrows():
            try:
                # ì£¼ì†Œ ì •ê·œí™”
                address_info = self._normalize_address(
                    address_raw=row.get('ì£¼ì†Œ', ''),
                    facility_name=row.get('ìƒí˜¸ëª…', ''),
                    facility_type='mart'
                )
                
                # ì‹œì„¤ ì •ë³´ êµ¬ì„±
                facility = {
                    'facility_id': self._generate_facility_id('mart'),
                    'cd': self._get_facility_cd('mart'),
                    'name': row.get('ìƒí˜¸ëª…', ''),
                    'address_raw': address_info['address_raw'],
                    'address_nm': address_info['address_nm'],
                    'address_id': address_info['address_id'],
                    'lat': address_info['lat'],
                    'lon': address_info['lon'],
                    'phone': row.get('ì „í™”ë²ˆí˜¸', ''),
                    'facility_extra': {
                        'ì—…ì¢…': row.get('ì—…ì¢…', ''),
                        'ìì¹˜êµ¬': row.get('ìì¹˜êµ¬', '')
                    },
                    'data_source': 'localdata'
                }
                
                self.normalized_facilities.append(facility)
                
            except Exception as e:
                logger.error(f"ë§ˆíŠ¸ ì •ê·œí™” ì˜¤ë¥˜: {row.get('ìƒí˜¸ëª…', '')} - {e}")
        
        logger.info(f"ë§ˆíŠ¸ ì •ê·œí™” ì™„ë£Œ: {len(df)}ê°œ")

    def _normalize_convenience_stores(self, file_path: Path):
        """í¸ì˜ì  ì •ë³´ ì •ê·œí™”"""
        logger.info(f"í¸ì˜ì  ì •ë³´ ì •ê·œí™” ì‹œì‘: {file_path}")
        
        df = read_csv_with_auto_encoding(file_path, dtype=str)
        logger.info(f"í¸ì˜ì  ë°ì´í„° ë¡œë“œ: {len(df)}ê°œ")
        
        for _, row in df.iterrows():
            try:
                # ì£¼ì†Œ ì •ê·œí™” (í¸ì˜ì ì€ ì¢Œí‘œê°€ ì´ë¯¸ ìˆìœ¼ë¯€ë¡œ ì¢Œí‘œë³€í™˜ì€ í•˜ì§€ ì•ŠìŒ)
                address_info = self._normalize_address(
                    address_raw=row.get('ì†Œì¬ì§€ì „ì²´ì£¼ì†Œ', ''),
                    facility_name=row.get('ì‚¬ì—…ì¥ëª…', ''),
                    facility_type='convenience'
                )
                
                # ê¸°ì¡´ ì¢Œí‘œ ì‚¬ìš© (EPSG5174 ì¢Œí‘œê³„ë¥¼ WGS84ë¡œ ë³€í™˜í•˜ì§€ ì•Šê³  ê·¸ëŒ€ë¡œ ì‚¬ìš©)
                lat = self._safe_float(row.get('ì¢Œí‘œì •ë³´Y(EPSG5174)'))
                lon = self._safe_float(row.get('ì¢Œí‘œì •ë³´X(EPSG5174)'))
                
                # ì‹œì„¤ ì •ë³´ êµ¬ì„±
                facility = {
                    'facility_id': self._generate_facility_id('convenience'),
                    'cd': self._get_facility_cd('convenience'),
                    'name': row.get('ì‚¬ì—…ì¥ëª…', ''),
                    'address_raw': address_info['address_raw'],
                    'address_nm': address_info['address_nm'],
                    'address_id': address_info['address_id'],
                    'lat': lat,
                    'lon': lon,
                    'phone': row.get('ì†Œì¬ì§€ì „í™”', ''),
                    'website': row.get('í™ˆí˜ì´ì§€', ''),
                    'operating_hours': None,  # ìš´ì˜ì‹œê°„ ì •ë³´ ì—†ìŒ
                    'is_24h': False,  # 24ì‹œê°„ ì •ë³´ ì—†ìŒ
                    'is_emergency': False,
                    'capacity': None,
                    'grade_level': None,
                    'facility_extra': {
                        'ì†Œì¬ì§€ë©´ì ': row.get('ì†Œì¬ì§€ë©´ì ', ''),
                        'ë„ë¡œëª…ì „ì²´ì£¼ì†Œ': row.get('ë„ë¡œëª…ì „ì²´ì£¼ì†Œ', '')
                    },
                    'data_source': 'localdata'
                }
                
                self.normalized_facilities.append(facility)
                
            except Exception as e:
                logger.error(f"í¸ì˜ì  ì •ê·œí™” ì˜¤ë¥˜: {row.get('ì‚¬ì—…ì¥ëª…', '')} - {e}")
        
        logger.info(f"í¸ì˜ì  ì •ê·œí™” ì™„ë£Œ: {len(df)}ê°œ")

    def _normalize_hospitals(self, file_path: Path):
        """ë³‘ì› ì •ë³´ ì •ê·œí™”"""
        logger.info(f"ë³‘ì› ì •ë³´ ì •ê·œí™” ì‹œì‘: {file_path}")
        
        df = read_csv_with_auto_encoding(file_path, dtype=str)
        logger.info(f"ë³‘ì› ë°ì´í„° ë¡œë“œ: {len(df)}ê°œ")
        
        for _, row in df.iterrows():
            try:
                # ì£¼ì†Œ ì •ê·œí™”
                address_info = self._normalize_address(
                    address_raw=row.get('ì£¼ì†Œ', ''),
                    facility_name=row.get('ê¸°ê´€ëª…', ''),
                    facility_type='hospital'
                )
                
                # ì‹œì„¤ ì •ë³´ êµ¬ì„±
                facility = {
                    'facility_id': self._generate_facility_id('hospital'),
                    'cd': self._get_facility_cd('hospital'),
                    'name': row.get('ê¸°ê´€ëª…', ''),
                    'address_raw': address_info['address_raw'],
                    'address_nm': address_info['address_nm'],
                    'address_id': address_info['address_id'],
                    'lat': address_info['lat'],
                    'lon': address_info['lon'],
                    'phone': row.get('ì „í™”ë²ˆí˜¸', ''),
                    'is_emergency': 'ì‘ê¸‰' in str(row.get('ì§„ë£Œê³¼ëª©', '')),
                    'facility_extra': {
                        'ì§„ë£Œê³¼ëª©': row.get('ì§„ë£Œê³¼ëª©', ''),
                        'ìì¹˜êµ¬': row.get('ìì¹˜êµ¬', ''),
                        'ì˜ë£Œê¸°ê´€ì¢…ë³„': row.get('ì˜ë£Œê¸°ê´€ì¢…ë³„', '')
                    },
                    'data_source': 'localdata'
                }
                
                self.normalized_facilities.append(facility)
                
            except Exception as e:
                logger.error(f"ë³‘ì› ì •ê·œí™” ì˜¤ë¥˜: {row.get('ê¸°ê´€ëª…', '')} - {e}")
        
        logger.info(f"ë³‘ì› ì •ê·œí™” ì™„ë£Œ: {len(df)}ê°œ")

# ì˜ˆì‹œ ì‚¬ìš©ë²• (CLIì—ì„œ í˜¸ì¶œë  ë•Œ)
if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    # í”„ë¡œì íŠ¸ ë£¨íŠ¸ì—ì„œ ì‹¤í–‰í•œë‹¤ê³  ê°€ì •
    project_root = Path(__file__).resolve().parents[4]
    openseoul_data_path = project_root / "backend" / "data" / "public-api" / "openseoul"
    
    normalizer = InfraNormalizer(data_dir=openseoul_data_path)
    normalized_data = normalizer.normalize_openseoul_data()
    
    print("\n--- ì •ê·œí™”ëœ ê³µê³µì‹œì„¤ ë°ì´í„° (ì¼ë¶€) ---")
    for i, facility in enumerate(normalized_data["public_facilities"][:5]):
        print(f"{i+1}. {facility['name']} (Category ID: {facility.get('category_id', 'N/A')})")

    print("\n--- ì •ê·œí™”ëœ ì§€í•˜ì² ì—­ ë°ì´í„° (ì¼ë¶€) ---")
    for i, station in enumerate(normalized_data["subway_stations"][:5]):
        print(f"{i+1}. {station['station_name']} (Line: {station['line_name']})")

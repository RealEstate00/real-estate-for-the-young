"""
LLM ëª¨ë¸ ì„¤ì • - Fallback ì²´ì¸ íŒ¨í„´
- ìë™ ë°±ì—…: Groq â†’ HuggingFace â†’ OpenAI
- ë¬´ë£Œ í† í° ì†Œì§„ ì‹œ ìë™ ëª¨ë¸ ì „í™˜
"""

import os
from dotenv import load_dotenv
from langchain_groq import ChatGroq
from langchain_openai import ChatOpenAI
<<<<<<<< Updated upstream:backend/services/llm/inha/models/llm.py
# from langchain_huggingface import ChatHuggingFace, HuggingFacePipeline
# from transformers import pipeline
========
from langchain_huggingface import ChatHuggingFace, HuggingFacePipeline
from transformers import pipeline
import transformers
import torch 

>>>>>>>> Stashed changes:backend/services/llm/inha/models/inha/llm_inha.py

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
# GPU ìˆìœ¼ë©´ GPU(ì–‘ìˆ˜), ì—†ìœ¼ë©´ CPU(0,ìŒìˆ˜)
device = 0 if torch.cuda.is_available() else -1


# ê¸°ë³¸llm ëª¨ë“œ ì„¤ì •
FORCE_LLM_PROVIDER = os.getenv("FORCE_LLM_PROVIDER", "huggingface")  # groq, huggingface, openai ë“± .envíŒŒì¼ì—ì„œ ì„¤ì •í•˜ë©´ ë¨
# í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë“œ ì„¤ì •
# USE_HYBRID = os.getenv("USE_HYBRID_LLM", "false").lower() == "true"
# í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë“œ ì„¤ì • (ë¹„í™œì„±í™”)
USE_HYBRID = False


openai_llm = ChatOpenAI(
    model="gpt-4o-mini",
    temperature=0.3,
    max_tokens=1000,
    api_key=os.getenv("OPENAI_API_KEY")
)

groq_llm = ChatGroq(
    model="llama-3.3-70b-versatile",
    temperature=0.5,
    max_tokens=2000,
    api_key=os.getenv("GROQ_API_KEY")
)

# ê³µì‹ ë¬¸ì„œ ë°©ì‹ìœ¼ë¡œ ìˆ˜ì • - transformers.pipeline ì§ì ‘ ì‚¬ìš©
# ë¡œì»¬ CPUìš© ê²½ëŸ‰ ëª¨ë¸ ì„ íƒ (í•œêµ­ì–´ ì§€ì›)
MODEL_CHOICES = {
    "korean_electra": "beomi/KcELECTRA-base",  # í•œêµ­ì–´ ELECTRA (ì¶”ì²œ)
    "korean_bert": "beomi/KcBERT-base",        # í•œêµ­ì–´ BERT
    "klue_roberta": "klue/roberta-base",       # KLUE í•œêµ­ì–´ ëª¨ë¸
    "dialogpt": "microsoft/DialoGPT-small",    # ëŒ€í™”í˜• ìƒì„±
    "gpt2": "gpt2",                            # ê¸°ë³¸ GPT-2
    "distilgpt2": "distilgpt2",                # ê²½ëŸ‰ GPT-2
}

# ì‚¬ìš©í•  ëª¨ë¸ ì§ì ‘ ì„ íƒ (ì½”ë“œì—ì„œ ì„¤ì •)
SELECTED_MODEL = "korean_electra"  # ì›í•˜ëŠ” ëª¨ë¸ë¡œ ë³€ê²½ ê°€ëŠ¥
model_name = MODEL_CHOICES.get(SELECTED_MODEL, MODEL_CHOICES["korean_electra"])

print(f"ğŸ”„ ë¡œì»¬ CPU ëª¨ë¸ ë¡œë”©: {model_name}")

# ê³µì‹ ë¬¸ì„œ ë°©ì‹ìœ¼ë¡œ pipeline ìƒì„± (CPU ìµœì í™”)
hf_pipeline = transformers.pipeline(
    "text-generation",
    model=model_name,
    model_kwargs={
        "torch_dtype": torch.float32,  # CPUìš© float32 ì‚¬ìš©
        "use_auth_token": os.getenv("HUGGINGFACEHUB_API_TOKEN")  # HuggingFace í† í° ì¶”ê°€
        #"device_map": "cpu",           # CPU ê°•ì œ ì‚¬ìš©
    },
    cache_dir=".\backend\data\models\hf",  # ëª¨ë¸ì´ ì €ì¥ë  í´ë” ì§€ì •
)

# ê³µì‹ ë¬¸ì„œì²˜ëŸ¼ eval ëª¨ë“œ ì„¤ì •
hf_pipeline.model.eval()
# HuggingFace pipeline ê°ì²´ë¥¼ ê·¸ëƒ¥ ì „ë‹¬í•˜ë©´ langchain ì—ì„œ ì§ì ‘ ì§€ì›í•˜ëŠ” ì¸í„°í˜ì´ìŠ¤ì™€ ë‹¤ë¥¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ,
# HuggingFacePipelineìœ¼ë¡œ í•œ ë²ˆ ë˜í•‘(wrap)í•´ì„œ ì „ë‹¬í•´ì•¼ ChatHuggingFaceì—ì„œ ì¼ê´€ëœ LLM interfaceë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.
# ì¦‰, HuggingFacePipelineì€ transformersì˜ pipelineì„ langchainì˜ LLM ê°ì²´ë¡œ ë³€í™˜í•´ì£¼ëŠ” ì–´ëŒ‘í„° ì—­í• ì„ í•œë‹¤.
huggingface_llm = ChatHuggingFace(
    llm=HuggingFacePipeline(pipeline=hf_pipeline)
    )


<<<<<<<< Updated upstream:backend/services/llm/inha/models/llm.py
openai_llm = ChatOpenAI(
    model="gpt-4o-mini",
    temperature=0.3,
    max_tokens=2000,
    api_key=os.getenv("OPENAI_API_KEY")
)
========
>>>>>>>> Stashed changes:backend/services/llm/inha/models/inha/llm_inha.py



def _create_specific_model(provider):
    """íŠ¹ì • ì œê³µìì˜ ëª¨ë¸ ìƒì„±"""
    if provider == 'groq':
        return groq_llm
    # elif provider == 'huggingface':
    #     return huggingface_llm
    elif provider == 'openai':
        return openai_llm
    else:
        raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì œê³µì: {provider}")

def create_llm(force_provider=None):
    """ëª¨ë¸ ì œê³µì ê°•ì œ ì§€ì • ë˜ëŠ” ìë™ ì„ íƒ"""
    
    if force_provider:
        print(f"ğŸ”„ {force_provider} ëª¨ë¸ ê°•ì œ ì‚¬ìš©")
        return _create_specific_model(force_provider)
    
    # ìë™ ì„ íƒ: Groq â†’ OpenAI (HuggingFace ì œì™¸)
    providers = ['groq', 'openai']
    
    for provider in providers:
        try:
            print(f"ğŸ”„ {provider} ëª¨ë¸ ì—°ê²° ì‹œë„...")
            model = _create_specific_model(provider)
            print(f"âœ… {provider} ëª¨ë¸ ì—°ê²° ì„±ê³µ")
            return model
        except Exception as e:
            print(f"âš ï¸ {provider} ëª¨ë¸ ì—°ê²° ì‹¤íŒ¨: {e}")
            continue
    
    raise ValueError("ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤")

def create_llm_with_fallback():
    """Groq â†’ OpenAI ìë™ ì „í™˜ (HuggingFace ì œì™¸)"""
    
    # 1ì°¨: Groq ì‹œë„
    try:
        groq_key = os.getenv("GROQ_API_KEY")
        if groq_key:
            print("ğŸ”„ Groq ëª¨ë¸ ì—°ê²° ì‹œë„...")
            return groq_llm
    except Exception as e:
        print(f"âš ï¸ Groq ì—°ê²° ì‹¤íŒ¨: {e}")
    
    # 2ì°¨: OpenAI ë°±ì—…
    try:
        print("ğŸ”„ OpenAI ë°±ì—… ëª¨ë¸ ì—°ê²° ì‹œë„...")
        return openai_llm
    except Exception as e:
        print(f"âŒ ëª¨ë“  ëª¨ë¸ ì—°ê²° ì‹¤íŒ¨: {e}")
        raise

# LLM ëª¨ë¸ ì„¤ì •
print("LLM ëª¨ë¸ ì—°ê²° ì¤‘...")

# ê¸°ë³¸ LLM ìƒì„± (ìë™ ë°±ì—… í¬í•¨)
llm = create_llm(FORCE_LLM_PROVIDER)

# í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë“œ ì„¤ì •
if USE_HYBRID:
    print(f"ğŸ”€ í•˜ì´ë¸Œë¦¬ë“œ LLM ëª¨ë“œ í™œì„±í™”")
    
    # Agent LLM (ë„êµ¬ í˜¸ì¶œ ì „ìš©) - OpenAI ìš°ì„ 
    try:
        agent_llm = openai_llm
        print(f"  â”œâ”€ Agent LLM: gpt-4o-mini (OpenAI)")
    except Exception as e:
        print(f"âš ï¸ OpenAI Agent LLM ì‹¤íŒ¨, Groq ì‚¬ìš©: {e}")
        agent_llm = llm
        print(f"  â”œâ”€ Agent LLM: llama-3.3-70b-versatile (Groq)")
    
    # Response LLM (ë‹µë³€ ìƒì„± ì „ìš©) - ê¸°ë³¸ LLM ì‚¬ìš©
    response_llm = llm
    print(f"  â””â”€ Response LLM: {llm.__class__.__name__}")
else:
    print(f"âš™ï¸  ë‹¨ì¼ LLM ëª¨ë“œ")
    # ë‹¨ì¼ ëª¨ë“œ: ëª¨ë“  LLMì„ ê¸°ë³¸ llmìœ¼ë¡œ ì„¤ì •
    agent_llm = llm
    response_llm = llm

print("âœ… LLM ì„¤ì • ì™„ë£Œ!")

# í† í° ë¶€ì¡± ê°ì§€ ì‹œ ëª¨ë¸ ì „í™˜ í•¨ìˆ˜
def handle_token_limit_exceeded():
    """í† í° í•œë„ ë„ë‹¬ ì‹œ OpenAIë¡œ ì „í™˜"""
    print("ğŸ”„ í† í° í•œë„ ë„ë‹¬, OpenAIë¡œ ì „í™˜...")
    global llm, agent_llm, response_llm
    
    try:
        new_llm = create_llm('openai')
        llm = new_llm
        agent_llm = new_llm
        response_llm = new_llm
        print("âœ… OpenAI ëª¨ë¸ë¡œ ì „í™˜ ì™„ë£Œ")
        return llm
    except Exception as e:
        print(f"âŒ OpenAI ì „í™˜ ì‹¤íŒ¨: {e}")
        raise

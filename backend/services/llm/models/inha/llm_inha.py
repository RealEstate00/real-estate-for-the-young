"""
LLM ëª¨ë¸ ì„¤ì • - Fallback ì²´ì¸ íŒ¨í„´
- ìë™ ë°±ì—…: Groq â†’ HuggingFace â†’ OpenAI
- ë¬´ë£Œ í† í° ì†Œì§„ ì‹œ ìë™ ëª¨ë¸ ì „í™˜
"""

import os
from dotenv import load_dotenv
from langchain_groq import ChatGroq
from langchain_openai import ChatOpenAI
from langchain_huggingface import ChatHuggingFace, HuggingFacePipeline
from transformers import pipeline

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ê¸°ë³¸llm ëª¨ë“œ ì„¤ì •
FORCE_LLM_PROVIDER = os.getenv("FORCE_LLM_PROVIDER", "openai")  # groq, huggingface, openai ë“± .envíŒŒì¼ì—ì„œ ì„¤ì •í•˜ë©´ ë¨

# í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë“œ ì„¤ì •
USE_HYBRID = os.getenv("USE_HYBRID_LLM", "false").lower() == "true"



groq_llm = ChatGroq(
    model="llama-3.3-70b-versatile",
    temperature=0.5,
    max_tokens=800,
    api_key=os.getenv("GROQ_API_KEY")
)


# ì¢‹ì€ ëª¨ë¸ ì¨ë³´ë ¤í•˜ë‹ˆ ì•ˆë¨. ì™œ ê·¸ëŸ°ì§€ ë¬¼ì–´ë³´ê¸°
# hf_pipeline = pipeline(
#     "text-generation",
#     model="MLP-KTLim/llama-3-Korean-Bllossom-8B",
#     max_new_tokens=1000,
#     temperature=0.7,
#     return_full_text=False,
#     truncation=True,
#     max_length=8192,  # ëª¨ë¸ì˜ max_position_embeddings(ì‹¤ì œ ìµœëŒ€ í† í° ê¸¸ì´)ì— ë§ì¶¤
#     do_sample=True,
#     top_p=0.9,
#     repetition_penalty=1.1,
#     # íŠ¹ë³„í•œ í† í° ì„¤ì •
#     pad_token_id=128009,  # eos_token_id ì‚¬ìš©
#     eos_token_id=128009,
#     bos_token_id=128000,
#     # ë°ì´í„° íƒ€ì… ì„¤ì • - ëª¨ë¸ì˜ ë°ì´í„° íƒ€ì… ë§ì¶¤ 
#     torch_dtype="bfloat16",
#     # ë©”ëª¨ë¦¬ ìµœì í™”
#     device_map="auto", # ë©”ëª¨ë¦¬ ìë™ ê´€ë¦¬
#     low_cpu_mem_usage=True
# )

# HuggingFace pipeline ê°ì²´ë¥¼ ê·¸ëƒ¥ ì „ë‹¬í•˜ë©´ langchain ì—ì„œ ì§ì ‘ ì§€ì›í•˜ëŠ” ì¸í„°í˜ì´ìŠ¤ì™€ ë‹¤ë¥¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ,
# HuggingFacePipelineìœ¼ë¡œ í•œ ë²ˆ ë˜í•‘(wrap)í•´ì„œ ì „ë‹¬í•´ì•¼ ChatHuggingFaceì—ì„œ ì¼ê´€ëœ LLM interfaceë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.
# ì¦‰, HuggingFacePipelineì€ transformersì˜ pipelineì„ langchainì˜ LLM ê°ì²´ë¡œ ë³€í™˜í•´ì£¼ëŠ” ì–´ëŒ‘í„° ì—­í• ì„ í•œë‹¤.
# huggingface_llm = ChatHuggingFace(
#     llm=HuggingFacePipeline(pipeline=hf_pipeline)
# )

openai_llm = ChatOpenAI(
    model="gpt-4o-mini",
    temperature=0.3,
    max_tokens=1000,
    api_key=os.getenv("OPENAI_API_KEY")
)



def _create_specific_model(provider):
    """íŠ¹ì • ì œê³µìì˜ ëª¨ë¸ ìƒì„±"""
    if provider == 'groq':
        return groq_llm
    elif provider == 'huggingface':
        return huggingface_llm
    elif provider == 'openai':
        return openai_llm
    else:
        raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì œê³µì: {provider}")

def create_llm(force_provider=None):
    """ëª¨ë¸ ì œê³µì ê°•ì œ ì§€ì • ë˜ëŠ” ìë™ ì„ íƒ"""
    
    if force_provider:
        print(f"ğŸ”„ {force_provider} ëª¨ë¸ ê°•ì œ ì‚¬ìš©")
        return _create_specific_model(force_provider)
    
    # ìë™ ì„ íƒ: Groq â†’ HuggingFace â†’ OpenAI
    providers = ['groq', 'huggingface', 'openai']
    
    for provider in providers:
        try:
            print(f"ğŸ”„ {provider} ëª¨ë¸ ì—°ê²° ì‹œë„...")
            model = _create_specific_model(provider)
            print(f"âœ… {provider} ëª¨ë¸ ì—°ê²° ì„±ê³µ")
            return model
        except Exception as e:
            print(f"âš ï¸ {provider} ëª¨ë¸ ì—°ê²° ì‹¤íŒ¨: {e}")
            continue
    
    raise ValueError("ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤")

def create_llm_with_fallback():
    """Groq â†’ HuggingFace ìë™ ì „í™˜"""
    
    # 1ì°¨: Groq ì‹œë„
    try:
        groq_key = os.getenv("GROQ_API_KEY")
        if groq_key:
            print("ğŸ”„ Groq ëª¨ë¸ ì—°ê²° ì‹œë„...")
            return groq_llm
    except Exception as e:
        print(f"âš ï¸ Groq ì—°ê²° ì‹¤íŒ¨: {e}")
    
    # 2ì°¨: HuggingFace ë°±ì—…
    try:
        print("ğŸ”„ HuggingFace ë°±ì—… ëª¨ë¸ ì—°ê²° ì‹œë„...")
        return huggingface_llm
    except Exception as e:
        print(f"âŒ ëª¨ë“  ëª¨ë¸ ì—°ê²° ì‹¤íŒ¨: {e}")
        raise

# LLM ëª¨ë¸ ì„¤ì •
print("LLM ëª¨ë¸ ì—°ê²° ì¤‘...")

# ê¸°ë³¸ LLM ìƒì„± (ìë™ ë°±ì—… í¬í•¨)
llm = create_llm(FORCE_LLM_PROVIDER)

# í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë“œ ì„¤ì •
if USE_HYBRID:
    print(f"ğŸ”€ í•˜ì´ë¸Œë¦¬ë“œ LLM ëª¨ë“œ í™œì„±í™”")
    
    # Agent LLM (ë„êµ¬ í˜¸ì¶œ ì „ìš©) - OpenAI ìš°ì„ 
    try:
        agent_llm = openai_llm
        print(f"  â”œâ”€ Agent LLM: gpt-4o-mini (OpenAI)")
    except Exception as e:
        print(f"âš ï¸ OpenAI Agent LLM ì‹¤íŒ¨, Groq ì‚¬ìš©: {e}")
        agent_llm = llm
        print(f"  â”œâ”€ Agent LLM: llama-3.3-70b-versatile (Groq)")
    
    # Response LLM (ë‹µë³€ ìƒì„± ì „ìš©) - ê¸°ë³¸ LLM ì‚¬ìš©
    response_llm = llm
    print(f"  â””â”€ Response LLM: {llm.__class__.__name__}")
else:
    print(f"âš™ï¸  ë‹¨ì¼ LLM ëª¨ë“œ")
    # ë‹¨ì¼ ëª¨ë“œ: ëª¨ë“  LLMì„ ê¸°ë³¸ llmìœ¼ë¡œ ì„¤ì •
    agent_llm = llm
    response_llm = llm

print("âœ… LLM ì„¤ì • ì™„ë£Œ!")

# í† í° ë¶€ì¡± ê°ì§€ ì‹œ ëª¨ë¸ ì „í™˜ í•¨ìˆ˜
def handle_token_limit_exceeded():
    """í† í° í•œë„ ë„ë‹¬ ì‹œ HuggingFaceë¡œ ì „í™˜"""
    print("ğŸ”„ í† í° í•œë„ ë„ë‹¬, HuggingFaceë¡œ ì „í™˜...")
    global llm, agent_llm, response_llm
    
    try:
        new_llm = create_llm('huggingface')
        llm = new_llm
        agent_llm = new_llm
        response_llm = new_llm
        print("âœ… HuggingFace ëª¨ë¸ë¡œ ì „í™˜ ì™„ë£Œ")
        return llm
    except Exception as e:
        print(f"âŒ HuggingFace ì „í™˜ ì‹¤íŒ¨: {e}")
        raise

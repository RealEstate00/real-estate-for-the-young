"""
LLM ëª¨ë¸ ì„¤ì •
- ë‹¨ì¼ ëª¨ë“œ: Groq gemma2-9b-it
- í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë“œ: OpenAI GPT-4o-mini (Agent) + Groq gemma2-9b-it (Response)
"""

import os
from dotenv import load_dotenv
from langchain_groq import ChatGroq
from langchain_openai import ChatOpenAI

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# API í‚¤
groq_api_key = os.getenv("GROQ_API_KEY")
openai_api_key = os.getenv("OPENAI_API_KEY")

# í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë“œ ì„¤ì •
USE_HYBRID = os.getenv("USE_HYBRID_LLM", "False").lower() == "true"
AGENT_MODEL = os.getenv("AGENT_MODEL", "gpt-4o-mini")  # OpenAI ê¸°ë³¸ê°’
AGENT_PROVIDER = os.getenv("AGENT_PROVIDER", "openai")  # openai or groq
RESPONSE_MODEL = os.getenv("RESPONSE_MODEL", "gemma2-9b-it")

# LLM ëª¨ë¸ ì„¤ì •
print("LLM ëª¨ë¸ ì—°ê²° ì¤‘...")

# Groq API í‚¤ ì²´í¬
if not groq_api_key:
    print("âŒ GROQ_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    print("Groq ê³„ì •ì—ì„œ API í‚¤ë¥¼ ë°œê¸‰ë°›ì•„ .env íŒŒì¼ì— ì„¤ì •í•´ì£¼ì„¸ìš”.")
    print("í† í° ë°œê¸‰: https://console.groq.com/keys")
    raise ValueError("Groq API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤.")

print(f"âœ… Groq API í‚¤ ë°œê²¬: {groq_api_key[:10]}...")

# í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë“œì—ì„œ OpenAI ì‚¬ìš© ì‹œ API í‚¤ ì²´í¬
if USE_HYBRID and AGENT_PROVIDER == "openai":
    if not openai_api_key:
        print("âŒ OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("OpenAI ê³„ì •ì—ì„œ API í‚¤ë¥¼ ë°œê¸‰ë°›ì•„ .env íŒŒì¼ì— ì„¤ì •í•´ì£¼ì„¸ìš”.")
        print("í† í° ë°œê¸‰: https://platform.openai.com/api-keys")
        raise ValueError("OpenAI API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
    print(f"âœ… OpenAI API í‚¤ ë°œê²¬: {openai_api_key[:10]}...")

# LLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
try:
    # ê¸°ë³¸ LLM (ë‹¨ì¼ ëª¨ë“œìš© - Groq)
    llm = ChatGroq(
        model="gemma2-9b-it",
        temperature=0.5,
        max_tokens=1000,
        api_key=groq_api_key
    )

    # í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë“œ ì„¤ì •
    if USE_HYBRID:
        print(f"ğŸ”€ í•˜ì´ë¸Œë¦¬ë“œ LLM ëª¨ë“œ í™œì„±í™”")

        # Agent LLM (ë„êµ¬ í˜¸ì¶œ ì „ìš©)
        if AGENT_PROVIDER == "openai":
            agent_llm = ChatOpenAI(
                model=AGENT_MODEL,
                temperature=0.3,  # ë‚®ì€ temperature: ì •í™•í•œ ë„êµ¬ í˜¸ì¶œ
                max_tokens=2000,
                api_key=openai_api_key
            )
            print(f"  â”œâ”€ Agent LLM: {AGENT_MODEL} (OpenAI)")
        else:
            agent_llm = ChatGroq(
                model=AGENT_MODEL,
                temperature=0.3,
                max_tokens=2000,
                api_key=groq_api_key
            )
            print(f"  â”œâ”€ Agent LLM: {AGENT_MODEL} (Groq)")

        # Response LLM (ë‹µë³€ ìƒì„± ì „ìš© - í•­ìƒ Groq)
        response_llm = ChatGroq(
            model=RESPONSE_MODEL,
            temperature=0.7,  # ë†’ì€ temperature: ìì—°ìŠ¤ëŸ¬ìš´ ë‹µë³€
            max_tokens=1000,
            api_key=groq_api_key
        )
        print(f"  â””â”€ Response LLM: {RESPONSE_MODEL} (Groq)")
    else:
        print(f"âš™ï¸  ë‹¨ì¼ LLM ëª¨ë“œ (gemma2-9b-it)")
        # ë‹¨ì¼ ëª¨ë“œ: ëª¨ë“  LLMì„ ê¸°ë³¸ llmìœ¼ë¡œ ì„¤ì •
        agent_llm = llm
        response_llm = llm

    print("âœ… LLM ì„¤ì • ì™„ë£Œ!")

except Exception as e:
    print(f"âŒ LLM ìƒì„± ì‹¤íŒ¨: {e}")
    raise

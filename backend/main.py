#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
ì„œìš¸ì£¼íƒë„ì‹œê³µì‚¬ ì£¼íƒì§€ì› ë´‡ - í†µí•© ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
í¬ë¡¤ë§, ë°ì´í„° ë¶„ì„, ë³€í™˜ì„ ëª¨ë‘ ì²˜ë¦¬
"""

import argparse
import sys
import pandas as pd
import json
from pathlib import Path
from datetime import datetime

# Add data_collection/crawler directory to path for imports
sys.path.append(str(Path(__file__).parent / "data_collection" / "crawler"))

from services.crawlers.base import Progress
# ì €ì¥ ê²½ë¡œ ê´€ë ¨ í•¨ìˆ˜ë“¤ import
from utils.helpers import clean_today, clean_all_platform_data, run_dir
from services.crawlers.so_co import SoHouseCrawler, CoHouseCrawler
from services.crawlers.youth import YouthCrawler
from services.crawlers.sh import HappyHousingCrawler, SHAnnouncementCrawler
from services.crawlers.lh import LHAnnouncementCrawler
from services.parsers.data_analyzer import RawDataAnalyzer


def crawl_platform(platform: str, fresh: bool = False, max_youth_bbs: int = 0):
    """í”Œë«í¼ë³„ í¬ë¡¤ë§ ì‹¤í–‰"""
    progress = Progress()
    
    if platform == "sohouse":
        # sohouse í¬ë¡¤ë§ ì‹œ ì €ì¥ ê²½ë¡œ ì„¤ì •
        if fresh: 
            clean_all_platform_data("sohouse")  # ê¸°ì¡´ sohouse ë°ì´í„° ì‚­ì œ (data/raw í´ë”)
            print("[FRESH] ê¸°ì¡´ sohouse ë°ì´í„° ì‚­ì œ ì™„ë£Œ")
        out_dir = run_dir("sohouse")  # data/raw/YYYY-MM-DD/sohouse/ ë””ë ‰í† ë¦¬ ìƒì„±
        
        # EPIPE ì˜¤ë¥˜ ì¬ì‹œë„ ë¡œì§
        max_retries = 3
        for retry in range(max_retries):
            try:
                crawler = SoHouseCrawler(progress)
                crawler.crawl()
                break
            except BrokenPipeError as e:
                if retry < max_retries - 1:
                    print(f"[RETRY {retry + 1}] EPIPE ì˜¤ë¥˜ ì¬ì‹œë„: {e}")
                    continue
                else:
                    raise e
                    
    elif platform == "cohouse":
        # cohouse í¬ë¡¤ë§ ì‹œ ì €ì¥ ê²½ë¡œ ì„¤ì •
        if fresh: 
            clean_all_platform_data("cohouse")  # ê¸°ì¡´ cohouse ë°ì´í„° ì‚­ì œ (data/raw í´ë”)
            print("[FRESH] ê¸°ì¡´ cohouse ë°ì´í„° ì‚­ì œ ì™„ë£Œ")
        out_dir = run_dir("cohouse")  # data/raw/YYYY-MM-DD/cohouse/ ë””ë ‰í† ë¦¬ ìƒì„±
        
        # EPIPE ì˜¤ë¥˜ ì¬ì‹œë„ ë¡œì§
        max_retries = 3
        for retry in range(max_retries):
            try:
                crawler = CoHouseCrawler(progress)
                crawler.crawl()
                break
            except BrokenPipeError as e:
                if retry < max_retries - 1:
                    print(f"[RETRY {retry + 1}] EPIPE ì˜¤ë¥˜ ì¬ì‹œë„: {e}")
                    continue
                else:
                    raise e
                    
    elif platform == "youth":
        # youth í¬ë¡¤ë§ ì‹œ ì €ì¥ ê²½ë¡œ ì„¤ì •
        if fresh: 
            clean_today("youth")  # ì˜¤ëŠ˜ ë‚ ì§œì˜ youth ë°ì´í„°ë§Œ ì‚­ì œ (data/raw í´ë”)
            print("[FRESH] ê¸°ì¡´ youth ë°ì´í„° ì‚­ì œ ì™„ë£Œ")
        out_dir = run_dir("youth")  # data/raw/YYYY-MM-DD/youth/ ë””ë ‰í† ë¦¬ ìƒì„±
        
        # EPIPE ì˜¤ë¥˜ ì¬ì‹œë„ ë¡œì§
        max_retries = 3
        for retry in range(max_retries):
            try:
                crawler = YouthCrawler(progress)
                crawler.crawl()
                break
            except BrokenPipeError as e:
                if retry < max_retries - 1:
                    print(f"[RETRY {retry + 1}] EPIPE ì˜¤ë¥˜ ì¬ì‹œë„: {e}")
                    continue
                else:
                    raise e
                    
    elif platform == "happy":
        # happy í¬ë¡¤ë§ ì‹œ ì €ì¥ ê²½ë¡œ ì„¤ì •
        if fresh: 
            clean_today("happy")  # ì˜¤ëŠ˜ ë‚ ì§œì˜ happy ë°ì´í„°ë§Œ ì‚­ì œ (data/raw í´ë”)
            print("[FRESH] ê¸°ì¡´ happy ë°ì´í„° ì‚­ì œ ì™„ë£Œ")
        out_dir = run_dir("happy")  # data/raw/YYYY-MM-DD/happy/ ë””ë ‰í† ë¦¬ ìƒì„±
        
        # EPIPE ì˜¤ë¥˜ ì¬ì‹œë„ ë¡œì§
        max_retries = 3
        for retry in range(max_retries):
            try:
                crawler = HappyHousingCrawler(progress)
                crawler.crawl()
                break
            except BrokenPipeError as e:
                if retry < max_retries - 1:
                    print(f"[RETRY {retry + 1}] EPIPE ì˜¤ë¥˜ ì¬ì‹œë„: {e}")
                    continue
                else:
                    raise e
                    
    elif platform == "lh-ann":
        # lh-ann í¬ë¡¤ë§ ì‹œ ì €ì¥ ê²½ë¡œ ì„¤ì •
        if fresh: 
            clean_today("lh-ann")  # ì˜¤ëŠ˜ ë‚ ì§œì˜ lh-ann ë°ì´í„°ë§Œ ì‚­ì œ (data/raw í´ë”)
            print("[FRESH] ê¸°ì¡´ lh-ann ë°ì´í„° ì‚­ì œ ì™„ë£Œ")
        out_dir = run_dir("lh-ann")  # data/raw/YYYY-MM-DD/lh-ann/ ë””ë ‰í† ë¦¬ ìƒì„±
        
        # EPIPE ì˜¤ë¥˜ ì¬ì‹œë„ ë¡œì§
        max_retries = 3
        for retry in range(max_retries):
            try:
                crawler = LHAnnouncementCrawler(progress)
                crawler.crawl()
                break
            except BrokenPipeError as e:
                if retry < max_retries - 1:
                    print(f"[RETRY {retry + 1}] EPIPE ì˜¤ë¥˜ ì¬ì‹œë„: {e}")
                    continue
                else:
                    raise e
                    
    elif platform == "sh-ann":
        # sh-ann í¬ë¡¤ë§ ì‹œ ì €ì¥ ê²½ë¡œ ì„¤ì •
        if fresh: 
            clean_today("sh-ann")  # ì˜¤ëŠ˜ ë‚ ì§œì˜ sh-ann ë°ì´í„°ë§Œ ì‚­ì œ (data/raw í´ë”)
            print("[FRESH] ê¸°ì¡´ sh-ann ë°ì´í„° ì‚­ì œ ì™„ë£Œ")
        out_dir = run_dir("sh-ann")  # data/raw/YYYY-MM-DD/sh-ann/ ë””ë ‰í† ë¦¬ ìƒì„±
        
        # EPIPE ì˜¤ë¥˜ ì¬ì‹œë„ ë¡œì§
        max_retries = 3
        for retry in range(max_retries):
            try:
                crawler = SHAnnouncementCrawler(progress)
                crawler.crawl()
                break
            except BrokenPipeError as e:
                if retry < max_retries - 1:
                    print(f"[RETRY {retry + 1}] EPIPE ì˜¤ë¥˜ ì¬ì‹œë„: {e}")
                    continue
                else:
                    raise e
    else:
        print(f"[ERROR] ì§€ì›í•˜ì§€ ì•ŠëŠ” í”Œë«í¼: {platform}")
        return False
    
    print(f"[SUCCESS] {platform} í¬ë¡¤ë§ ì™„ë£Œ!")
    return True


def extract_price_range(price_str):
    """ê°€ê²© ë¬¸ìì—´ì—ì„œ ìµœì†Œ/ìµœëŒ€ê°’ ì¶”ì¶œ"""
    if not price_str or price_str.strip() == "":
        return None, None
    
    import re
    numbers = re.findall(r'[\d,]+', price_str.replace('ì›', ''))
    if not numbers:
        return None, None
    
    # ë¹ˆ ë¬¸ìì—´ ì œê±°í•˜ê³  ìˆ«ìë¡œ ë³€í™˜
    clean_numbers = []
    for num in numbers:
        cleaned = num.replace(',', '').strip()
        if cleaned and cleaned.isdigit():
            clean_numbers.append(int(cleaned))
    
    if not clean_numbers:
        return None, None
    
    return min(clean_numbers), max(clean_numbers)

def count_files_from_paths(paths_str, file_type):
    """ê²½ë¡œ ë¬¸ìì—´ì—ì„œ íŒŒì¼ ê°œìˆ˜ ê³„ì‚°"""
    if not paths_str or (isinstance(paths_str, str) and paths_str.strip() == ""):
        return 0
    
    # floatë‚˜ ë‹¤ë¥¸ íƒ€ì…ì¸ ê²½ìš° ë¬¸ìì—´ë¡œ ë³€í™˜
    if not isinstance(paths_str, str):
        paths_str = str(paths_str) if paths_str else ""
    
    if not paths_str or paths_str.strip() == "":
        return 0
    
    paths = [p.strip() for p in paths_str.split(';') if p.strip()]
    return len([p for p in paths if file_type in p.lower()])

def analyze_data():
    """ë°ì´í„° ë¶„ì„ ì‹¤í–‰"""
    print("ğŸ” RAW ë°ì´í„° ë¶„ì„ ì‹œì‘...")
    
    # í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê¸°ì¤€ìœ¼ë¡œ data/raw ê²½ë¡œ ì„¤ì • (ë¶„ì„í•  ë°ì´í„° ìœ„ì¹˜)
    data_root = Path(__file__).parent / "data" / "raw"
    analyzer = RawDataAnalyzer(str(data_root))
    results = analyzer.analyze_all_platforms()
    
    if results:
        analyzer.print_summary()
        analyzer.save_analysis_report()
        print("\nâœ… ë¶„ì„ ì™„ë£Œ!")
        print("ğŸ“„ ìƒì„¸ ë³´ê³ ì„œ: data_analysis_report.json")
        return True
    else:
        print("âŒ ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return False


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(
        description="ì„œìš¸ì£¼íƒë„ì‹œê³µì‚¬ ì£¼íƒì§€ì› ë´‡ - í†µí•© ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì‚¬ìš© ì˜ˆì‹œ:
  # í¬ë¡¤ë§
  python main.py crawl sohouse --fresh          # ì‚¬íšŒì£¼íƒ í¬ë¡¤ë§
  python main.py crawl all --fresh             # ëª¨ë“  í”Œë«í¼ í¬ë¡¤ë§
  
  
  # ë°ì´í„° ë¶„ì„
  python main.py analyze                        # RAW ë°ì´í„° ë¶„ì„
  
  # ì „ì²´ í”„ë¡œì„¸ìŠ¤
  python main.py all --fresh                   # í¬ë¡¤ë§ â†’ ë¶„ì„
        """
    )
    
    subparsers = parser.add_subparsers(dest='command', help='ì‹¤í–‰í•  ëª…ë ¹ì–´')
    
    # í¬ë¡¤ë§ ì„œë¸Œì»¤ë§¨ë“œ
    crawl_parser = subparsers.add_parser('crawl', help='í¬ë¡¤ë§ ì‹¤í–‰')
    crawl_parser.add_argument(
        "platform", 
        choices=["sohouse", "cohouse", "youth", "happy", "lh-ann", "sh-ann", "all"],
        help="í¬ë¡¤ë§í•  í”Œë«í¼ ì„ íƒ"
    )
    crawl_parser.add_argument(
        "--fresh", 
        action="store_true", 
        help="ê¸°ì¡´ ë°ì´í„° ì‚­ì œ í›„ í¬ë¡¤ë§"
    )
    crawl_parser.add_argument(
        "--max-youth-bbs", 
        type=int, 
        default=0,
        help="ì²­ë…„ì£¼íƒ ìµœëŒ€ BBS ìƒì„¸ í¬ë¡¤ë§ ìˆ˜ (0 = ì „ì²´)"
    )
    
    
    # ë¶„ì„ ì„œë¸Œì»¤ë§¨ë“œ
    subparsers.add_parser('analyze', help='ë°ì´í„° ë¶„ì„')
    
    # ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì„œë¸Œì»¤ë§¨ë“œ
    all_parser = subparsers.add_parser('all', help='ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰')
    all_parser.add_argument(
        "--fresh", 
        action="store_true", 
        help="ê¸°ì¡´ ë°ì´í„° ì‚­ì œ í›„ í¬ë¡¤ë§"
    )
    
    args = parser.parse_args()
    
    if args.command == 'crawl':
        if args.platform == "all":
            # ëª¨ë“  í”Œë«í¼ í¬ë¡¤ë§
            platforms = ["sohouse", "cohouse", "youth", "happy", "lh-ann", "sh-ann"]
            print(f"[START] ëª¨ë“  í”Œë«í¼ í¬ë¡¤ë§ ì‹œì‘ (ì´ {len(platforms)}ê°œ)")
            
            success_count = 0
            for platform in platforms:
                print(f"\n{'='*50}")
                print(f"[PLATFORM] {platform} í¬ë¡¤ë§ ì‹œì‘")
                print(f"{'='*50}")
                
                if crawl_platform(platform, args.fresh, args.max_youth_bbs):
                    success_count += 1
                else:
                    print(f"[ERROR] {platform} í¬ë¡¤ë§ ì‹¤íŒ¨")
            
            print(f"\n{'='*50}")
            print(f"[COMPLETE] ì „ì²´ í¬ë¡¤ë§ ì™„ë£Œ: {success_count}/{len(platforms)} ì„±ê³µ")
            print(f"{'='*50}")
        else:
            # ë‹¨ì¼ í”Œë«í¼ í¬ë¡¤ë§
            crawl_platform(args.platform, args.fresh, args.max_youth_bbs)
    
    
    elif args.command == 'analyze':
        analyze_data()
    
    elif args.command == 'all':
        # ì „ì²´ í”„ë¡œì„¸ìŠ¤: í¬ë¡¤ë§ â†’ ë¶„ì„
        print("ğŸš€ ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì‹œì‘: í¬ë¡¤ë§ â†’ ë¶„ì„")
        print("="*60)
        
        # 1. í¬ë¡¤ë§
        print("\n1ï¸âƒ£ í¬ë¡¤ë§ ë‹¨ê³„")
        platforms = ["sohouse", "cohouse", "youth", "happy", "lh-ann", "sh-ann"]
        success_count = 0
        for platform in platforms:
            if crawl_platform(platform, args.fresh, 0):
                success_count += 1
        
        if success_count > 0:
            # 2. ë¶„ì„
            print("\n2ï¸âƒ£ ë°ì´í„° ë¶„ì„ ë‹¨ê³„")
            analyze_data()
            
            print("\nğŸ‰ ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ!")
        else:
            print("\nâŒ í¬ë¡¤ë§ ì‹¤íŒ¨ë¡œ ì¸í•´ í”„ë¡œì„¸ìŠ¤ ì¤‘ë‹¨")
    
    else:
        parser.print_help()


if __name__ == "__main__":
    main()
